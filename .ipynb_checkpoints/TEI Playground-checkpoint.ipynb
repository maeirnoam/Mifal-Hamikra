{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558de29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import unicodedata\n",
    "from docx import Document\n",
    "from lxml import etree\n",
    "import os\n",
    "from win32com import client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted successfully and saved as 'output3.txt'\n"
     ]
    }
   ],
   "source": [
    "def convert_doc_to_docx(doc_path):\n",
    "    word = client.Dispatch(\"Word.Application\")\n",
    "    absolute_doc_path = os.path.abspath(doc_path)\n",
    "    doc = word.Documents.Open(absolute_doc_path)\n",
    "    doc_path_new = absolute_doc_path.replace(\".doc\", \".docx\")\n",
    "    doc.SaveAs2(doc_path_new, FileFormat=16)  # FileFormat=16 is for docx\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "    return doc_path_new\n",
    "\n",
    "# Example Usage\n",
    "docx_path = convert_doc_to_docx('Hosea.2.doc')\n",
    "\n",
    "def convert_docx_to_txt(docx_file_path, txt_file_path):\n",
    "    # Load the .docx file\n",
    "    doc = Document(docx_file_path)\n",
    "\n",
    "    # Extract text from each paragraph in the document\n",
    "    text_content = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Write the extracted text to a .txt file\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text_content)\n",
    "\n",
    "    print(f\"File converted successfully and saved as '{txt_file_path}'\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "convert_docx_to_txt('Hosea.2.docx', 'output3.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c96e775",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hosea 2\\n',\n",
       " '\\n',\n",
       " '1–2] ]* 110–11\\n',\n",
       " '1 ימד] * verb/nom\\n',\n",
       " 'ולא יספר] ]h >\\n',\n",
       " 'יֵאָמֵר להם2] ][ diath | ]h pr ἐκεῖ = [\\n',\n",
       " '3 לַאֲחֵיכם] ] num\\n',\n",
       " 'ולאחותיכם] ]* num\\n',\n",
       " '4 ו(תסר)] *[ >\\n',\n",
       " '(ו)תסר ... מפניה] ] ἐξαρῶ … ἐκ προσώπου μου\\n',\n",
       " '5 פן] ] ὅπως ἂν\\n',\n",
       " 'כ(יום)] ]h καθὼς ἐν\\n',\n",
       " 'ושמתיה ... ושתה] [ condens\\n',\n",
       " 'כ(מדבר) ... כ(ארץ)] ]h prep\\n',\n",
       " '7 נתני] ] + μοι\\n',\n",
       " 'שמני] ]-[ & | ]h pr καὶ τὸν οἶνόν μου | T- > pron\\n',\n",
       " '(ו)שקויָי] ] πάντα ὅσα μοι καθήκει = [, T 8\\n',\n",
       " '8 דרכך] ][ pron | *h[T num\\n',\n",
       " 'גדרהּ] ] τὰς ὁδοὺς αὐτῆς = [ | ]hT > pron\\n',\n",
       " 'ונתיבותיה] ][ num | [Th > pron \\n',\n",
       " '9 תמצא] ][ + pron\\n',\n",
       " 'אלכה ואשובה] [ ~\\n',\n",
       " '10 וכסף] ]h + καὶ χρυσίον\\n',\n",
       " 'vוזהב,] ] vαὐτὴ δὲ,ἀργυρᾶ καὶ χρυσᾶ \\n',\n",
       " 'עשו] ] pers | [ pr ܘܡܢܗ + T\\n',\n",
       " 'ל(בעל)] ] τῇ\\n',\n",
       " '11 ותירושי] ]h + καὶ τὸ ἔλαιόν μου\\n',\n",
       " 'לכסות] ] τοῦ μὴ καλύπτειν | * quae operiebant\\n',\n",
       " '12 נבלתה] ]h τὴν ἀσχημοσύνην αὐτῆς = [T \\n',\n",
       " 'מידי] ~ pron\\n',\n",
       " '13 (ו)הִשְׁבַּתִּי] ] ἀποστρέψω\\n',\n",
       " 'חגה – מועדה] ][T num\\n',\n",
       " 'חגה] ]h &\\n',\n",
       " 'ו(שבתה)] *- >\\n',\n",
       " 'מועדה] * numII\\n',\n",
       " '14 לי ... לי] [ reduct\\n',\n",
       " 'ושמתים ... ואכלתם] * pron\\n',\n",
       " 'ליער] ] εἰς μαρτύριον\\n',\n",
       " 'השדה] ]- + καὶ τὰ πετεινὰ τοῦ οὐρανοῦ καὶ τὰ ἑρπετὰ τῆς γῆς \\n',\n",
       " '15 אשר ... להם] ]- ἐν αἷς … αὐτοῖς + ]h\\n',\n",
       " 'להם] [ ܒܗܘܢ\\n',\n",
       " '(ו)תעד] ~ + pron | [ ܫܩܠܬ\\n',\n",
       " 'נזמה וחליתה] ][T num\\n',\n",
       " '16 והלכתיה המדבר] ]h καὶ τάξω αὐτὴν ὡς ἔρημον\\n',\n",
       " '17 כְּרָמיה] * vinitores eius = ~\\n',\n",
       " 'לְפֶתַח] ] διανοῖξαι + *[\\n',\n",
       " 'תקוה] ] σύνεσιν αὐτῆς | [ ܣܘܟܠܗܿ\\n',\n",
       " 'וכיום] ]* num\\n',\n",
       " 'עלותה] ]h τῆς ἐξόδου αὐτῆς\\n',\n",
       " '18 תקראי1, 2] ]*[ pers | ]*[ + pronII III\\n',\n",
       " 'בעלי] ] Βααλιμ = *h\\n',\n",
       " '19 והסרתי] [h ܘܐܘܒܕ\\n',\n",
       " 'שמות] ]h*h[T num\\n',\n",
       " 'מפיה] ]h pron\\n',\n",
       " 'יִזָּכרו ... בשמם] ] μνησθῶσιν … τὰ ὀνόματα αὐτῶν + *[\\n',\n",
       " '20 ועם ... ו(רמש)] ]-* rep\\n',\n",
       " 'ומלחמה] ]h >\\n',\n",
       " 'אשבור] [ ܐܒܛܠ = T\\n',\n",
       " '(ו)השכבתי(ם)] ] κατοικιῶ = [T\\n',\n",
       " '(והשכבתי)ם] ]- pron\\n',\n",
       " '21 ב(צדק ו)ב(משפט)] *- reduct\\n',\n",
       " 'וב(חסד ו)ב(רחמים)] ]h*- reduct\\n',\n",
       " '22 את יהוה] * quia ego DominusII III IV\\n',\n",
       " '23 אענה1] ][ >II\\n',\n",
       " 'והם] ]h καὶ ὁ οὐρανός\\n',\n",
       " '25 אתה] [ >\\n',\n",
       " 'יאמר] [ + pron\\n',\n",
       " 'אלהי] ] κύριος ὁ θεός μου εἶ σύ + *']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('output3.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    txt = file\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da15db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_biblical_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    books = {}\n",
    "    current_book = None\n",
    "    current_chapter = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"ספר\"):\n",
    "            current_book = line\n",
    "            books[current_book] = {}\n",
    "        elif line.startswith(\"פרק\"):\n",
    "            current_chapter = line\n",
    "            books[current_book][current_chapter] = []\n",
    "        else:\n",
    "            if current_book is not None and current_chapter is not None:\n",
    "                books[current_book][current_chapter].append(line)\n",
    "\n",
    "    return books\n",
    "\n",
    "# Example usage\n",
    "file_path = 'output.txt'\n",
    "books = process_biblical_text(file_path)\n",
    "\n",
    "# # Example to print the structure\n",
    "# for book, chapters in books.items():\n",
    "#     for chapter, texts in chapters.items():\n",
    "#         print(f'{book} {chapter}: {texts[:2]}...')  # print first two lines of each chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff866a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantillation_marks = {\n",
    "    '\\u05BD': 'Meteg',\n",
    "    '\\u0594': 'Zaqef Qatan',\n",
    "    '\\u0597': 'Revia',\n",
    "    '\\u0598': 'Tsinnorit',\n",
    "    '\\u05A1': 'Pazer',\n",
    "    '\\u059F': 'Qarney Para',\n",
    "    '\\u0595': 'Zaqef Gadol',\n",
    "    '\\u059B': 'Tvir',\n",
    "    '\\u0591': 'Atnach',\n",
    "    '\\u05AA': 'Galgal',\n",
    "    '\\u05A7': 'Darga',\n",
    "    '\\u05BE': 'Maqqef',\n",
    "    '\\u05C3': 'Sof Pasuq',\n",
    "    '\\u0592': 'Segol',\n",
    "    '\\u05AE': 'Zarqa',\n",
    "    '\\u0599': 'Pashta',\n",
    "    '\\u05A9': 'Tlisha Qtana',\n",
    "    '\\u059A': 'Yetiv',\n",
    "    '\\u059D': 'Geresh Muqdam',\n",
    "    '\\u05AD': 'Dechi',\n",
    "    '\\u05A0': 'Tlisha Gdola',  \n",
    "    '\\u05AB': 'Ole',\n",
    "    '\\u059C': 'Geresh',\n",
    "    '\\u059E': 'Gershayim',\n",
    "    '\\u05A8': 'Qadma',  \n",
    "    '\\u05AC': 'Illuy',\n",
    "    '\\u0593': 'Shalshelet',\n",
    "    '\\u05A4': 'Mahpakh',\n",
    "    '\\u05A5': 'Merkha',\n",
    "    '\\u05A6': 'Merkha Khfula',\n",
    "    '\\u0596': 'Tipcha',\n",
    "    '\\u05A3': 'Munnach',\n",
    "}\n",
    "\n",
    "def tag_word_with_cantillation(word, cantillation_marks):\n",
    "    for name, mark in cantillation_marks.items():\n",
    "        if mark in word:\n",
    "            return (mark, name)  # Returns the word and the cantillation mark's name\n",
    "    return (None, None)  # Returns the word with None if no mark is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac9f47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#render main text into TEI\n",
    "def create_tei_xml(books):\n",
    "    root = etree.Element('TEI', xmlns=\"http://www.tei-c.org/ns/1.0\")\n",
    "    teiHeader = etree.SubElement(root, 'teiHeader')\n",
    "    # Add header elements as needed here...\n",
    "\n",
    "    text_element = etree.SubElement(root, 'text')\n",
    "    body = etree.SubElement(text_element, 'body')\n",
    "\n",
    "    for book_name, chapters in books.items():\n",
    "        book_div = etree.SubElement(body, 'div', type='book', n=book_name)\n",
    "        for chapter_name, verses in chapters.items():\n",
    "            chapter_div = etree.SubElement(book_div, 'div', type='chapter', n=chapter_name)\n",
    "            for verse in verses:\n",
    "                lg = etree.SubElement(chapter_div, 'lg', type='verse')\n",
    "                process_verse(verse, lg)\n",
    "\n",
    "    return etree.tostring(root, pretty_print=True, encoding='UTF-8', xml_declaration=True)\n",
    "\n",
    "def process_verse(verse_text, chapter_div):\n",
    "    # Regular expression to capture verses and dividers\n",
    "    pattern = r'(:|\\[פ\\]|\\[ס\\])'\n",
    "    parts = re.split(pattern, verse_text)\n",
    "\n",
    "    verse_counter = 1\n",
    "    for part in parts:\n",
    "        if part in [\":\", \"[פ]\", \"[ס]\"]:\n",
    "            # If the part is a divider, add it as a separate element\n",
    "            divider = etree.SubElement(chapter_div, 'divider')\n",
    "            divider.text = part\n",
    "        else:\n",
    "            # Process normal verse text\n",
    "            if part.strip():  # ignore empty strings resulting from split\n",
    "                verse_number = f\"פסוק {verse_counter}\"\n",
    "                lg = etree.SubElement(chapter_div, 'lg', type='verse', n=verse_number)\n",
    "                add_tokens_to_verse(part, lg)\n",
    "                verse_counter += 1\n",
    "\n",
    "def add_tokens_to_verse(verse, lg_element):\n",
    "    tokens = verse.split()\n",
    "    for token in tokens:\n",
    "        # Handle special signs within the token\n",
    "        process_word(token, lg_element)\n",
    "\n",
    "def process_word(word, lg_element):\n",
    "    # Check for the \"׀\" sign and tag it as a \"טעם\" type \"פסק\"\n",
    "    if \"׀\" in word:\n",
    "        taam_element = etree.SubElement(lg_element, 'taam', type='פסק')\n",
    "        taam_element.text = \"׀\"\n",
    "    else:\n",
    "        # Process word for hyphen-like signs\n",
    "        process_hyphenated_word(word, lg_element)\n",
    "\n",
    "def process_hyphenated_word(word, lg_element):\n",
    "    # Check if the word contains \"־\" and process accordingly\n",
    "    if \"־\" in word:\n",
    "        # Create the full word element without special signs\n",
    "        full_word_element = etree.SubElement(lg_element, 'w')\n",
    "        full_word_element.text = re.sub(r'\\$[1-4]', '', word)\n",
    "        # Process each part of the hyphenated word\n",
    "        subwords = word.split(\"־\")\n",
    "        for subword in subwords:\n",
    "            nested_word_element = etree.SubElement(full_word_element, 'nestedWord')\n",
    "            # Remove special signs from the nested word and add them as nested elements\n",
    "            nested_special_signs(subword, nested_word_element)\n",
    "    else:\n",
    "        # If not a hyphenated word, process normally\n",
    "        word_element = etree.SubElement(lg_element, 'w')\n",
    "        nested_special_signs(word, word_element)\n",
    "\n",
    "def nested_special_signs(word, word_element):\n",
    "    # First, tag the word with its cantillation mark if present\n",
    "    mark, cantillation_name = tag_word_with_cantillation(word, cantillation_marks)\n",
    "    if cantillation_name:\n",
    "        # Create a subelement for the cantillation mark\n",
    "        cantillation_element = etree.SubElement(word_element, 'taam', type=cantillation_name)\n",
    "        cantillation_element.text = mark\n",
    "    \n",
    "    # Then, process the special signs within the word\n",
    "    special_signs = re.finditer(r'\\$[1-4]', word)\n",
    "    start = 0\n",
    "    for match in special_signs:\n",
    "        before_sign = word[start:match.start()]\n",
    "        if before_sign:\n",
    "            word_element.text = (word_element.text or '') + before_sign\n",
    "        \n",
    "        sign_element = etree.SubElement(word_element, 'specialSign')\n",
    "        sign_element.text = match.group()\n",
    "        start = match.end()\n",
    "    \n",
    "    remaining_text = word[start:]\n",
    "    if remaining_text:\n",
    "        word_element.text = (word_element.text or '') + remaining_text\n",
    "\n",
    "def tag_word_with_cantillation(word, cantillation_marks):\n",
    "    for mark, names in cantillation_marks.items():\n",
    "        if mark in word:\n",
    "            # Assuming multiple names might be associated with a single mark, join them with a comma\n",
    "            name = ', '.join(names) if isinstance(names, list) else names\n",
    "            return (mark, name)  # Remove the mark from the word\n",
    "    return (word, None)  # No cantillation mark found, return the word as is\n",
    "            \n",
    "def is_special_token(token):\n",
    "    # Define your logic to identify a special token here\n",
    "    # For example, if special tokens are enclosed in brackets:\n",
    "    return token.startswith('[') and token.endswith(']')\n",
    "\n",
    "# Example usage\n",
    "tei_xml = create_tei_xml(books)\n",
    "with open('output.tei.xml', 'wb') as file:\n",
    "    file.write(tei_xml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_special_signs(word, word_element, cantillation_marks):\n",
    "    # First, tag the word with its cantillation mark if present\n",
    "    processed_word, cantillation_name = tag_word_with_cantillation(word, cantillation_marks)\n",
    "    if cantillation_name:\n",
    "        # Create a subelement for the cantillation mark\n",
    "        cantillation_element = etree.SubElement(word_element, 'taam', type=cantillation_name)\n",
    "        cantillation_element.text = processed_word\n",
    "    \n",
    "    # Then, process the special signs within the word\n",
    "    special_signs = re.finditer(r'\\$[1-4]', processed_word)\n",
    "    start = 0\n",
    "    for match in special_signs:\n",
    "        before_sign = processed_word[start:match.start()]\n",
    "        if before_sign:\n",
    "            word_element.text = (word_element.text or '') + before_sign\n",
    "        \n",
    "        sign_element = etree.SubElement(word_element, 'specialSign')\n",
    "        sign_element.text = match.group()\n",
    "        start = match.end()\n",
    "    \n",
    "    remaining_text = processed_word[start:]\n",
    "    if remaining_text:\n",
    "        word_element.text = (word_element.text or '') + remaining_text\n",
    "\n",
    "def tag_word_with_cantillation(word, cantillation_marks):\n",
    "    for mark, names in cantillation_marks.items():\n",
    "        if mark in word:\n",
    "            # Assuming multiple names might be associated with a single mark, join them with a comma\n",
    "            name = ', '.join(names) if isinstance(names, list) else names\n",
    "            return (word.replace(mark, ''), name)  # Remove the mark from the word\n",
    "    return (word, None)  # No cantillation mark found, return the word as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e0c3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Revia ֗'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_cantillation(word, cantillation_marks):\n",
    "    for mark, name in cantillation_marks.items():\n",
    "        if mark in word:\n",
    "            return name+' '+mark  # Return the name of the first cantillation mark found\n",
    "    return \"No cantillation mark found\"  # Return this if no cantillation mark is present in the word\n",
    "\n",
    "# Example word\n",
    "example_word = \"הָיָ֗ה\"\n",
    "\n",
    "# Let's identify the cantillation mark in the example word\n",
    "cantillation_name = identify_cantillation(example_word, cantillation_marks)\n",
    "cantillation_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## render apparatus 3 ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new logic\n",
    "#1. split into lemma and app_entry. process each separately\n",
    "# lemma: split into number range and lemma range. \n",
    "#numbers include \\d+'-', lemmas need to include potential k\\q attribute, and word number.\n",
    "# app_entry: splits into mss., reading, comments, cross-reference.\n",
    "# \n",
    "#to do: \n",
    "#1. get_verse, if verse is empty. same for get_witness (if its inside a comma)\n",
    "#2. QA: \"איסירם] 30 89 איסרם | 96 (pm) אייסרם, (sm) אייסירים | 150 (pm) אסירים\" (when split on comma allow for witness completion)\n",
    "#3. include more special signs ~ and include .. in things that could appear at the end of the hebrew reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811ca584",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_entries = [\n",
    "\"8 k ואמאסאך / q ואמאסך] 30 93 (sm) 96 150 (pm) q IV | 93 (pm) אמסך\",\n",
    "\"8 שָׂך] 30 (pm) 93 (pm) 150 (pm) סךII IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))\",\n",
    "\"6 עוד1] 30 (pm) >I II IV (similarly b. Pesaḥim 87bmss)\",\n",
    "\"6 הדעת1] 93 (pm) 150 (pm) דעתII\",\n",
    "\"14 זעקו] 30 (pm) יזעקו | 150 (pm) >\",\n",
    "\"10 k שעריריה / q שערוריה] 30 (pm) 89 (pm) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) שערורהIV\",\n",
    "\"2 ללבבם] 96 (pm) ללבם | 30 93 150 (pm) בלבבםI\",\n",
    "    \"איסירם] 30 89 איסרם | 96 (pm) אייסרם, (sm) אייסירים | 150 (pm) אסירים\",\n",
    "    \"7 נדמֶה] 93 נדמָה | 30 (pm) + אפרים\",\n",
    "    \"10 ואסרם] 96 (pm) יאשרם | 150 ..על\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2fbbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'verses': [8],\n",
       "   'lemmas': [{'lemma': 'ואמאסאך', 'k': True},\n",
       "    {'lemma': 'ואמאסך', 'q': True}]},\n",
       "  [[({'Witnesses': [('', '30', ''),\n",
       "       ('', '93', 'sm'),\n",
       "       ('', '96', ''),\n",
       "       ('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'q  '}},\n",
       "     {'Cross References': ['IV']})],\n",
       "   [({'Witnesses': [('', '93', 'pm')]},\n",
       "     {'Reading': {'Reading': 'אמסך'}},\n",
       "     {'Cross References': []})]]),\n",
       " ({'verses': [8], 'lemmas': [{'lemma': 'שָׂך'}]},\n",
       "  [[({'Witnesses': [('', '30', 'pm'), ('', '93', 'pm'), ('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'סך  ',\n",
       "       'Comment': '(See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'}},\n",
       "     {'Cross References': ['II', 'IV']})]]),\n",
       " ({'verses': [6], 'lemmas': [{'lemma': 'עוד', 'number': '1'}]},\n",
       "  [[({'Witnesses': [('', '30', 'pm')]},\n",
       "     {'Reading': {'Sigla': '>', 'Comment': '(similarly b. Pesaḥim 87bmss)'}},\n",
       "     {'Cross References': ['I', 'II', 'IV']})]]),\n",
       " ({'verses': [6], 'lemmas': [{'lemma': 'הדעת', 'number': '1'}]},\n",
       "  [[({'Witnesses': [('', '93', 'pm'), ('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'דעת'}},\n",
       "     {'Cross References': ['II']})]]),\n",
       " ({'verses': [14], 'lemmas': [{'lemma': 'זעקו'}]},\n",
       "  [[({'Witnesses': [('', '30', 'pm')]},\n",
       "     {'Reading': {'Reading': 'יזעקו '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '150', 'pm')]},\n",
       "     {'Reading': {'Sigla': '>'}},\n",
       "     {'Cross References': []})]]),\n",
       " ({'verses': [10],\n",
       "   'lemmas': [{'lemma': 'שעריריה', 'k': True},\n",
       "    {'lemma': 'שערוריה', 'q': True}]},\n",
       "  [[({'Witnesses': [('', '30', 'pm'), ('', '89', 'pm'), ('', '150', 'sm')]},\n",
       "     {'Reading': {'Reading': 'k'}},\n",
       "     {'Cross References': []}),\n",
       "    ({'Witnesses': [('', '30', 'sm'),\n",
       "       ('', '89', 'sm'),\n",
       "       ('', '93', 'sm'),\n",
       "       ('', '96', ''),\n",
       "       ('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'q'}},\n",
       "     {'Cross References': []}),\n",
       "    ({'Witnesses': [('', '93', 'pm')]},\n",
       "     {'Reading': {'Reading': 'שערורה'}},\n",
       "     {'Cross References': ['IV']})]]),\n",
       " ({'verses': [2], 'lemmas': [{'lemma': 'ללבבם'}]},\n",
       "  [[({'Witnesses': [('', '96', 'pm')]},\n",
       "     {'Reading': {'Reading': 'ללבם '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '30', ''), ('', '93', ''), ('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'בלבבם'}},\n",
       "     {'Cross References': ['I']})]]),\n",
       " ({'verses': [], 'lemmas': [{'lemma': 'איסירם'}]},\n",
       "  [[({'Witnesses': [('', '30', ''), ('', '89', '')]},\n",
       "     {'Reading': {'Reading': 'איסרם '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '96', 'pm')]},\n",
       "     {'Reading': {'Reading': 'אייסרם'}},\n",
       "     {'Cross References': []}),\n",
       "    ({'Witnesses': []},\n",
       "     {'Reading': {'Reading': 'אייסירים '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '150', 'pm')]},\n",
       "     {'Reading': {'Reading': 'אסירים'}},\n",
       "     {'Cross References': []})]]),\n",
       " ({'verses': [7], 'lemmas': [{'lemma': 'נדמֶה'}]},\n",
       "  [[({'Witnesses': [('', '93', '')]},\n",
       "     {'Reading': {'Reading': 'נדמָה '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '30', 'pm')]},\n",
       "     {'Reading': {'Sigla': '+', 'Reading': 'אפרים'}},\n",
       "     {'Cross References': []})]]),\n",
       " ({'verses': [10], 'lemmas': [{'lemma': 'ואסרם'}]},\n",
       "  [[({'Witnesses': [('', '96', 'pm')]},\n",
       "     {'Reading': {'Reading': 'יאשרם '}},\n",
       "     {'Cross References': []})],\n",
       "   [({'Witnesses': [('', '150', '')]},\n",
       "     {'Reading': {'Reading': 'על'}},\n",
       "     {'Cross References': []})]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[process_full_entry(example) for example in full_entries]#[-1]#[0]['verses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e5b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_entry(text):\n",
    "    lemma, part_entry = split_full_entry(text)\n",
    "    lemma_dict = lemma_verse_processor(lemma)\n",
    "\n",
    "    # Split part_entry by '|'\n",
    "    if '|' in part_entry:\n",
    "        entry_parts = part_entry.split('|')\n",
    "    else:\n",
    "        entry_parts = [part_entry]\n",
    "\n",
    "    # Initialize a list to hold all processed parts\n",
    "    processed_parts = []\n",
    "\n",
    "    # Process each part separately\n",
    "    for part in entry_parts:\n",
    "        # Split part by ',' not inside parentheses\n",
    "        sub_parts = split_on_comma_not_in_parentheses(part)\n",
    "\n",
    "        # Process each sub-part and collect the results\n",
    "        processed_sub_parts = [process_entry(sub_part) for sub_part in sub_parts]\n",
    "\n",
    "        # Assuming you want to concatenate processed sub-parts for each part\n",
    "        processed_parts.append(processed_sub_parts)\n",
    "\n",
    "    # Flatten the list if necessary or keep it nested based on your requirement\n",
    "    decoded_entries = processed_parts\n",
    "\n",
    "    return lemma_dict, decoded_entries\n",
    "\n",
    "def split_on_comma_not_in_parentheses(part):\n",
    "    \"\"\"\n",
    "    Splits the string on ',' not inside parentheses.\n",
    "    \"\"\"\n",
    "    sub_parts = []\n",
    "    current_part = []\n",
    "    paren_depth = 0  # Track depth of parentheses\n",
    "\n",
    "    for char in part:\n",
    "        if char == '(':\n",
    "            paren_depth += 1\n",
    "        elif char == ')':\n",
    "            paren_depth -= 1\n",
    "        elif char == ',' and paren_depth == 0:\n",
    "            # At a top-level comma, split here\n",
    "            sub_parts.append(''.join(current_part))\n",
    "            current_part = []\n",
    "            continue\n",
    "\n",
    "        current_part.append(char)\n",
    "\n",
    "    # Add the last part if there's any\n",
    "    if current_part:\n",
    "        sub_parts.append(''.join(current_part))\n",
    "\n",
    "    return sub_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadb22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_full_entry(text):\n",
    "    sliced_entry = text.split(sep=']')\n",
    "    lemma, entry = sliced_entry\n",
    "#         print(f\"lemma: {lemma}\")\n",
    "#         print(f\"entry: {entry}\")\n",
    "    return lemma, entry    \n",
    "\n",
    "def lemma_verse_processor(text):\n",
    "    # Simplified approach: first split into digits and lemmas\n",
    "    # Regex to match the verse numbers at the beginning\n",
    "    verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "    # Extract verses\n",
    "    verses_match = re.match(verse_regex, text)\n",
    "    verses = list(map(int, verses_match.group(1).split('–'))) if verses_match else []\n",
    "    \n",
    "    # Isolate lemmas part by removing the verses\n",
    "    lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "    return {\n",
    "        'verses': verses,\n",
    "        'lemmas': process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "    }\n",
    "\n",
    "# Function to process individual lemmas or ranges, after the split,\n",
    "lemma_regex = r'(k|q)?\\s*([^\\d\\s]+)(\\d?\\,?\\d?)'#(\\d+(?:–\\d+)?)\\s\n",
    "\n",
    "def process_lemma_with_range_and_diacritics(lemma):\n",
    "    # Adjust regex to include diacritical marks and punctuation within Hebrew words\n",
    "    \n",
    "    \n",
    "    # Check for range indicated by \"–\" and process accordingly\n",
    "    if \"–\" in lemma:\n",
    "        from_lemma, to_lemma = lemma.split(\"–\")\n",
    "        return {\n",
    "            'from': process_individual_lemma(from_lemma.strip()),\n",
    "            'to': process_individual_lemma(to_lemma.strip())\n",
    "        }\n",
    "\n",
    "    # Split lemma if there are separate lemmas with \"/\"\n",
    "    split_lemmas = re.split(r'\\s*/\\s*', lemma) if '/' in lemma else [lemma]\n",
    "    \n",
    "    processed_lemmas = []\n",
    "    for split_lemma in split_lemmas:\n",
    "        processed = process_individual_lemma(split_lemma)\n",
    "        processed_lemmas.extend(processed)\n",
    "    \n",
    "    return processed_lemmas\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "    processed_lemmas = []\n",
    "    for match in matches:\n",
    "        prefix, word, number = match\n",
    "        lemma_dict = {'lemma': word}\n",
    "        if prefix: lemma_dict[prefix] = True\n",
    "        if number: lemma_dict['number'] = (number)\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "    return processed_lemmas\n",
    "\n",
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def extract_cross_references(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "def parse_witnesses(text): #process witnesses\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d\\.]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "def parse_comma_witnesses(text): #process witnesses within comma, i.e., consider it is only mentioned once\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)?\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~.]?)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "#splitting entry into witnesses and reading (if only one group assign to witnesses)\n",
    "def witness_reading_splitter(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>~]?\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>~])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "\n",
    "def process_entry(entry):\n",
    "    clean_entry, cross_references = extract_cross_references(entry)\n",
    "    split_entry = witness_reading_splitter(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry[0])}\n",
    "        if len(split_entry)==2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else: #there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1]+split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry)}\n",
    "        reading = ''\n",
    "    return witnesses, {\"Reading\":reading}, {\"Cross References\":cross_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over full entries. get verse number from previous if needed. also get reading from lemma. and split on | ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "715948c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process lemma:\n",
    "#split into digits and lemmas. then process each separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "b18607da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'verses': [10], 'lemmas': [{'lemma': 'ואסרם'}]},\n",
       " [[({'Witnesses': [('', '96', 'pm')]},\n",
       "    {'Reading': {'Reading': 'יאשרם '}},\n",
       "    {'Cross References': []})],\n",
       "  [({'Witnesses': [('', '150', '')]},\n",
       "    {'Reading': {'Reading': 'על'}},\n",
       "    {'Cross References': []})]])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[process_full_entry(example) for example in full_entries][-1]#[0]['verses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d89e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "b8c31720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old function, didnt take the splitting into commas consideration\n",
    "# def process_full_entry(text):\n",
    "#     lemma, part_entry = split_full_entry(text)\n",
    "#     lemma_dict = lemma_verse_processor(lemma)\n",
    "# #     if len(lemma_dict['verses'])==0: #get verse from previous entry\n",
    "# #         lemma_dict['verses'] = \n",
    "        \n",
    "#     #entry_units = split_entry_units # splits on | and ,\n",
    "#     #for entry in entry_units:\n",
    "#     decoded_entry = process_entry(part_entry)\n",
    "#     return lemma_dict, decoded_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "4b9188ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def remove_and_list_roman_numerals(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "def custom_split_string(text): #process witnesses\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~]?)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "#splitting entry into witnesses and reading (if only one group assign to witnesses)\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>]?\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "\n",
    "def process_entry(entry):\n",
    "    clean_entry, cross_references = remove_and_list_roman_numerals(entry)\n",
    "    split_entry = split_string(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': custom_split_string(split_entry[0])}\n",
    "        if len(split_entry)==2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else: #there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1]+split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': custom_split_string(split_entry)}\n",
    "        reading = ''\n",
    "    return witnesses, {\"Reading\":reading}, {\"Cross References\":cross_references}\n",
    "\n",
    "\n",
    "# for entry in sample_texts:\n",
    "#     print(f\"entry: {entry}\")\n",
    "#     clean_entry, cross_references = remove_and_list_roman_numerals(entry)\n",
    "#     split_entry = split_string(clean_entry)\n",
    "#     if type(split_entry) is tuple:\n",
    "#         witnesses = {'witnesses': custom_split_string(split_entry[0])}\n",
    "#         reading = parse_reading_entry(split_entry[1])\n",
    "#         print(f\"witnesses: {witnesses}\")\n",
    "#         print(f\"reading: {reading}\")\n",
    "#     else:\n",
    "#         witnesses = {'witnesses': custom_split_string(split_entry)}\n",
    "#         print(f\"witnesses: {witnesses}\")\n",
    "#     print(f\"references: {cross_references}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "cd41862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"96 (non voc)\",\n",
    "    \"30 (pm) 93 (pm) 150 (pm) + סךII IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))\",\n",
    "    \"93 (non voc) 96 150 (non voc) + את\",\n",
    "    \"30 (pm) >\",\n",
    "    \"30 + לי (non voc)I II\",\n",
    "    \"30 (pm) >I II IV (similarly b. Pesaḥim 87bmss)\",\n",
    "    \"93 (pm) ביהושעIV (similarly PesiqtaR 33 (153b))\",\n",
    "    \"96 >I II IV\",\n",
    "    \"130 k\",\n",
    "    \"G-B Msr 34 k ממני / q ממנוIV\",\n",
    "    \"93 כד..\",\n",
    "    \"150 ..דברים\",\n",
    "    \"G-B Eb 94 ותָעָד (understood as \\עוד (rather than \\עדי))\",\n",
    "    \"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "a0ae7422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Witnesses': [('', '93', 'pm')]},\n",
       " {'Reading': {'Reading': 'ביהושע ',\n",
       "   'Comment': '(similarly PesiqtaR 33 (153b))'}},\n",
       " {'Cross References': ['IV']})"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_entry(sample_texts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "2a94af94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Reading': 'ביהושע ', 'Comment': '(similarly PesiqtaR 33 (153b))'}"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_entry, cross_references = remove_and_list_roman_numerals(sample_texts[6])\n",
    "split_entry = split_string(clean_entry)\n",
    "if type(split_entry) is tuple:\n",
    "    witnesses = {'Witnesses': custom_split_string(split_entry[0])}\n",
    "    if len(split_entry)==2:\n",
    "        reading = parse_reading_entry(split_entry[1])\n",
    "    else: #there are 3 groups:\n",
    "        reading = parse_reading_entry(split_entry[1]+split_entry[2])\n",
    "\n",
    "reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "08fa1e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '30', 'pm'), ('', '93', ''), ('', '150', 'pm')]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_split_string(text): #process witnesses\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "# \"96 (non voc)\",\n",
    "# \"30 (pm) 93 (pm) 150 (pm)\n",
    "test_witness = \"30 (pm) 93 150 (pm)\"\n",
    "custom_split_string(test_witness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "fbefcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['96 (non voc)', ('30 (pm) 93 (pm) 150 (pm) ', '+ סך', 'II IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'), ('93 (non voc) 96 150 (non voc) ', '+ את', ''), ('30 (pm) ', '>', ''), ('30 ', '+ לי', ' (non voc)I II'), ('30 (pm) ', '>', 'I II IV (similarly b. Pesaḥim 87bmss)'), ('93 (pm)', ' ביהושע', 'IV (similarly PesiqtaR 33 (153b))'), ('96 ', '>', 'I II IV'), ('130', ' k', ''), ('G-B Msr 34', ' k', ' ממני / q ממנוIV'), ('93', ' כד', '..'), ('150 ..', 'דברים', ''), ('G-B Eb 94', ' ותָעָד', ' (understood as \\\\עוד (rather than \\\\עדי))'), ('30 89 (sm) 93 (pm) 150 (non voc) ', '+ כי', 'I II IV')]\n"
     ]
    }
   ],
   "source": [
    "#try parsing single entry app, splitting into witnesses and reading (if only one group assign to witnesses)\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>]?\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "processed_sample = [split_string(text) for text in sample_texts]\n",
    "\n",
    "print(processed_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "473068c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Reading': 'סך ',\n",
       "  'Comment': '(See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'},\n",
       " {'Sigla': '+', 'Reading': 'את'},\n",
       " {'Sigla': '>'},\n",
       " {'Sigla': '+', 'Reading': 'לי ', 'Comment': '(non voc)'},\n",
       " {'Sigla': '>', 'Comment': '(similarly b. Pesaḥim 87bmss)'},\n",
       " {'Reading': 'ביהושע ', 'Comment': '(similarly PesiqtaR 33 (153b))'},\n",
       " {},\n",
       " {'Reading': 'k'},\n",
       " {'Reading': 'k ממני / q ממנו'},\n",
       " {'Reading': 'כד..'},\n",
       " {'Reading': '..דברים'},\n",
       " {'Reading': 'נַחֵם ',\n",
       "  'Comment': '(taken as infinitive, see Yeivin, Babylonian Vocalization, 1:542)'},\n",
       " {'Reading': 'חכֵם ', 'Comment': '(!)'}]"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for parsing the reading+comment (assumes witnesses and cross references have been removed)\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~]?)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "# Process the sample reading texts with the refined function\n",
    "parse_reading_entry = [parse_reading_entry(text) for text in sample_reading_texts]\n",
    "\n",
    "parse_reading_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "94a5de6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [{'witnesses': '30 (pm) 93 (pm) 150 (pm)', 'reading': '+ סך', 'comments': 'II IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'}], [{'witnesses': '93 (non voc) 96 150 (non voc)', 'reading': '+ את', 'comments': ''}], [], [{'witnesses': '30', 'reading': '+ לי', 'comments': '(non voc)I II'}], [], [{'witnesses': '93 (pm)', 'reading': 'ביהושע', 'comments': 'IV (similarly PesiqtaR 33 (153b))'}], [], [], [{'witnesses': 'G-B Msr 34 k', 'reading': 'ממני', 'comments': '/ q ממנוIV'}], [{'witnesses': '93', 'reading': 'כד', 'comments': '..'}], [{'witnesses': '150 ..', 'reading': 'דברים', 'comments': ''}], [{'witnesses': 'G-B Eb 94', 'reading': 'ותָעָד', 'comments': '(understood as \\\\עוד (rather than \\\\עדי))'}], [{'witnesses': '30 89 (sm) 93 (pm) 150 (non voc)', 'reading': '+ כי', 'comments': 'I II IV'}]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_string_processor(input_string, regex_pattern):\n",
    "    # Helper function to apply regex and extract groups\n",
    "    def apply_regex_and_extract(text):\n",
    "        matches = re.finditer(regex_pattern, text)\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            results.append({\n",
    "                'witnesses': match.group(1).strip(),\n",
    "                'reading': match.group(2).strip(),\n",
    "                'comments': match.group(3).strip() if match.group(3) else ''\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Process splits with \"|\", then \",\"\n",
    "    def process_splits(text, delimiter):\n",
    "        parts = text.split(delimiter)\n",
    "        processed_parts = []\n",
    "        for part in parts:\n",
    "            # Apply regex to each part\n",
    "            processed = apply_regex_and_extract(part)\n",
    "            if processed:\n",
    "                processed_parts.extend(processed)\n",
    "        return processed_parts\n",
    "\n",
    "    # Start processing\n",
    "    processed_result = process_splits(input_string, '|')  # Start with the highest level of split\n",
    "\n",
    "    return processed_result\n",
    "\n",
    "# Custom regex pattern as provided\n",
    "custom_regex = r'^(.*?)([\\+<~>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$'\n",
    "\n",
    "# Test with the provided sample input\n",
    "sample_input = \"G-B msr. 30 (pm) G-A 89 (sm?) 150 (non voc) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) + שערורהIV II (bla bla (f)) | 150 >\"\n",
    "processed_sample = [custom_string_processor(text, custom_regex) for text in sample_texts]\n",
    "\n",
    "\n",
    "print(processed_sample)\n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        return None  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "# text = \"G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV (bla bla (f))\"#\"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"#\"93 (pm) < ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "# split_parts = split_string(text)\n",
    "# if split_parts:\n",
    "#     print(\"witnesses:\", split_parts[0])\n",
    "#     print(\"reading:\", split_parts[1])\n",
    "#     print(\"After dividers:\", split_parts[2])\n",
    "# else:\n",
    "#     print(\"No dividers found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a5b4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_apparatus_entry(entry):\n",
    "    \"\"\"Parse an apparatus entry into lemma(s) and content.\"\"\"\n",
    "    parts = entry.split(']')\n",
    "    lemmas_contents = []\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            lemma, content = part.split('[', 1) if '[' in part else (part, '')\n",
    "            lemmas_contents.append((lemma.strip(), content.strip()))\n",
    "    return lemmas_contents\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    \"\"\"Create a TEI document from apparatus lines.\"\"\"\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    NSMAP = {\"tei\": TEI_NAMESPACE}\n",
    "    \n",
    "    tei_root = ET.Element(TEI+\"TEI\", nsmap=NSMAP)\n",
    "    tei_header = ET.SubElement(tei_root, TEI+\"teiHeader\")\n",
    "    text = ET.SubElement(tei_root, TEI+\"text\")\n",
    "    body = ET.SubElement(text, TEI+\"body\")\n",
    "    current_chapter = None\n",
    "    last_verse = None\n",
    "    \n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('Chapter'):\n",
    "            chapter_number = line.split(' ')[1]\n",
    "            current_chapter = ET.SubElement(body, TEI+\"div\", type=\"chapter\", n=chapter_number)\n",
    "            last_verse = None\n",
    "            continue\n",
    "        # Use regex to check if the line starts with a verse number and capture it\n",
    "        match = re.match(r\"^(\\d+)\\s*(.*)\", line)\n",
    "        if match:\n",
    "            verse_number, entry = match.groups()\n",
    "            last_verse = verse_number\n",
    "        else:\n",
    "            entry = line\n",
    "            verse_number = last_verse\n",
    "        \n",
    "        if current_chapter is not None and verse_number:\n",
    "            lemmas_contents = parse_apparatus_entry(entry)\n",
    "            for lemma, content in lemmas_contents:\n",
    "                app = ET.SubElement(current_chapter, TEI+\"app\")\n",
    "                lem = ET.SubElement(app, TEI+\"lem\", n=verse_number)\n",
    "                lem.text = lemma\n",
    "                if content:\n",
    "                    rdg = ET.SubElement(app, TEI+\"rdg\")\n",
    "                    rdg.text = content\n",
    "\n",
    "    return ET.ElementTree(tei_root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    \"\"\"Save the TEI XML tree to a file.\"\"\"\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\", short_empty_elements=True)\n",
    "\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caaa3e86",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line does not conform to expected format: ﻿App III: Hosea\n",
      "Line does not conform to expected format: יחזקיה] 30 93 (pm) 96 יחזקיהו\n",
      "Line does not conform to expected format: ירבעם בן] 30 + נבט (non voc)\n",
      "Line does not conform to expected format: לו] 96 >I  II IV\n",
      "Line does not conform to expected format: ממלכוּת] 96 ממלכוֹת\n",
      "Line does not conform to expected format: יזרעאל] 150 ישראל (parall; but 150-Tg: יזרעאל)\n",
      "Line does not conform to expected format: כי] 93 (pm) + את\n",
      "Line does not conform to expected format: אוסיף] 150 (pm) >\n",
      "Line does not conform to expected format: את] 93 (pm) >\n",
      "Line does not conform to expected format: בסוסים] 96 ובסוסיםI IV\n",
      "Line does not conform to expected format: אשר2] 96 + לא\n",
      "Line does not conform to expected format: אחד] 30 (pm) 150 (pm) >\n",
      "Line does not conform to expected format: והצגתיה] 93 (pm) + כיום ערומה והצגתיה\n",
      "Line does not conform to expected format: ושתִּה] 150 (pm) ושמתיה\n",
      "Line does not conform to expected format: כי] 150 (non voc) + כי\n",
      "Line does not conform to expected format: שכְחה] 150 (pm) שכחת\n",
      "Line does not conform to expected format: מפתיה] 89 מְפַּתֶיהָ\n",
      "Line does not conform to expected format: המדבר] 96 המדברה (similarly SifreDeut 313 (356:6), RuthR 5:6)\n",
      "Line does not conform to expected format: תקראי1] 150 (pm) תקראו | 30 + לי (non voc)I II\n",
      "Line does not conform to expected format: לי] G-B Eb 54 (pm) >II\n",
      "Line does not conform to expected format: עוד] 30 (pm?) >\n",
      "Line does not conform to expected format: את] 30 89 (sm) 93 (pm) אניI II IV\n",
      "Line does not conform to expected format: לא] 96 ולאI\n",
      "Line does not conform to expected format: תזני] 93 (pm) תתי (Caused by ligature ז+נ)\n",
      "Line does not conform to expected format: אליך] 93 (pm) אלהיך\n",
      "Line does not conform to expected format: וגם] 30 (pm) וכל\n",
      "Line does not conform to expected format: אתה] 96 (pm) את\n",
      "Line does not conform to expected format: k ואמאסאך / q ואמאסך] 30 93 (sm) 96 150 (pm) q IV | 93 (pm) אמסך\n",
      "Line does not conform to expected format: ותשכח] 150 (pm) תשכחי\n",
      "Line does not conform to expected format: נפשו] 89 (pm) 96 (pm) 150 (pm) נפשםI IV\n",
      "Line does not conform to expected format: עליו] 96 + ודמו (non voc)\n",
      "Line does not conform to expected format: דרכיו] 30 (pm) כדרכיו\n",
      "Line does not conform to expected format: הזנו] 150 (pm) והזנו\n",
      "Line does not conform to expected format: כי] 96 >\n",
      "Line does not conform to expected format: צלה] 93 (pm) יגלה\n",
      "Line does not conform to expected format: וכלותיכם] 150 (pm) + כי\n",
      "Line does not conform to expected format: הזֹנות] 93 (pm) זנות\n",
      "Line does not conform to expected format: יְפָרדו] 93 96 יִפָּרדו\n",
      "Line does not conform to expected format: ואל1] 150 (pm) אלI IV\n",
      "Line does not conform to expected format: ואַל3] 30 ואֵל\n",
      "Line does not conform to expected format: אהבו] 93 (pm) אתם\n",
      "Line does not conform to expected format: קלון] 150 (pm) קלו\n",
      "Line does not conform to expected format: מגניה] 96 (pm) מקלון\n",
      "Line does not conform to expected format: מִזִּבְחותם] 30 93 150 מִזְבְּחתםI IV | 96 מִזְבְחותם\n",
      "Line does not conform to expected format: אל] 93 (pm) + אלI IV\n",
      "Line does not conform to expected format: בקרבם] 96 (pm) בקרבכם\n",
      "Line does not conform to expected format: עמם] 93 (pm) עמכם\n",
      "Line does not conform to expected format: את] 93 + דבר (non voc)II IV\n",
      "Line does not conform to expected format: עתה] 93 (pm) ועתה\n",
      "Line does not conform to expected format: את] 30 (pm) >\n",
      "Line does not conform to expected format: בית] 150 בין (similarly b. R.HaŠanams 32b)\n",
      "Line does not conform to expected format: אחריך] 93 (pm) >\n",
      "Line does not conform to expected format: גבול] 93 + עולם (non voc) (cf גבול עולם Prov 2228 2310)\n",
      "Line does not conform to expected format: וְכָרקב] 93 96 וּכְרקב\n",
      "Line does not conform to expected format: וישלח] 93 מ..\n",
      "Line does not conform to expected format: מיֹמים] 30 (pm?) >\n",
      "Line does not conform to expected format: כמלקוש] 150 (pm) ומלקושI II IV\n",
      "Line does not conform to expected format: מה2] 30 93 150 (pm) ומהI\n",
      "Line does not conform to expected format: לְךָ2] 96 לָךְ | 30 (pm) + אפרים\n",
      "Line does not conform to expected format: וחסדכם] 93 150 חסדכםI\n",
      "Line does not conform to expected format: הֹלך] 96 והולךI\n",
      "Line does not conform to expected format: חבר] 93 (pm) וחבר\n",
      "Line does not conform to expected format: שׁם] 96 שׂם\n",
      "Line does not conform to expected format: וגנב] 30 וכגנב\n",
      "Line does not conform to expected format: פשט] 93 (pm) ופשטI (similarly S.Eli.R 22 (125))\n",
      "Line does not conform to expected format: בחוץ] 96 (pm) בחרץ\n",
      "Line does not conform to expected format: מעיר] 96 (sm) עיר\n",
      "Line does not conform to expected format: מלוש] 150 (pm) בלוש\n",
      "Line does not conform to expected format: שרים] 30 (pm) >\n",
      "Line does not conform to expected format: ידו את] 30 ~\n",
      "Line does not conform to expected format: אֹפֵהֶם] 93 (pm) אפריםI\n",
      "Line does not conform to expected format: לב] 89 (pm) לבי\n",
      "Line does not conform to expected format: איסירם] 30 89 איסרם | 96 (pm) אייסרם, (sm) אייסירים | 150 (pm) אסירים\n",
      "Line does not conform to expected format: פשעוּ] 96 פשעִי\n",
      "Line does not conform to expected format: ואנכי] 150 (pm) ואניIV\n",
      "Line does not conform to expected format: דברו] 93 (pm) >\n",
      "Line does not conform to expected format: בלבם] 93 (pm) בלבבםIV\n",
      "Line does not conform to expected format: יתגוררו] 93 (pm) >\n",
      "Line does not conform to expected format: על היו] 150 (pm) ~\n",
      "Line does not conform to expected format: זוֹ] 96 זוּ\n",
      "Line does not conform to expected format: עברו] 96 (pm) עבר\n",
      "Line does not conform to expected format: תורָתי] 89 (pm) תורֹתי\n",
      "Line does not conform to expected format: בו] 96 בֹה\n",
      "Line does not conform to expected format: שרים] 96 150 ושריםI II IV\n",
      "Line does not conform to expected format: אפרים] 30 (pm) ישראל\n",
      "Line does not conform to expected format: לו] 93 (pm) לי\n",
      "Line does not conform to expected format: k רבו / q רבי] 30 89 93 96 q, 150 (pm) רובו\n",
      "Line does not conform to expected format: יהוה] 150 (pm) ויהוהI IV\n",
      "Line does not conform to expected format: עתה] 150 (pm) ועתה\n",
      "Line does not conform to expected format: חטֹאותם] 89 93 96 150 (pm) חטאתם, 30 חטָאתם\n",
      "Line does not conform to expected format: המה] 150 (pm) והמהIV\n",
      "Line does not conform to expected format: הרבה] 30 (pm) + מזבחות\n",
      "Line does not conform to expected format: ואכלה] 150 (pm) + כל\n",
      "Line does not conform to expected format: ארמנתיה] 93 (pm) ארמנותיו\n",
      "Line does not conform to expected format: בית] 93 (pm) >\n",
      "Line does not conform to expected format: לכספם] 150 לנפשם\n",
      "Line does not conform to expected format: אויל] 96 (pm) >\n",
      "Line does not conform to expected format: רֹב] 96 רַב\n",
      "Line does not conform to expected format: עונְךָ] 96 עונֵךְ\n",
      "Line does not conform to expected format: ורבה] 150 (pm) רבהIV\n",
      "Line does not conform to expected format: נביא] 96 (pm) הנביא\n",
      "Line does not conform to expected format: יקוֹש] 93 96 יקוּש\n",
      "Line does not conform to expected format: חטאותם] 89 93 150 (pm) חטֹאתם, 30 חטָאתם\n",
      "Line does not conform to expected format: וַינזרו] 30 וְינזרו\n",
      "Line does not conform to expected format: ויהיו] 96 (pm) ויהי\n",
      "Line does not conform to expected format: רחם] 96 (pm) מרחם\n",
      "Line does not conform to expected format: מביתי] 93 (pm) מביתIV\n",
      "Line does not conform to expected format: ילדון] 93 (pm) ילזון\n",
      "Line does not conform to expected format: לו] 150 >\n",
      "Line does not conform to expected format: למזבחות] 93 (pm) למזבח\n",
      "Line does not conform to expected format: יָרֵאנו] 30 יַרְאֵנו\n",
      "Line does not conform to expected format: כרֹת] 96 (pm) וכרותI\n",
      "Line does not conform to expected format: ופרח] 150 (pm) ופתח\n",
      "Line does not conform to expected format: וכְמריו] G-B Msr 34 כֹ\n",
      "Line does not conform to expected format: יגילו] 150 (sm) יגלו\n",
      "Line does not conform to expected format: על] 93 (pm) ועל\n",
      "Line does not conform to expected format: ממנו] 96 (pm) ממני\n",
      "Line does not conform to expected format: מלכהּ] 93 (pm) מלכם\n",
      "Line does not conform to expected format: מים] 93 (pm) המים\n",
      "Line does not conform to expected format: כַּסונו] 89 כִּסונו (?)\n",
      "Line does not conform to expected format: חטאתָ] 93 (sm) 96 (sm) חטא (ת non voc)\n",
      "Line does not conform to expected format: עלוה] 89 (sm) עולהI\n",
      "Line does not conform to expected format: k עינתם / q עונֹתם] 30 k, 89 G-B Eb 16 q, 93 96 150 (pm) עונותם\n",
      "Line does not conform to expected format: ועת] 150 (pm) עת\n",
      "Line does not conform to expected format: לדרוש] 96 >\n",
      "Line does not conform to expected format: את] 30 (pm) >, 96 (non voc)\n",
      "Line does not conform to expected format: יהוה] 93 (pm) יהודה\n",
      "Line does not conform to expected format: ויֹרה] 93 (pm) G-B Eb 16 יורה\n",
      "Line does not conform to expected format: שלמן] 96 שלומך\n",
      "Line does not conform to expected format: ארבֵאל] 93 ארבְּאֵל\n",
      "Line does not conform to expected format: ולפסִלים] 96 לפסילים\n",
      "Line does not conform to expected format: רפאתים] 96 (pm) רפאתיו\n",
      "Line does not conform to expected format: לחֵיהם] 30 93 לחָיֵיהם\n",
      "Line does not conform to expected format: ממֹעצותיהם] 96 (pm) ממעֲוצותיהם\n",
      "Line does not conform to expected format: למשובתי] 30 (pm) למשבתוI\n",
      "Line does not conform to expected format: ולא2] 93 (sm) 150 (pm) לאII IV\n",
      "Line does not conform to expected format: ושֹׁד] 93 (pm) >\n",
      "Line does not conform to expected format: וברית] 96 (pm) ובריית\n",
      "Line does not conform to expected format: ישיב] 30 (pm) 150 (pm) אשיבI\n",
      "Line does not conform to expected format: בגלגל] 150 (pm) ובגלגל\n",
      "Line does not conform to expected format: זבחו] 93 (pm) >\n",
      "Line does not conform to expected format: גם מזבחותם] 96 >\n",
      "Line does not conform to expected format: ויושיעֲך] 96 (pm) ויושיעוך\n",
      "Line does not conform to expected format: לי] 96 לנו\n",
      "Line does not conform to expected format: הוא] 93 (pm) 150 (pm) והואIV\n",
      "Line does not conform to expected format: לא1] 93 ולא\n",
      "Line does not conform to expected format: דבריך] 30 93 (pm) 96 דברךI II IV\n",
      "Line does not conform to expected format: נֹחם] G-B Msr 34 נַחֵם (taken as infinitive, see Yeivin, Babylonian Vocalization, 1:542)\n",
      "Line does not conform to expected format: ויבוש] 93 (pm) תיבש\n",
      "Line does not conform to expected format: הוא2] 93 (pm) 96 150 (pm) והואI II\n",
      "Line does not conform to expected format: אמרו] 30 (pm) 93 (pm) 96 150 (pm) ואמרוII IV\n",
      "Line does not conform to expected format: כל] 89 (pm) בלI\n",
      "Line does not conform to expected format: וצדקים] 96 צדיקים\n",
      "Line does not conform to expected format: ‏App III: Hosea\n",
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    \n",
    "    tei_root = ET.Element(TEI + \"TEI\", xmlns=TEI_NAMESPACE)\n",
    "    tei_header = ET.SubElement(tei_root, TEI + \"teiHeader\")\n",
    "    text = ET.SubElement(tei_root, TEI + \"text\")\n",
    "    body = ET.SubElement(text, TEI + \"body\")\n",
    "    current_chapter = None\n",
    "    last_verse_number = None\n",
    "    \n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('Chapter'):\n",
    "            chapter_number = line.split(' ')[1].strip()\n",
    "            current_chapter = ET.SubElement(body, TEI + \"div\", type=\"chapter\", n=chapter_number)\n",
    "        else:\n",
    "            # Attempt to extract verse number and lemma content\n",
    "            parts = re.match(r\"^(\\d+)\\s*(.*)\", line)\n",
    "            if parts:\n",
    "                verse_number, remainder = parts.groups()\n",
    "                last_verse_number = verse_number  # Update last verse number with current\n",
    "                \n",
    "                # Further split to separate lemma from variants, if present\n",
    "                lemma_section, variants_section = remainder.split(']', 1) if ']' in remainder else (remainder, \"\")\n",
    "                lemma_section = lemma_section.strip()\n",
    "                variants_section = variants_section.strip()\n",
    "\n",
    "                if current_chapter is not None and verse_number:\n",
    "                    # Create an apparatus entry for the lemma\n",
    "                    app = ET.SubElement(current_chapter, TEI + \"app\")\n",
    "                    lem = ET.SubElement(app, TEI + \"lem\", n=verse_number)\n",
    "                    lem.text = lemma_section\n",
    "                    \n",
    "                    # Add variant readings if present\n",
    "                    if variants_section:\n",
    "                        rdg = ET.SubElement(app, TEI + \"rdg\")\n",
    "                        rdg.text = variants_section\n",
    "            else:\n",
    "                print(f\"Line does not conform to expected format: {line}\")\n",
    "\n",
    "    return ET.ElementTree(tei_root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\", short_empty_elements=True)\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808c9cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "def create_apparatus_entry(verse_number, content, TEI):\n",
    "    \"\"\"Create TEI element for an apparatus entry.\"\"\"\n",
    "    app = ET.Element(TEI + \"app\")\n",
    "    \n",
    "    # Extract lemma text and the rest (witnesses, variant reading, and comments)\n",
    "    lemma_text, _, rest = content.partition(']')\n",
    "    lem = ET.SubElement(app, TEI + \"lem\")\n",
    "    lem.text = lemma_text.strip()\n",
    "    \n",
    "    # Extract comments\n",
    "    comments = re.findall(r'\\((.*?)\\)', rest)\n",
    "    for comment in comments:\n",
    "        note = ET.SubElement(app, TEI + \"note\")\n",
    "        note.text = comment\n",
    "    \n",
    "    # Remove comments from rest for further processing\n",
    "    rest = re.sub(r'\\(.*?\\)', '', rest).strip()\n",
    "    \n",
    "    # Extract and process witnesses and cross-references\n",
    "    if rest:\n",
    "        rdg = ET.SubElement(app, TEI + \"rdg\")\n",
    "        witnesses, _, variant_reading = rest.partition(' ')\n",
    "        if witnesses:\n",
    "            rdg.set('wit', witnesses.strip())\n",
    "        if variant_reading:\n",
    "            rdg.text = variant_reading.strip()\n",
    "        \n",
    "        # Extract cross-references, assuming they are indicated by Roman numerals at the start\n",
    "        cross_refs = re.findall(r'\\bI{1,3}V?|\\bIV', rest)\n",
    "        for ref in cross_refs:\n",
    "            ref_element = ET.SubElement(rdg, TEI + \"ref\")\n",
    "            ref_element.set('target', '#' + ref)  # Assuming target IDs are prefixed with '#'\n",
    "            ref_element.text = \"See apparatus entry \" + ref\n",
    "    \n",
    "    return app\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    root = ET.Element(TEI + \"TEI\", xmlns=TEI_NAMESPACE)\n",
    "    header = ET.SubElement(root, TEI + \"teiHeader\")\n",
    "    text = ET.SubElement(root, TEI + \"text\")\n",
    "    body = ET.SubElement(text, TEI + \"body\")\n",
    "    div = ET.SubElement(body, TEI + \"div\")\n",
    "    \n",
    "    last_verse_number = None\n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Determine if the line starts with a verse number\n",
    "        match = re.match(r'^(\\d+)', line)\n",
    "        if match:\n",
    "            last_verse_number = match.group(1)\n",
    "            content = line[len(last_verse_number):].strip()\n",
    "        else:\n",
    "            content = line\n",
    "        \n",
    "        if last_verse_number:\n",
    "            entry = create_apparatus_entry(last_verse_number, content, TEI)\n",
    "            div.append(entry)\n",
    "    \n",
    "    return ET.ElementTree(root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\")\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a6244acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "witnesses: G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) \n",
      "reading: > שערורה\n",
      "After dividers: IV (bla bla (f))\n"
     ]
    }
   ],
   "source": [
    "#split entry into witnesses, reading, and comments \n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        return None  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "text = \"G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV (bla bla (f))\"#\"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"#\"93 (pm) < ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "split_parts = split_string(text)\n",
    "if split_parts:\n",
    "    print(\"witnesses:\", split_parts[0])\n",
    "    print(\"reading:\", split_parts[1])\n",
    "    print(\"After dividers:\", split_parts[2])\n",
    "else:\n",
    "    print(\"No dividers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "70cc4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('G-B msr. ', '30', 'pm'), ('G-A ', '89', 'pm?'), ('', '150', 'non voc'), ('', '30', 'sm'), ('', '89', 'sm'), ('', '93', 'sm'), ('MS-G ', '150', 'pm')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "25936b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified text: (See b. R.HaŠanamss 23b, LamR Buber 1:16 (40b))\n",
      "Numerals found: ['II', 'IV']\n"
     ]
    }
   ],
   "source": [
    "#parse comments\n",
    "import re\n",
    "\n",
    "def remove_and_list_roman_numerals(text):\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "# Example usage\n",
    "text = \"II IV (See b. R.HaŠanamss 23b, LamR Buber 1:16 (40b))\"\n",
    "result_text, numerals_found = remove_and_list_roman_numerals(text)\n",
    "print(\"Modified text:\", result_text.strip())\n",
    "print(\"Numerals found:\", numerals_found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d33f9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(entry):\n",
    "    split_parts = split_string(entry)\n",
    "    if split_parts is None:\n",
    "        return None\n",
    "    \n",
    "    witnesses, reading, comments = split_parts\n",
    "\n",
    "    structured_entry = {\n",
    "        'witnesses': [],\n",
    "        'reading': reading,\n",
    "        'comments': '',\n",
    "        'cross_references': []\n",
    "    }\n",
    "\n",
    "    for part in custom_split_string(witnesses):\n",
    "        # Assuming part[1] contains the witness number and part[0], part[2], part[3] contain additional info\n",
    "        witness_info = {\n",
    "            'n': part[1],\n",
    "            'text': f\"{part[0]}{part[1]} {part[2].strip()}{part[3]}\"\n",
    "        }\n",
    "        structured_entry['witnesses'].append(witness_info)\n",
    "\n",
    "    comments_text, numerals_found = remove_and_list_roman_numerals(comments)\n",
    "    structured_entry['comments'] = comments_text\n",
    "    structured_entry['cross_references'] = numerals_found\n",
    "\n",
    "    return structured_entry\n",
    "\n",
    "def create_apparatus_entry(verse_number, content, TEI):\n",
    "    \"\"\"Create TEI element for an apparatus entry.\"\"\"\n",
    "    TEI_ns = {'tei': TEI}  # Define the namespace dictionary if needed\n",
    "    app = ET.Element(f\"{{{TEI}}}app\")  # Using namespace in the tag\n",
    "    \n",
    "    # Extract lemma text and the rest (witnesses, variant reading, and comments)\n",
    "    lemma_text, _, rest = content.partition(']')\n",
    "    lem = ET.SubElement(app, f\"{{{TEI}}}lem\")\n",
    "    lem.text = lemma_text.strip('[] ')\n",
    "\n",
    "    structured_entry = process_entry(rest)\n",
    "    if not structured_entry:\n",
    "        return None\n",
    "\n",
    "    for witness in structured_entry['witnesses']:\n",
    "        wit_element = ET.SubElement(app, f\"{{{TEI}}}wit\", {'n': witness['n']})\n",
    "        wit_element.text = witness['text']\n",
    "    \n",
    "    rdg_element = ET.SubElement(app, f\"{{{TEI}}}rdg\")\n",
    "    rdg_element.text = structured_entry['reading']\n",
    "\n",
    "    if structured_entry['comments']:\n",
    "        comment_element = ET.SubElement(app, f\"{{{TEI}}}note\")\n",
    "        comment_element.text = structured_entry['comments']\n",
    "\n",
    "    for ref in structured_entry['cross_references']:\n",
    "        ref_element = ET.SubElement(app, f\"{{{TEI}}}ref\")\n",
    "        ref_element.text = ref\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8e18d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    ET.register_namespace('', TEI_NAMESPACE)  # Register the default namespace\n",
    "\n",
    "    # Create the root element without redundantly specifying the xmlns attribute\n",
    "    root = ET.Element(\"{%s}TEI\" % TEI_NAMESPACE)\n",
    "    header = ET.SubElement(root, \"{%s}teiHeader\" % TEI_NAMESPACE)\n",
    "    text = ET.SubElement(root, \"{%s}text\" % TEI_NAMESPACE)\n",
    "    body = ET.SubElement(text, \"{%s}body\" % TEI_NAMESPACE)\n",
    "    div = ET.SubElement(body, \"{%s}div\" % TEI_NAMESPACE)\n",
    "    \n",
    "    last_verse_number = None\n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Determine if the line starts with a verse number\n",
    "        match = re.match(r'^(\\d+)', line)\n",
    "        if match:\n",
    "            last_verse_number = match.group(1)\n",
    "            content = line[len(last_verse_number):].strip()\n",
    "        else:\n",
    "            content = line\n",
    "        \n",
    "        if last_verse_number:\n",
    "            entry = create_apparatus_entry(last_verse_number, content, TEI_NAMESPACE)\n",
    "            if entry is not None:  # Ensure entry creation was successful\n",
    "                div.append(entry)\n",
    "    \n",
    "    return ET.ElementTree(root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\")\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a4a220d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'n': '30', 'text': 'G-B msr. 30 (pm) '}, {'n': '89', 'text': 'G-A 89 (pm?) '}, {'n': '150', 'text': '150 (non voc) k'}, {'n': '30', 'text': ' 30 (sm) '}, {'n': '89', 'text': '89 (sm) '}, {'n': '93', 'text': '93 (sm) '}, {'n': '96', 'text': '96 '}, {'n': '150', 'text': '150 (pm) q'}, {'n': '93', 'text': ' 93 (pm) '}], 'reading': '> שערורה', 'comments': '  (bla bla (f))', 'cross_references': ['IV', 'II']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<~>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def custom_split_string(text):\n",
    "    pattern = re.compile(r'([^,\\d]*?)?(\\d+)\\s?(\\([^\\)]+\\)?)?([\\skq]*)?', re.DOTALL|re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "def remove_and_list_roman_numerals(text):\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "# def process_entry(entry):\n",
    "#     split_parts = split_string(entry)\n",
    "#     if split_parts is None:\n",
    "#         return \"Unable to process entry: No valid dividers found.\"\n",
    "    \n",
    "#     witnesses, reading, comments = split_parts\n",
    "\n",
    "#     witness_entries = []\n",
    "#     for part in custom_split_string(witnesses):\n",
    "#         witness_entry = f'<witness n=\"{part[1]}\">{part[0]}{part[1]} {part[2].strip()}{part[3]}</witness>'\n",
    "#         witness_entries.append(witness_entry)\n",
    "#     witnesses_tagged = \"\\n\".join(witness_entries)\n",
    "\n",
    "#     reading_tagged = f'<reading>{reading}</reading>'\n",
    "\n",
    "#     comments_text, numerals_found = remove_and_list_roman_numerals(comments)\n",
    "#     comments_tagged = f'<comment>{comments_text}</comment>'\n",
    "#     cross_references = \"\\n\".join([f'<ref>{numeral}</ref>' for numeral in numerals_found])\n",
    "\n",
    "#     # Combine all parts, placing cross_references outside the comment\n",
    "#     tei_entry = f\"{witnesses_tagged}\\n{reading_tagged}\\n{comments_tagged}\\n{cross_references}\"\n",
    "#     return tei_entry\n",
    "\n",
    "# Example usage\n",
    "entry =\"G-B msr. 30 (pm) G-A 89 (pm?) 150 (non voc) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV II (bla bla (f))\"\n",
    "processed_entry = process_entry(entry)\n",
    "print(processed_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12325297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'93 (pm) + ביהושעIV (similarly PesiqtaR 33 (153b))'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest = \"93 (pm) + ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caa1d542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tei_hebrew_output_enhanced.xml'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def strip_non_hebrew(word):\n",
    "    normalized_word = unicodedata.normalize('NFD', word)\n",
    "    stripped_word = ''.join(re.findall(r'[\\u05D0-\\u05EA]', normalized_word))\n",
    "    return unicodedata.normalize('NFC', stripped_word)\n",
    "\n",
    "def process_word(token, verse_id, word_id, parent_element):\n",
    "    parts = token.split('־')\n",
    "    pe_count = 1  # Counter for 'פ' tags\n",
    "\n",
    "    for part in parts:\n",
    "        w = ET.SubElement(parent_element, 'w', id=f'verse{verse_id}_word{word_id}')\n",
    "\n",
    "        alphabetic = strip_non_hebrew(part)\n",
    "        non_alphabetic = ''.join(re.findall(r'[^\\u05D0-\\u05EA]', part))\n",
    "\n",
    "        original = ET.SubElement(w, 'original')\n",
    "        original.text = part\n",
    "        stripped = ET.SubElement(w, 'stripped')\n",
    "        stripped.text = alphabetic\n",
    "        punctuation = ET.SubElement(w, 'punctuation')\n",
    "        punctuation.text = non_alphabetic\n",
    "\n",
    "        if \"פ\" in part:\n",
    "            pe_tag = ET.SubElement(w, 'pe', id=f'verse{verse_id}_pe{pe_count}')\n",
    "            pe_tag.text = \"פ\"\n",
    "            pe_count += 1\n",
    "        \n",
    "        word_id += 1\n",
    "    return word_id\n",
    "\n",
    "def encode_tei_hebrew_word_details_enhanced(file_path, output_file):\n",
    "    text = read_text_from_file(file_path)\n",
    "    TEI = ET.Element('TEI', xmlns='http://www.tei-c.org/ns/1.0')\n",
    "    text_element = ET.SubElement(TEI, 'text')\n",
    "    body = ET.SubElement(text_element, 'body')\n",
    "\n",
    "    chapter_id = 1\n",
    "    verse_id = 1\n",
    "\n",
    "    chapters = text.split('פרק')\n",
    "    for chapter in chapters[1:]:\n",
    "        div = ET.SubElement(body, 'div', type='chapter', id=f'chapter{chapter_id}')\n",
    "        chapter_id += 1\n",
    "\n",
    "        verses = re.split(r'(\\[\\פ\\]|:)', chapter)\n",
    "        for verse in verses:\n",
    "            if verse.strip() and verse not in ['[פ]', ':']:\n",
    "                p = ET.SubElement(div, 'p', type='verse', id=f'verse{verse_id}')\n",
    "                word_id = 1\n",
    "\n",
    "                tokens = verse.strip().split()\n",
    "                for token in tokens:\n",
    "                    word_id = process_word(token, verse_id, word_id, p)\n",
    "\n",
    "                verse_id += 1\n",
    "\n",
    "    tree = ET.ElementTree(TEI)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        tree.write(f, encoding=\"unicode\")\n",
    "\n",
    "# Specify the file paths\n",
    "file_path = 'file.txt'  # Replace with your input file path\n",
    "output_file = 'tei_hebrew_output_enhanced.xml'  # Replace with your output file path\n",
    "\n",
    "# Run the function\n",
    "encode_tei_hebrew_word_details_enhanced(file_path, output_file)\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[' ', '\"', '$', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', 'E', 'I', 'T', '_', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', '֑', '֔', '֕', '֖', '֗', '֙', '֛', '֜', '֞', '֣', '֤', '֥', '֨', '֩', 'ְ', 'ֱ', 'ֲ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֻ', 'ּ', 'ֽ', '׀', 'ׁ', 'ׂ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee3cf327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '$', '$1', '$2', '$4', '2', ':', '[', ']', '֑', '֔', '֕', '֖', '֗', '֙', '֜', '֣', '֤', '֥', '֥$', '֨', '֩', 'ְ', 'ְ$', 'ְ֙', 'ְּ', 'ְׁ', 'ְׂ', 'ֱ', 'ֲ', 'ִ', 'ִ$', 'ִ֔', 'ִ֖', 'ִ֜', 'ִ֨', 'ִּ', 'ִֽ', 'ִׁ', 'ֵ', 'ֵ$', 'ֵ֔', 'ֵ֖', 'ֵ֗', 'ֵ֛', 'ֵ֣', 'ֵ֤', 'ֵ֨', 'ֵּ', 'ֵֽ', 'ֵׁ', 'ֶ', 'ֶ֑', 'ֶ֙', 'ֶ֣', 'ֶ֤', 'ֶ֥', 'ֶּ', 'ֶֽ', 'ֶׁ', 'ַ', 'ַ֗', 'ַ֙', 'ַּ', 'ַׁ', 'ָ', 'ָ֑', 'ָ֔', 'ָ֖', 'ָ֗', 'ָ֛', 'ָ֜', 'ָ֞', 'ָ֣', 'ָ֥', 'ָ֨', 'ָּ', 'ָֽ', 'ֹ', 'ֹ֖', 'ֹ֣', 'ֹ֤', 'ֹ֨', 'ֹּ', 'ֹׂ', 'ֻ', 'ּ', 'ּ֣', 'ֽ', '־', '־$', '׀', 'ׁ', 'ׂ֖', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "def extract_consecutive_non_hebrew_groups(file_path):\n",
    "    text = read_text_from_file(file_path)\n",
    "    non_hebrew_groups = set()\n",
    "\n",
    "    # Using a regular expression to find sequences of non-Hebrew characters\n",
    "    pattern = re.compile(r'([^\\u05D0-\\u05EA]{,2})')\n",
    "    matches = pattern.findall(unicodedata.normalize('NFD', text))\n",
    "\n",
    "    for match in matches:\n",
    "        non_hebrew_groups.add(match.strip())\n",
    "\n",
    "    return non_hebrew_groups\n",
    "\n",
    "# Extract and print groups of consecutive non-Hebrew characters\n",
    "file_path = 'file.txt'\n",
    "\n",
    "consecutive_non_hebrew_groups = extract_consecutive_non_hebrew_groups(file_path)\n",
    "print(sorted(consecutive_non_hebrew_groups))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558de29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from docx import Document\n",
    "from lxml import etree\n",
    "import zipfile\n",
    "import os\n",
    "from win32com import client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a470ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_footnotes(docx_path):\n",
    "    # Dictionary to store footnote data\n",
    "    footnotes_dict = {}\n",
    "\n",
    "    # Step 1: Extract the footnotes XML from the .docx file\n",
    "    with zipfile.ZipFile(docx_path, 'r') as docx:\n",
    "        # Look for footnotes XML part\n",
    "        if 'word/footnotes.xml' in docx.namelist():\n",
    "            footnote_xml = docx.read('word/footnotes.xml').decode('utf-8')\n",
    "        else:\n",
    "            print(\"No footnotes.xml found in this document.\")\n",
    "            return footnotes_dict\n",
    "\n",
    "    # Step 2: Parse the footnote XML and store in dictionary\n",
    "    if footnote_xml:\n",
    "        root = ET.fromstring(footnote_xml)\n",
    "        namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "        \n",
    "        non_blank_count = 1  # Start counting footnotes for non-blank lines only\n",
    "\n",
    "        for footnote in root.findall('w:footnote', namespaces):\n",
    "            footnote_text = \"\"\n",
    "\n",
    "            # Extract each run's text, font, and superscript/subscript information in the footnote\n",
    "            for run in footnote.findall('.//w:r', namespaces):\n",
    "                text_elem = run.find('w:t', namespaces)\n",
    "                font_elem = run.find('.//w:rPr//w:rFonts', namespaces)\n",
    "                vert_align_elem = run.find('.//w:rPr//w:vertAlign', namespaces)\n",
    "\n",
    "                if text_elem is not None:\n",
    "                    text = text_elem.text\n",
    "                    font = font_elem.get(f'{{{namespaces[\"w\"]}}}ascii') if font_elem is not None else \"Unknown\"\n",
    "\n",
    "                    # Check for superscript or subscript alignment\n",
    "                    if vert_align_elem is not None:\n",
    "                        align_val = vert_align_elem.get(f'{{{namespaces[\"w\"]}}}val')\n",
    "                        if align_val == \"superscript\":\n",
    "                            text = f\"<superscript {text} >\"\n",
    "                        elif align_val == \"subscript\":\n",
    "                            text = f\"<subscript {text} >\"\n",
    "\n",
    "                    # Wrap the text in <specialFont ...> if it’s in the special font\n",
    "                    if font == \"HUBPSigla\":  # Replace with your actual font name if different\n",
    "                        footnote_text += f\"<specialFont {text} >\"\n",
    "                    else:\n",
    "                        footnote_text += text\n",
    "\n",
    "            # Only store non-blank footnotes in the dictionary\n",
    "            if footnote_text.strip():  # Check if footnote text is non-blank\n",
    "                footnotes_dict[f'footnote-{non_blank_count}'] = footnote_text.strip()\n",
    "                non_blank_count += 1  # Increment count only for non-blank footnotes\n",
    "\n",
    "    return footnotes_dict\n",
    "\n",
    "\n",
    "def process_main_text_with_normalized_footnotes(docx_path):\n",
    "    document_xml = None\n",
    "\n",
    "    # Step 1: Extract and normalize valid footnotes\n",
    "    normalized_footnotes = {}  # Map of normalized ID -> actual footnote text\n",
    "    id_mapping = {}  # Map of actual ID -> normalized ID\n",
    "    with zipfile.ZipFile(docx_path, 'r') as docx:\n",
    "        if 'word/footnotes.xml' in docx.namelist():\n",
    "            footnotes_xml = docx.read('word/footnotes.xml').decode('utf-8')\n",
    "            root = ET.fromstring(footnotes_xml)\n",
    "            namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "\n",
    "            normalized_id = 1\n",
    "            for footnote in root.findall('w:footnote', namespaces):\n",
    "                actual_id = int(footnote.get(f'{{{namespaces[\"w\"]}}}id'))\n",
    "                footnote_type = footnote.get(f'{{{namespaces[\"w\"]}}}type', 'regular')\n",
    "                text = ''.join(run.text or '' for run in footnote.findall('.//w:t', namespaces))\n",
    "\n",
    "                # Include only regular footnotes with valid IDs\n",
    "                if actual_id > 0 and footnote_type == 'regular':\n",
    "                    normalized_footnotes[normalized_id] = text.strip()\n",
    "                    id_mapping[actual_id] = normalized_id\n",
    "                    normalized_id += 1\n",
    "\n",
    "    # Step 2: Process `document.xml` with normalized IDs\n",
    "    with zipfile.ZipFile(docx_path, 'r') as docx:\n",
    "        if 'word/document.xml' in docx.namelist():\n",
    "            document_xml = docx.read('word/document.xml').decode('utf-8')\n",
    "\n",
    "    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "    main_text_dict = {}\n",
    "\n",
    "    if document_xml:\n",
    "        document_root = ET.fromstring(document_xml)\n",
    "        title_and_chapter = \"\"\n",
    "        first_row_processed = False\n",
    "\n",
    "        for i, paragraph in enumerate(document_root.findall('.//w:p', namespaces), start=1):\n",
    "            paragraph_text = \"\"\n",
    "            if not first_row_processed:\n",
    "                title_and_chapter = ''.join(\n",
    "                    run.find('w:t', namespaces).text or ''\n",
    "                    for run in paragraph.findall('.//w:r', namespaces)\n",
    "                    if run.find('w:t', namespaces) is not None\n",
    "                ).strip()\n",
    "                first_row_processed = True\n",
    "                continue\n",
    "\n",
    "            if not any(run.find('w:t', namespaces) is not None for run in paragraph.findall('.//w:r', namespaces)):\n",
    "                continue\n",
    "\n",
    "            app_id = f\"app-{i-2}\"\n",
    "            combined_special_fonts = []  # To collect specialFont text for merging\n",
    "\n",
    "            for run in paragraph.findall('.//w:r', namespaces):\n",
    "                text_elem = run.find('w:t', namespaces)\n",
    "                footnote_ref = run.find('.//w:footnoteReference', namespaces)\n",
    "                font_elem = run.find('.//w:rPr//w:rFonts', namespaces)\n",
    "                vert_align_elem = run.find('.//w:rPr//w:vertAlign', namespaces)\n",
    "\n",
    "                if text_elem is not None:\n",
    "                    text = text_elem.text\n",
    "                    font = font_elem.get(f'{{{namespaces[\"w\"]}}}ascii') if font_elem is not None else \"Unknown\"\n",
    "\n",
    "                    # Check for superscript or subscript alignment\n",
    "                    if vert_align_elem is not None:\n",
    "                        align_val = vert_align_elem.get(f'{{{namespaces[\"w\"]}}}val')\n",
    "                        if align_val == \"superscript\":\n",
    "                            text = f\"<superscript {text}>\"\n",
    "                        elif align_val == \"subscript\":\n",
    "                            text = f\"<subscript {text}>\"\n",
    "\n",
    "                    # Collect special font text instead of appending immediately\n",
    "                    if font == \"HUBPSigla\":  # Replace with your actual font name if different\n",
    "                        combined_special_fonts.append(text)\n",
    "                    else:\n",
    "                        # If there's non-specialFont text, finalize combined_special_fonts\n",
    "                        if combined_special_fonts:\n",
    "                            paragraph_text += f\"<specialFont {''.join(combined_special_fonts)}>\"\n",
    "                            combined_special_fonts = []  # Reset buffer\n",
    "                        paragraph_text += text\n",
    "\n",
    "                elif footnote_ref is not None:\n",
    "                    actual_id = int(footnote_ref.get(f'{{{namespaces[\"w\"]}}}id'))\n",
    "                    # Use the normalized ID if the actual ID exists in the mapping\n",
    "                    if actual_id in id_mapping:\n",
    "                        normalized_id = id_mapping[actual_id]\n",
    "                        paragraph_text += f\"<ref {normalized_id}>\"\n",
    "\n",
    "            # Finalize any remaining combined_special_fonts\n",
    "            if combined_special_fonts:\n",
    "                paragraph_text += f\"<specialFont {''.join(combined_special_fonts)}>\"\n",
    "\n",
    "            main_text_dict[app_id] = paragraph_text\n",
    "\n",
    "    return {\n",
    "        'title': title_and_chapter.split()[0],\n",
    "        'chapter': title_and_chapter.split()[1],\n",
    "        'content': main_text_dict\n",
    "    }\n",
    "\n",
    "def process_lemma_with_superscript(lemma_text):\n",
    "    # Regex to match verse, lemma, and superscript number\n",
    "    pattern = r'^(\\d+)\\s+([^\\s<]+)(?:<superscript\\s*(\\d+)>)?'\n",
    "    \n",
    "    # Match the pattern\n",
    "    match = re.match(pattern, lemma_text)\n",
    "    if not match:\n",
    "        return None  # Return None if the pattern doesn't match\n",
    "    \n",
    "    # Extract verse, lemma, and superscript number\n",
    "    verse = int(match.group(1))\n",
    "    lemma = match.group(2)\n",
    "    number = int(match.group(3)) if match.group(3) else None\n",
    "    \n",
    "    # Return structured data\n",
    "    return {\n",
    "        'verse': verse,\n",
    "        'lemma': lemma,\n",
    "        'number': number\n",
    "    }\n",
    "\n",
    "def process_single_lemma_with_range(lemma_text):\n",
    "    \"\"\"\n",
    "    Process a single lemma unit, checking first for ranges and then tagging appropriately.\n",
    "    \"\"\"\n",
    "    # Check if there's a range (– or -)\n",
    "    if '–' in lemma_text or '-' in lemma_text:\n",
    "        delimiter = '–' if '–' in lemma_text else '-'\n",
    "        parts = lemma_text.split(delimiter, 1)\n",
    "        part_from = parts[0].strip()\n",
    "        part_to = parts[1].strip()\n",
    "\n",
    "        return [\n",
    "            {'tag': 'from lemma', 'content': part_from},\n",
    "            {'delimiter': delimiter, 'type': 'range'},\n",
    "            {'tag': 'to lemma', 'content': part_to}\n",
    "        ]\n",
    "\n",
    "    # If no range, split and tag as lemma 1, lemma 2, etc.\n",
    "    lemma_parts = lemma_text.split()\n",
    "    structured_parts = []\n",
    "    for i, part in enumerate(lemma_parts, start=1):\n",
    "        structured_parts.append({'tag': f'lemma {i}', 'content': part})\n",
    "\n",
    "    return structured_parts\n",
    "\n",
    "def process_lemma_with_transposition(lemma, previous_verse=None):\n",
    "    # Pattern to identify if the lemma starts with a digit (verse number)\n",
    "    verse_pattern = re.compile(r'^\\d+')\n",
    "    verse_match = verse_pattern.match(lemma)\n",
    "\n",
    "    # Determine verse number\n",
    "    if verse_match:\n",
    "        verse_number = int(verse_match.group(0))\n",
    "        lemma_text = lemma[verse_match.end():].strip()\n",
    "    else:\n",
    "        verse_number = previous_verse\n",
    "        lemma_text = lemma.strip()\n",
    "\n",
    "    # Check for transposition (~)\n",
    "    if '~' in lemma_text:\n",
    "        parts = lemma_text.split('~', 1)\n",
    "        part_a = parts[0].strip()\n",
    "        part_b = parts[1].strip()\n",
    "\n",
    "        return {\n",
    "            'verse': verse_number,\n",
    "            'lemma_parts': [\n",
    "                {'type': 'transposition', 'part': 'a', 'details': process_single_lemma_with_range(part_a)},\n",
    "                {'type': 'transposition', 'part': 'b', 'details': process_single_lemma_with_range(part_b)}\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        # No transposition, process as a single lemma unit\n",
    "        return {\n",
    "            'verse': verse_number,\n",
    "            'lemma_parts': process_single_lemma_with_range(lemma_text)\n",
    "        }\n",
    "\n",
    "    \n",
    "def split_full_entry(text):\n",
    "    \"\"\"\n",
    "    Splits a full entry into lemma and entry. Processes sequences of <subscript> tags\n",
    "    into a single concatenated <subscript> tag.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split at the first occurrence of ']'\n",
    "    sliced_entry = text.split(sep=']', maxsplit=1)\n",
    "    \n",
    "    # Initialize `lemma` and `entry`\n",
    "    lemma = sliced_entry[0]\n",
    "    entry = sliced_entry[1] if len(sliced_entry) > 1 else \"\"\n",
    "\n",
    "    # Process subscript sequences in the entry\n",
    "    subscript_pattern = r'(?:<subscript\\s*([^>]+)>)+'\n",
    "    \n",
    "    def merge_subscripts(match):\n",
    "        subscripts = match.group(0)\n",
    "        combined_content = ''.join(re.findall(r'<subscript\\s*([^>]+)>', subscripts))\n",
    "        return f'<subscript {combined_content}>'\n",
    "\n",
    "    entry = re.sub(subscript_pattern, merge_subscripts, entry)\n",
    "\n",
    "    # Check if a superscript follows immediately after the `]`\n",
    "    superscript_pattern = re.compile(r'^\\s*<ref\\s*([^>]+)>')\n",
    "    match = superscript_pattern.match(entry)\n",
    "    lemma_dict = {'lemma': lemma}\n",
    "    if match:\n",
    "        # Append the superscript to the lemma\n",
    "        lemma_dict.update({'ref': match.group(0)})\n",
    "        \n",
    "        # Remove the superscript from the start of the entry\n",
    "        entry = entry[len(match.group(0)):].strip()\n",
    "\n",
    "    # Check for specific patterns in the lemma\n",
    "    to_check_patterns = [\n",
    "        r'<specialFont\\s*<subscript\\s*v>>',  # Pattern for subscript v\n",
    "        r'<specialFont\\s*,>'                 # Pattern for specialFont ,\n",
    "    ]\n",
    "    if any(re.search(pattern, lemma) for pattern in to_check_patterns):\n",
    "        lemma_dict['to_check'] = True  # Add \"to_check\" flag if any pattern matches\n",
    "        \n",
    "    return lemma_dict, entry\n",
    "\n",
    "\n",
    "def extract_initial_sigla(reading):\n",
    "       \n",
    "    # Pattern to match one or more sigla at the start of the reading\n",
    "    sigla_pattern = re.compile(r'^[\\+\\>\\~]+')\n",
    "    \n",
    "    # Dictionary to store extracted sigla and cleaned reading\n",
    "    result = {\n",
    "        'sigla': \"\",\n",
    "        'cleaned_reading': reading.strip()  # Initialize cleaned reading as the full reading\n",
    "    }\n",
    "    \n",
    "    # Find initial sigla if present\n",
    "    match = sigla_pattern.match(reading)\n",
    "    if match:\n",
    "        # Extract sigla and set them in the result dictionary\n",
    "        result['sigla'] = match.group(0)\n",
    "        \n",
    "        # Remove the matched sigla from the beginning of the reading\n",
    "        result['cleaned_reading'] = reading[len(result['sigla']):].strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_cross_references(reading):\n",
    "    # Pattern to find all <subscript ...> tags\n",
    "    subscript_pattern = re.compile(r'<subscript\\s*([^>]+)>')\n",
    "    \n",
    "    # List to store cross-references and clean reading with subscripts\n",
    "    cross_references = []\n",
    "    cleaned_reading = reading\n",
    "\n",
    "    # Define a pattern to identify Roman numerals with only I's and V's (e.g., I, II, III, IV, V, VI, VII, VIII)\n",
    "    roman_numeral_pattern = re.compile(r'[IV\\s]{0,}$')\n",
    "\n",
    "    # Process each subscript match\n",
    "    matches = subscript_pattern.findall(reading)\n",
    "    for match in matches:\n",
    "        if roman_numeral_pattern.match(match):\n",
    "            for item in match.split():\n",
    "                cross_references.append(item)  # Store as cross-reference if it's a valid Roman numeral with I's and V's\n",
    "            cleaned_reading = cleaned_reading.replace(f\"<subscript {match}>\", \"\")\n",
    "        \n",
    "    # Remove duplicates in cross-references and retain order\n",
    "    unique_references = sorted(set(cross_references), key=cross_references.index)\n",
    "    \n",
    "    # Final cleanup of <subscript ...> tags for Roman numerals in cleaned reading\n",
    "#     cleaned_reading = roman_numeral_pattern.sub('', cleaned_reading).strip()\n",
    "\n",
    "    return {\n",
    "        'cross_references': unique_references,\n",
    "        'reading': cleaned_reading\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_and_classify_entry(entry):\n",
    "    # Define splitting characters with classifications\n",
    "    splitters = {\n",
    "        '|': 'additional_variant',\n",
    "        '=': 'synonymous_variant',\n",
    "        ',': 'related_variant',\n",
    "        '<specialFont +>': 'similar_variant'\n",
    "    }\n",
    "    \n",
    "    special_font_pattern = re.compile(r'<specialFont\\s*([^>]+)>')\n",
    "\n",
    "    \n",
    "    # Preprocess: Replace <specialFont ...> with placeholders\n",
    "    special_font_placeholders = []\n",
    "    def replace_special_font(match):\n",
    "        special_font_placeholders.append(match.group(0))  # Store the full tag\n",
    "        return f\"__SPECIAL_FONT_{len(special_font_placeholders) - 1}__\"\n",
    "\n",
    "    entry = special_font_pattern.sub(replace_special_font, entry)\n",
    "\n",
    "    # Compile regex to split by any of the splitters\n",
    "    splitter_pattern = re.compile(r'(\\||=|<specialFont\\s\\+>|,)')\n",
    "\n",
    "    # List to store parsed entries with classifications\n",
    "    parsed_entries = []\n",
    "\n",
    "    # Split the entry by the main split characters, keeping split characters separate\n",
    "    parts = splitter_pattern.split(entry)\n",
    "\n",
    "    # Initialize a default classification for the first part\n",
    "    current_classification = \"variant\"\n",
    "\n",
    "    # Process each part separately\n",
    "    for part in parts:\n",
    "        part = part.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        # Skip if the part is a splitter, set classification for the next part\n",
    "        if part in splitters:\n",
    "            current_classification = splitters[part]\n",
    "            continue  # Skip to the next part\n",
    "\n",
    "        # Restore <specialFont ...> tags in this part\n",
    "        for i, placeholder in enumerate(special_font_placeholders):\n",
    "            part = part.replace(f\"__SPECIAL_FONT_{i}__\", placeholder)\n",
    "\n",
    "        # Initialize dictionaries to store witnesses, reading, classification, and cross-references\n",
    "        entry_data = {\n",
    "            'classification': current_classification,\n",
    "            'witnesses': [],\n",
    "            'reading': \"\",\n",
    "            'cross_references': [],\n",
    "            'sigla': \"\"\n",
    "        }\n",
    "                \n",
    "        combined_witnesses = []  # Collect all witness contents\n",
    "\n",
    "        i = 0\n",
    "        while i < len(part):\n",
    "            # Match <specialFont ...>\n",
    "            special_font_match = special_font_pattern.match(part, i)\n",
    "            if special_font_match:\n",
    "                combined_witnesses.append(special_font_match.group(1))  # Collect witness content\n",
    "                i = special_font_match.end()\n",
    "            else:\n",
    "                # Once done collecting witnesses, finalize the <specialFont> wrapper\n",
    "                if combined_witnesses:\n",
    "                    combined_witness_text = ''.join(combined_witnesses)\n",
    "                    entry_data['witnesses'].append(f\"<specialFont {combined_witness_text}>\")\n",
    "                    combined_witnesses = []  # Reset the buffer\n",
    "                # Anything else becomes the reading\n",
    "                entry_data['reading'] = part[i:].strip()\n",
    "                break\n",
    "\n",
    "        # If any witnesses remain in the buffer, finalize them\n",
    "        if combined_witnesses:\n",
    "            combined_witness_text = ''.join(combined_witnesses)\n",
    "            entry_data['witnesses'].append(f\"<specialFont {combined_witness_text}>\")\n",
    "        # process witnesses\n",
    "        entry_data['witnesses'] = process_combined_witnesses(entry_data['witnesses'])\n",
    "        \n",
    "        # Extract cross-references from the reading\n",
    "        result = extract_cross_references(entry_data['reading'])\n",
    "        entry_data['cross_references'] = result['cross_references']\n",
    "        entry_data['reading'] = result['reading']  # Cleaned reading without <subscript ...> tags\n",
    "\n",
    "        \n",
    "        sigla_result = extract_initial_sigla(entry_data['reading'])\n",
    "        entry_data['sigla'] = sigla_result['sigla']\n",
    "        entry_data['reading'] = sigla_result['cleaned_reading']  # Cleaned reading without sigla\n",
    "\n",
    "        # Add entry data to the parsed entries list\n",
    "        parsed_entries.append(entry_data)\n",
    "\n",
    "    return parsed_entries\n",
    "\n",
    "\n",
    "def split_and_process_witnesses(witness_text):\n",
    "    \"\"\"\n",
    "    Split witnesses from text, attaching 'h' or '-' to the preceding witness.\n",
    "    \"\"\"\n",
    "    processed_witnesses = []\n",
    "    \n",
    "    for char in witness_text:\n",
    "        if char in ['h', '-'] and processed_witnesses:\n",
    "            # Attach 'h' or '-' to the last witness\n",
    "            processed_witnesses[-1] += char\n",
    "        else:\n",
    "            # Start a new witness\n",
    "            processed_witnesses.append(char)\n",
    "\n",
    "    return processed_witnesses\n",
    "\n",
    "\n",
    "def process_combined_witnesses(witnesses):\n",
    "    \"\"\"\n",
    "    Process combined witnesses, splitting and wrapping them in <specialFont> tags.\n",
    "    \"\"\"\n",
    "    processed_witnesses = []\n",
    "\n",
    "    for witness in witnesses:\n",
    "        # Extract the content within <specialFont>\n",
    "        match = re.match(r'<specialFont\\s*(.*?)>', witness)\n",
    "        if match:\n",
    "            content = match.group(1)\n",
    "            # Split the content into individual witnesses\n",
    "            split_witnesses = split_and_process_witnesses(content)\n",
    "            # Wrap each witness back in <specialFont> tags\n",
    "            processed_witnesses.extend([f\"<specialFont {w}>\" for w in split_witnesses])\n",
    "        else:\n",
    "            # If no <specialFont>, just append as-is\n",
    "            processed_witnesses.append(witness)\n",
    "\n",
    "    return processed_witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6562a3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Main Text</th>\n",
       "      <th>Footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hosea</td>\n",
       "      <td>1</td>\n",
       "      <td>{'app-1': '1 יותם אחז יחזקיה] &lt;specialFont ][&gt;...</td>\n",
       "      <td>{'footnote-1': 'cf app Mic 1&lt;subscript 1 &gt;', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hosea</td>\n",
       "      <td>2</td>\n",
       "      <td>{'app-1': '1–2] &lt;specialFont ]*&gt; 1&lt;subscript 1...</td>\n",
       "      <td>{'footnote-1': '“there”, for במקום אשר ... שם ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hosea</td>\n",
       "      <td>3</td>\n",
       "      <td>{'app-1': '1 אהב] &lt;specialFont ]*h&gt; &lt;specialFo...</td>\n",
       "      <td>{'footnote-1': 'voc אֹהֶבֶת רָע; cf Mic 3&lt;subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hosea</td>\n",
       "      <td>4</td>\n",
       "      <td>{'app-0': '1 (ל)יהוה] &lt;specialFont ]h&gt; div', '...</td>\n",
       "      <td>{'footnote-1': '“(in) the fear of (God)”, theo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hosea</td>\n",
       "      <td>5</td>\n",
       "      <td>{'app-1': '1 לְמצפָּה] &lt;specialFont ]&gt; τῇ σκοπ...</td>\n",
       "      <td>{'footnote-1': '“to the lookout”, similarly &lt;s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Title  Chapter                                          Main Text  \\\n",
       "0  Hosea        1  {'app-1': '1 יותם אחז יחזקיה] <specialFont ][>...   \n",
       "1  Hosea        2  {'app-1': '1–2] <specialFont ]*> 1<subscript 1...   \n",
       "2  Hosea        3  {'app-1': '1 אהב] <specialFont ]*h> <specialFo...   \n",
       "3  Hosea        4  {'app-0': '1 (ל)יהוה] <specialFont ]h> div', '...   \n",
       "4  Hosea        5  {'app-1': '1 לְמצפָּה] <specialFont ]> τῇ σκοπ...   \n",
       "\n",
       "                                           Footnotes  \n",
       "0  {'footnote-1': 'cf app Mic 1<subscript 1 >', '...  \n",
       "1  {'footnote-1': '“there”, for במקום אשר ... שם ...  \n",
       "2  {'footnote-1': 'voc אֹהֶבֶת רָע; cf Mic 3<subs...  \n",
       "3  {'footnote-1': '“(in) the fear of (God)”, theo...  \n",
       "4  {'footnote-1': '“to the lookout”, similarly <s...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "def process_docx_files(folder_path):\n",
    "    # Initialize an empty list to collect data for each file\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each .docx file in the folder\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.docx'):\n",
    "            docx_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Extract main text and footnotes\n",
    "            main_text_data = process_main_text_with_normalized_footnotes(docx_path)\n",
    "            footnotes_data = process_footnotes(docx_path)\n",
    "            \n",
    "            # Extract title and chapter from main text data\n",
    "            title = main_text_data.get('title', 'Unknown Title')\n",
    "            chapter = main_text_data.get('chapter', 'Unknown Chapter')\n",
    "            \n",
    "            # Append to data list with structured dictionary\n",
    "            data.append({\n",
    "                'Title': title,\n",
    "                'Chapter': int(chapter),\n",
    "                'Main Text': main_text_data.get('content', {}),\n",
    "                'Footnotes': footnotes_data\n",
    "            })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data).sort_values(by=['Title', 'Chapter']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Folder path containing .docx files\n",
    "folder_path = 'Hosea.App.1'\n",
    "\n",
    "# Process and get DataFrame\n",
    "df = process_docx_files(folder_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25a3d9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“there”, for במקום אשר ... שם cf e.g. Num 9<subscript 17 > 2Sam 15<subscript 21 > Jer 22<subscript 12 >, <specialFont ] > 1Kgs 21[20]<subscript 19 >, <specialFont [ > Ezek 21<subscript 35 >'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Footnotes'][1]['footnote-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c6b56e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'יֵאָמֵר להם<superscript 2>] <specialFont ][> diath | <specialFont ]h> pr ἐκεῖ<ref 1> = <specialFont [>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Main Text'][1]['app-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d54692f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classification': 'variant',\n",
       "  'witnesses': ['<specialFont ]>', '<specialFont [>'],\n",
       "  'reading': 'diath',\n",
       "  'cross_references': [],\n",
       "  'sigla': ''},\n",
       " {'classification': 'additional_variant',\n",
       "  'witnesses': ['<specialFont ]h>'],\n",
       "  'reading': 'pr ἐκεῖ<ref 1>',\n",
       "  'cross_references': [],\n",
       "  'sigla': ''},\n",
       " {'classification': 'synonymous_variant',\n",
       "  'witnesses': ['<specialFont [>'],\n",
       "  'reading': '',\n",
       "  'cross_references': [],\n",
       "  'sigla': ''}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma, entry = split_full_entry(df['Main Text'][1]['app-4'])\n",
    "parse_and_classify_entry(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc3b36dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'יֵאָמֵר להם<superscript 2>'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2962016a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verse': {'verse': None}, 'lemma': 'יֵאָמֵר להם<superscript 2>'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_verse = split_verse_lemma(lemma['lemma'])\n",
    "lemma_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3e8373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tag': 'lemma1', 'content': 'יֵאָמֵר'},\n",
       " {'tag': 'lemma2', 'content': 'להם', 'numbers': '2'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_lemma(lemma_verse['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7c952be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'ולא יספר'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma # Ων v,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f21d0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_verse_lemma(entry_text, previous_verse=None):\n",
    "    \"\"\"\n",
    "    Splits the entry into verse and lemma. If a verse range is found, splits into from_verse and to_verse.\n",
    "    If no verse is found, it uses the previous entry's verse.\n",
    "    \"\"\"\n",
    "    # Regex to match verse patterns: digits optionally followed by a dash or range\n",
    "    pattern = re.match(r'^(\\d+(?:–\\d+)?)(.*)$', entry_text.strip())\n",
    "    if pattern:\n",
    "        verse_part = pattern.group(1).strip()  # Extract verse\n",
    "        lemma = pattern.group(2).strip()  # Extract lemma\n",
    "\n",
    "        # Check if the verse contains a range\n",
    "        if '–' in verse_part:\n",
    "            from_verse, to_verse = map(str.strip, verse_part.split('–', 1))\n",
    "            verse = {'from_verse': from_verse, 'to_verse': to_verse}\n",
    "        else:\n",
    "            verse = {'verse': verse_part}  # Single verse\n",
    "    else:\n",
    "        verse = {'verse': previous_verse}  # Use previous verse if no verse found\n",
    "        lemma = entry_text.strip()  # Assume the rest is lemma\n",
    "\n",
    "    return {\n",
    "        'verse': verse,\n",
    "        'lemma': lemma\n",
    "    }\n",
    "\n",
    "def process_lemma_specific_range(lemma_text):\n",
    "    \"\"\"\n",
    "    Process a lemma with one or multiple specific ranges (...).\n",
    "    Handles cases like 'word ... word ... word ... word'.\n",
    "    \"\"\"\n",
    "    # Split the lemma by '...'\n",
    "    parts = [part.strip() for part in lemma_text.split('...') if part.strip()]\n",
    "\n",
    "    # Handle cases with multiple specific ranges\n",
    "    processed_ranges = []\n",
    "    for i, part in enumerate(parts):\n",
    "        if i == 0:\n",
    "            # The first part is the start\n",
    "            processed_ranges.append({'tag': 'start lemma', 'content': part})\n",
    "        elif i == len(parts) - 1:\n",
    "            # The last part is the end\n",
    "            processed_ranges.append({'tag': 'end lemma', 'content': part})\n",
    "        else:\n",
    "            # Intermediate parts are tagged as middle ranges\n",
    "            processed_ranges.append({'tag': f'middle lemma {i}', 'content': part})\n",
    "\n",
    "    # Return the processed range structure\n",
    "    return {\n",
    "        'type': 'specific_range',\n",
    "        'parts': processed_ranges\n",
    "    }\n",
    "\n",
    "def process_lemma_range(lemma_text):\n",
    "    \"\"\"\n",
    "    Process a lemma with a range (–).\n",
    "    \"\"\"\n",
    "    parts = lemma_text.split('–', 1)\n",
    "    part_from = parts[0].strip()\n",
    "    part_to = parts[1].strip()\n",
    "\n",
    "    return {\n",
    "        'type': 'full_range',\n",
    "        'from': part_from,\n",
    "        'to': part_to\n",
    "    }\n",
    "\n",
    "def process_lemma_transposition(lemma_text):\n",
    "    \"\"\"\n",
    "    Process a lemma with a transposition (~).\n",
    "    \"\"\"\n",
    "    parts = lemma_text.split('~', 1)\n",
    "    part_a = parts[0].strip()\n",
    "    part_b = parts[1].strip()\n",
    "\n",
    "    return {\n",
    "        'type': 'transposition',\n",
    "        'parts': {\n",
    "            'a': process_lemma(part_a),\n",
    "            'b': process_lemma(part_b)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def split_k_q_lemmas(lemma_text):\n",
    "    \"\"\"\n",
    "    Splits lemmas prefixed by k and q (e.g., 'k עינתם / q עוֹנֹתם')\n",
    "    into distinct lemmas with their respective tags, dropping k/q prefixes.\n",
    "    \"\"\"\n",
    "    # Regex to match prefixed lemmas and split them\n",
    "    pattern = r'\\b[kq]\\s+([^\\s/]+)\\s*/\\s*\\b[kq]\\s+([^\\s]+)'\n",
    "    match = re.match(pattern, lemma_text)\n",
    "\n",
    "    if match:\n",
    "        lemma_k = match.group(1).strip()  # Extract the lemma content after 'k'\n",
    "        lemma_q = match.group(2).strip()  # Extract the lemma content after 'q'\n",
    "\n",
    "        return [\n",
    "            {'tag': 'lemma_k', 'content': lemma_k},\n",
    "            {'tag': 'lemma_q', 'content': lemma_q}\n",
    "        ]\n",
    "\n",
    "    # If no match, return the lemma as-is\n",
    "    return [{'tag': 'lemma', 'content': lemma_text}]\n",
    "\n",
    "\n",
    "def process_lemma(lemma_text):\n",
    "    \"\"\"\n",
    "    Process the lemma string. Checks for transposition, range, or specific range\n",
    "    and delegates to specialized functions. If none found, processes individual lemmas.\n",
    "    \"\"\"\n",
    "    if not lemma_text:\n",
    "        return None  # Return if lemma is empty\n",
    "    \n",
    "    if '/' in lemma_text and re.search(r'\\b[kq]\\s', lemma_text):\n",
    "        return split_k_q_lemmas(lemma_text)\n",
    "    elif '~' in lemma_text:\n",
    "        return process_lemma_transposition(lemma_text)\n",
    "    elif '–' in lemma_text:\n",
    "        return process_lemma_range(lemma_text)\n",
    "    elif '...' in lemma_text:\n",
    "        return process_lemma_specific_range(lemma_text)\n",
    "\n",
    "    # Default: Process individual lemmas\n",
    "    return process_individual_lemma(lemma_text)\n",
    "\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    \"\"\"\n",
    "    Process individual lemmas, concatenating multiple superscripts into a single number if present,\n",
    "    and including parentheses when they are part of the lemma.\n",
    "    \"\"\"\n",
    "    # Regex to match words (including parentheses as part of the lemma) with optional superscripts\n",
    "    lemma_regex = r'([^\\s<]+)((?:<superscript\\s*[^>]+>)*)'\n",
    "    superscript_regex = r'<superscript\\s*([^>]+)>'\n",
    "\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "\n",
    "    processed_lemmas = []\n",
    "    for i, (word, superscripts) in enumerate(matches, start=1):\n",
    "        lemma_dict = {'tag': f'lemma{i}', 'content': word}\n",
    "\n",
    "        # Concatenate all superscripts into a single string\n",
    "        if superscripts:\n",
    "            superscript_values = re.findall(superscript_regex, superscripts)\n",
    "            lemma_dict['numbers'] = ''.join(superscript_values)  # Combine into a single string\n",
    "\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "\n",
    "    return processed_lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d693bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b596a560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verse': None,\n",
       " 'lemma_parts': [{'type': 'transposition',\n",
       "   'part': 'a',\n",
       "   'details': [{'tag': 'from lemma', 'content': 'כי'},\n",
       "    {'delimiter': '–', 'type': 'range'},\n",
       "    {'tag': 'to lemma', 'content': 'עמי'}]},\n",
       "  {'type': 'transposition',\n",
       "   'part': 'b',\n",
       "   'details': [{'tag': 'from lemma', 'content': 'ואנכי'},\n",
       "    {'delimiter': '–', 'type': 'range'},\n",
       "    {'tag': 'to lemma', 'content': 'לכם'}]}]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_lemma_with_range_and_transposition(lemma['lemma'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66d7161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_lemma_with_range_and_transposition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mכי – עמי ~ ואנכי – לכם\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m----> 3\u001b[0m result_with_range \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_lemma_with_range_and_transposition\u001b[49m(test_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m result_with_range\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_lemma_with_range_and_transposition' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test input\n",
    "test_input = {'lemma': 'כי – עמי ~ ואנכי – לכם'}\n",
    "result_with_range = process_lemma_with_range_and_transposition(test_input['lemma'])\n",
    "result_with_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37038e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Lemma: {'verse': 6, 'lemma': 'עוד', 'number': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "example_lemma = '6 עוד<superscript 1>'\n",
    "processed_data = process_lemma_with_superscript(example_lemma)\n",
    "\n",
    "# Display the result\n",
    "print(\"Processed Lemma:\", processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae2c947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'מלכי'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf0d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_verse_processor(text, chapter):\n",
    "    # Regex to match the verse numbers at the beginning\n",
    "    verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "    # Extract verses\n",
    "    verses_match = re.match(verse_regex, text)\n",
    "    if verses_match:\n",
    "        verse_range = verses_match.group(1).split('–')\n",
    "        if len(verse_range) == 2 and verse_range[0] != verse_range[1]:\n",
    "            verses = {'from': int(verse_range[0]), 'to': int(verse_range[1])}\n",
    "        else:\n",
    "            verses = int(verse_range[0])\n",
    "    else:\n",
    "        verses = None\n",
    "    \n",
    "    # Isolate lemmas part by removing the verses\n",
    "    lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "    return {\n",
    "        'chapter': chapter,\n",
    "        'verses': verses,\n",
    "        'lemmas': process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to process individual lemmas or ranges, after the split,\n",
    "lemma_regex = r'(k|q)?\\s*([^\\d\\s]+)(\\d?\\,?\\d?)'#(\\d+(?:–\\d+)?)\\s\n",
    "\n",
    "def process_lemma_with_range_and_diacritics(lemma):\n",
    "    # Adjust regex to include diacritical marks and punctuation within Hebrew words\n",
    "    \n",
    "    \n",
    "    # Check for range indicated by \"–\" and process accordingly\n",
    "    if \"–\" in lemma:\n",
    "        from_lemma, to_lemma = lemma.split(\"–\")\n",
    "        return {\n",
    "            'from': process_individual_lemma(from_lemma.strip()),\n",
    "            'to': process_individual_lemma(to_lemma.strip())\n",
    "        }\n",
    "\n",
    "    # Split lemma if there are separate lemmas with \"/\"\n",
    "    split_lemmas = re.split(r'\\s*/\\s*', lemma) if '/' in lemma else [lemma]\n",
    "    \n",
    "    processed_lemmas = []\n",
    "    for split_lemma in split_lemmas:\n",
    "        processed = process_individual_lemma(split_lemma)\n",
    "        processed_lemmas.extend(processed)\n",
    "    \n",
    "    return processed_lemmas\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "    processed_lemmas = []\n",
    "    for match in matches:\n",
    "        prefix, word, number = match\n",
    "        lemma_dict = {'lemma': word}\n",
    "        if prefix: lemma_dict[prefix] = True\n",
    "        if number: lemma_dict['number'] = (number)\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "    return processed_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2dc7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing functions, including chapter information\n",
    "def process_full_entry(text, chapter, previous_verse=None):\n",
    "    lemma, part_entry = split_full_entry(text)\n",
    "    lemma_dict = lemma_verse_processor(lemma, chapter)\n",
    "\n",
    "#     return lemma_dict, decoded_entries, lemma_dict['verses']\n",
    "    return part_entry\n",
    "\n",
    "def split_on_comma_not_in_parentheses(part):\n",
    "    \"\"\"\n",
    "    Splits the string on ',' not inside parentheses.\n",
    "    \"\"\"\n",
    "    sub_parts = []\n",
    "    current_part = []\n",
    "    paren_depth = 0  # Track depth of parentheses\n",
    "\n",
    "    for char in part:\n",
    "        if char == '(':\n",
    "            paren_depth += 1\n",
    "        elif char == ')':\n",
    "            paren_depth -= 1\n",
    "        elif char == ',' and paren_depth == 0:\n",
    "            # At a top-level comma, split here\n",
    "            sub_parts.append(''.join(current_part))\n",
    "            current_part = []\n",
    "            continue\n",
    "\n",
    "        current_part.append(char)\n",
    "\n",
    "    # Add the last part if there's any\n",
    "    if current_part:\n",
    "        sub_parts.append(''.join(current_part))\n",
    "\n",
    "    return sub_parts\n",
    "\n",
    "def split_full_entry(text):\n",
    "    sliced_entry = text.split(sep=']', maxsplit=1)\n",
    "    lemma, entry = sliced_entry\n",
    "#         print(f\"lemma: {lemma}\")\n",
    "#         print(f\"entry: {entry}\")\n",
    "    return lemma, entry    \n",
    "\n",
    "# def lemma_verse_processor(text, chapter):\n",
    "#     # Simplified approach: first split into digits and lemmas\n",
    "#     # Regex to match the verse numbers at the beginning\n",
    "#     verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "#     # Extract verses\n",
    "#     verses_match = re.match(verse_regex, text)\n",
    "#     verses = list(map(int, verses_match.group(1).split('–'))) if verses_match else []\n",
    "    \n",
    "#     # Isolate lemmas part by removing the verses\n",
    "#     lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "#     return {\n",
    "#         'chapter': chapter,\n",
    "#         'verses': verses,\n",
    "#         'lemmas': process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "#     }\n",
    "\n",
    "def lemma_verse_processor(text, chapter):\n",
    "    # Regex to match the verse numbers at the beginning\n",
    "    verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "    # Extract verses\n",
    "    verses_match = re.match(verse_regex, text)\n",
    "    if verses_match:\n",
    "        verse_range = verses_match.group(1).split('–')\n",
    "        if len(verse_range) == 2 and verse_range[0] != verse_range[1]:\n",
    "            verses = {'from': int(verse_range[0]), 'to': int(verse_range[1])}\n",
    "        else:\n",
    "            verses = int(verse_range[0])\n",
    "    else:\n",
    "        verses = None\n",
    "    \n",
    "    # Isolate lemmas part by removing the verses\n",
    "    lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "    return {\n",
    "        'chapter': chapter,\n",
    "        'verses': verses,\n",
    "        'lemmas': process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to process individual lemmas or ranges, after the split,\n",
    "lemma_regex = r'(k|q)?\\s*([^\\d\\s]+)(\\d?\\,?\\d?)'#(\\d+(?:–\\d+)?)\\s\n",
    "\n",
    "def process_lemma_with_range_and_diacritics(lemma):\n",
    "    # Adjust regex to include diacritical marks and punctuation within Hebrew words\n",
    "    \n",
    "    \n",
    "    # Check for range indicated by \"–\" and process accordingly\n",
    "    if \"–\" in lemma:\n",
    "        from_lemma, to_lemma = lemma.split(\"–\")\n",
    "        return {\n",
    "            'from': process_individual_lemma(from_lemma.strip()),\n",
    "            'to': process_individual_lemma(to_lemma.strip())\n",
    "        }\n",
    "\n",
    "    # Split lemma if there are separate lemmas with \"/\"\n",
    "    split_lemmas = re.split(r'\\s*/\\s*', lemma) if '/' in lemma else [lemma]\n",
    "    \n",
    "    processed_lemmas = []\n",
    "    for split_lemma in split_lemmas:\n",
    "        processed = process_individual_lemma(split_lemma)\n",
    "        processed_lemmas.extend(processed)\n",
    "    \n",
    "    return processed_lemmas\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "    processed_lemmas = []\n",
    "    for match in matches:\n",
    "        prefix, word, number = match\n",
    "        lemma_dict = {'lemma': word}\n",
    "        if prefix: lemma_dict[prefix] = True\n",
    "        if number: lemma_dict['number'] = (number)\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "    return processed_lemmas\n",
    "\n",
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def extract_cross_references(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "def parse_witnesses(text):\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d.]+)?\\)?', re.DOTALL | re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    # Filter out empty tuples\n",
    "    return [part for part in parts if any(part)]\n",
    "\n",
    "def post_process_witnesses(witnesses):\n",
    "    # Define the set of specific values for \"x\"\n",
    "    specific_values = {\"G-B Eb \", \"G-B Kb \", \"G-B Msr \"}  # Replace with the actual values\n",
    "\n",
    "    # Iterate over the witnesses, except for the last one\n",
    "    for i in range(len(witnesses) - 1):\n",
    "        z, y, x = witnesses[i]\n",
    "\n",
    "        # Check if \"x\" is one of the specific values\n",
    "        if x in specific_values:\n",
    "            # Remove \"x\" from the current tuple and prepend it to the \"z\" of the next witness\n",
    "            witnesses[i] = (z, y, '')\n",
    "            next_z, next_y, next_x = witnesses[i + 1]\n",
    "            witnesses[i + 1] = (x + next_z, next_y, next_x)\n",
    "\n",
    "    # Remove the first tuple if it becomes empty\n",
    "    if witnesses and witnesses[0] == ('', '', ''):\n",
    "        witnesses.pop(0)\n",
    "\n",
    "    return witnesses\n",
    "\n",
    "def parse_comma_witnesses(text):\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d*)?\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL | re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    # Filter out empty tuples\n",
    "    return [part for part in parts if any(part)]\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~\\.]*)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]*\\s?)[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        \\s*\n",
    "        (?P<GeneralComment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "#splitting entry into witnesses and reading (if only one group assign to witnesses)\n",
    "def witness_reading_splitter(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>~.]*\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>~])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "\n",
    "def process_entry(entry):\n",
    "    clean_entry, cross_references = extract_cross_references(entry)\n",
    "    split_entry = witness_reading_splitter(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry[0])}\n",
    "        if len(split_entry) == 2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else:  # there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1] + split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry)}\n",
    "        reading = ''\n",
    "    # Include \"Cross References\" only if the list is not empty\n",
    "    result = [witnesses, {\"Rdg\": reading}]\n",
    "    if cross_references:\n",
    "        result.append({\"Cross References\": cross_references})\n",
    "    return result\n",
    "\n",
    "def process_comma_entry(entry):\n",
    "    clean_entry, cross_references = extract_cross_references(entry)\n",
    "    split_entry = witness_reading_splitter(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': parse_comma_witnesses(split_entry[0])}\n",
    "        if len(split_entry) == 2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else:  # there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1] + split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': parse_comma_witnesses(split_entry)}\n",
    "        reading = ''\n",
    "    # Include \"Cross References\" only if the list is not empty\n",
    "    result = [witnesses, {\"Rdg\": reading}]\n",
    "    if cross_references:\n",
    "        result.append({\"Cross References\": cross_references})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908c3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8240bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample structure of doc_content.body_runs:\n",
      "[[[[['\\t', 'Hosea 1'], [], ['1 יותם אחז יחזקיה] ][ &'], ['מלכי] ]h numII', '----footnote1----'], ['2 דִּבֶּר] ] λόγου', '----footnote2----', ' = [T'], ['ב(הושע)] ]h πρός', '----footnote3----', ' + [ܕܗܘܐ ܥܠ', '----footnote4----'], ['ו(יאמר)] ]h[ >'], ['לֵךְ] ]h >II'], ['3 ויקח] [ + ܠܗ', '----footnote5----'], ['לו] ]h*- >II III IV', '----footnote6----'], ['4 יהוא] ]h Ιουδα', '----footnote7----'], ['(ו)הִשְׁבַּתִּי] ]- ἀποστρέψω', '----footnote8----'], ['בית2] ]h*hT- + prep', '----footnote9----'], ['5 והיה (ביום ההוא)] [ >', '----footnote10----'], ['(ו)היה] * >'], ['ההוא] ]h + dicit dominus', '----footnote11----'], ['6 עוד1] *hT- >II III IV'], ['לו] ]h + κύριοςII', '----footnote12----', ' = [| [ pron', '----footnote13----'], ['נשׂא אשׂא] ] ἀντιτασσόμενος ἀντιτάξομαι', '----footnote14----', ' | * oblivione obliviscar', '----footnote15----', ' + ~'], ['7 בית] ] υἱούς', '----footnote16----'], ['יהודה] ]h >'], ['ולא] ][ rep'], ['(ו)במלחמה] ]- + (οὐδὲ) ἐν ἅρμασιν', '----footnote17----'], ['בסוסים] ]*[ &III IV'], ['8 ותהר] ]- + ἔτιII', '----footnote18----', ' = ['], ['ותלד] ]h + αὐτῷIV', '----footnote19----', ' = *h'], ['9 ויאמר] ]h + κύριος (αὐτῷ)II', '----footnote20----', ' = Th + ['], ['\\t', 'כי – עמי ~ ואנכי – לכם] 9 ~'], ['לכם] ]h + θεός', '----footnote21----', ' = *h + [h']]]]]\n"
     ]
    }
   ],
   "source": [
    "from docx2python import docx2python\n",
    "\n",
    "# Path to the .docx file\n",
    "docx_path = 'Hosea.1.App I.Full.docx'\n",
    "\n",
    "# Load the document using docx2python\n",
    "doc_content = docx2python(docx_path)\n",
    "\n",
    "# Print a portion of the structure to examine it\n",
    "print(\"Sample structure of doc_content.body_runs:\")\n",
    "print(doc_content.body_runs[:5])  # Print the first few elements for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0c8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Document with Footnote References:\n",
      "<p>Hosea 1</p>\n",
      "<p></p>\n",
      "<p>1 יותם אחז יחזקיה] ][ &</p>\n",
      "<p>מלכי] ]h numII<ref 1> cf app Mic 11</ref></p>\n",
      "<p>2 דִּבֶּר] ] λόγου<ref 2> (a) voc דְּבַר (יהוה), formula, cf v1 41 et al; cf app 131; cf תחלת דִּבְרֵי Qoh 1013; note seq; (b) noun דִּבֵּר, cf app Jer 513 and Rabb Heb; cf gerund in * loquendi “of speaking”</ref> = [T</p>\n",
      "<p>ב(הושע)] ]h πρός<ref 3> “to”; main evid, cf v1</ref> + [ܕܗܘܐ ܥܠ<ref 4> “which was to”, ex v1</ref></p>\n",
      "<p>ו(יאמר)] ]h[ ></p>\n",
      "<p>לֵךְ] ]h >II</p>\n",
      "<p>3 ויקח] [ + ܠܗ<ref 5> “for himself”, cf v2</ref></p>\n",
      "<p>לו] ]h*- >II III IV<ref 6> cf vv6,8; contrast Hier 10154</ref></p>\n",
      "<p>4 יהוא] ]h Ιουδα<ref 7> main evid; inner-Grk (בית יהודה common collocation), cf Hier 12208−211</ref></p>\n",
      "<p>(ו)הִשְׁבַּתִּי] ]- ἀποστρέψω<ref 8> voc הֲשִׁבֹתִי, similarly app 213 Ezek 724; for parall השיב//פקד cf 49 123; main evid ]h καταπαύσω (=x)</ref></p>\n",
      "<p>בית2] ]h*hT- + prep<ref 9> common formula השבית מן, cf e.g. Lev 266 Jer 734</ref></p>\n",
      "<p>5 והיה (ביום ההוא)] [ ><ref 10> formulaic change, cf app Joel 418 Mic 59 et al</ref></p>\n",
      "<p>(ו)היה] * ></p>\n",
      "<p>ההוא] ]h + dicit dominus<ref 11> “says the Lord”, formula (נאם יהוה), cf 218,23 et al</ref></p>\n",
      "<p>6 עוד1] *hT- >II III IV</p>\n",
      "<p>לו] ]h + κύριοςII<ref 12> “the Lord”, cf v4, app v9 (ויאמר)</ref> = [| [ pron<ref 13> 1sg, cf 31</ref></p>\n",
      "<p>נשׂא אשׂא] ] ἀντιτασσόμενος ἀντιτάξομαι<ref 14> “opposing I shall oppose” (= ] 1Kgs 1134); p etym \\ נשׁא“oppress, set against” (cf Ps 8923), cf Obad 7 הִשִּׁיאוּךָ J ] ἀντέστησάν σοι “they opposed you” (for interchange of ἀντιτάσσεσθαι “to oppose” / ἀνθίστασθαι “to stand against” cf ]i 4Macc 1623); cf also Jer 4916 הִשִּׁיא = ] [2917] ἐνεχείρησε “(it) attacked” (contrast etym \\נשׂא ~9 and pObad 3 ]) </ref> | * oblivione obliviscar<ref 15> “by forgetfulness I shall forget”, etym \\נשׁה, contrast app Jer 2339</ref> + ~</p>\n",
      "<p>7 בית] ] υἱούς<ref 16> “sons (of)”, for בני/בית cf app Amos 15 31 Zeph 18  </ref></p>\n",
      "<p>יהודה] ]h ></p>\n",
      "<p>ולא] ][ rep</p>\n",
      "<p>(ו)במלחמה] ]- + (οὐδὲ) ἐν ἅρμασιν<ref 17> p (ו)ברכב; common collocation, cf e.g. Ezek 267 בסוס וברכב ובפרשים</ref></p>\n",
      "<p>בסוסים] ]*[ &III IV</p>\n",
      "<p>8 ותהר] ]- + ἔτιII<ref 18> “again”, cf v6</ref> = [</p>\n",
      "<p>ותלד] ]h + αὐτῷIV<ref 19> “(to) him”, cf v3</ref> = *h</p>\n",
      "<p>9 ויאמר] ]h + κύριος (αὐτῷ)II<ref 20> “the Lord (to him)”, cf app v6 (לו); for [ pron cf n14</ref> = Th + [</p>\n",
      "<p>כי – עמי ~ ואנכי – לכם] 9 ~</p>\n",
      "<p>לכם] ]h + θεός<ref 21> “God”, cf e.g. Exod 67 Lev 2612 Zech 88; note diverse word-order in ]h</ref> = *h + [h</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Path to your .docx file\n",
    "docx_path = 'Hosea.1.App I.Full.docx'\n",
    "\n",
    "# Temporary storage for extracted XML content\n",
    "document_xml = None\n",
    "footnotes_xml = None\n",
    "\n",
    "# Step 1: Extract `document.xml` and `footnotes.xml` from the .docx file\n",
    "with zipfile.ZipFile(docx_path, 'r') as docx:\n",
    "    if 'word/document.xml' in docx.namelist():\n",
    "        document_xml = docx.read('word/document.xml').decode('utf-8')\n",
    "    if 'word/footnotes.xml' in docx.namelist():\n",
    "        footnotes_xml = docx.read('word/footnotes.xml').decode('utf-8')\n",
    "\n",
    "# Step 2: Parse `footnotes.xml` to create a dictionary of footnote content\n",
    "footnotes_dict = {}\n",
    "if footnotes_xml:\n",
    "    footnotes_root = ET.fromstring(footnotes_xml)\n",
    "    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "\n",
    "    for footnote in footnotes_root.findall('w:footnote', namespaces):\n",
    "        footnote_id = footnote.get(f'{{{namespaces[\"w\"]}}}id')\n",
    "        footnote_text = ''.join(node.text or '' for node in footnote.findall('.//w:t', namespaces))\n",
    "        footnotes_dict[footnote_id] = footnote_text\n",
    "\n",
    "# Step 3: Parse `document.xml` to identify and tag footnote references\n",
    "formatted_text = \"\"\n",
    "if document_xml:\n",
    "    document_root = ET.fromstring(document_xml)\n",
    "    for paragraph in document_root.findall('.//w:p', namespaces):\n",
    "        paragraph_text = \"\"\n",
    "        \n",
    "        # Process each run in the paragraph\n",
    "        for run in paragraph.findall('.//w:r', namespaces):\n",
    "            text_elem = run.find('w:t', namespaces)\n",
    "            footnote_ref = run.find('.//w:footnoteReference', namespaces)\n",
    "            \n",
    "            if text_elem is not None:\n",
    "                paragraph_text += text_elem.text or ''\n",
    "            elif footnote_ref is not None:\n",
    "                # Get the ID of the footnote reference and wrap it in <ref ...> tags\n",
    "                footnote_id = footnote_ref.get(f'{{{namespaces[\"w\"]}}}id')\n",
    "                footnote_content = footnotes_dict.get(footnote_id, \"\")\n",
    "                paragraph_text += f\"<ref {footnote_id}>{footnote_content}</ref>\"\n",
    "\n",
    "        # Add formatted paragraph text to the main text\n",
    "        formatted_text += f\"<p>{paragraph_text}</p>\\n\"\n",
    "\n",
    "print(\"Formatted Document with Footnote References:\")\n",
    "print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b2df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified document saved as Hosea.1.App I.Modified.docx\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from docx import Document\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths for input and output files\n",
    "docx_path = 'Hosea.1.App I.Full.docx'  # Original document path\n",
    "output_docx_path = 'Hosea.1.App I.Modified.docx'  # Output document with modified content\n",
    "special_font_name = 'HUBPSigla'  # Replace with actual font name for special characters\n",
    "replacement_font_name = 'Times New Roman'  # Font to replace special font\n",
    "\n",
    "# Temporary directory to modify the .docx content\n",
    "temp_dir = \"temp_docx\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "\n",
    "# Step 1: Extract .docx contents to a temporary directory\n",
    "with zipfile.ZipFile(docx_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# Step 2: Access and modify footnotes.xml in the extracted content\n",
    "footnotes_path = os.path.join(temp_dir, \"word\", \"footnotes.xml\")\n",
    "if os.path.exists(footnotes_path):\n",
    "    tree = ET.parse(footnotes_path)\n",
    "    root = tree.getroot()\n",
    "    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
    "\n",
    "    # Modify each footnote element\n",
    "    for footnote in root.findall('w:footnote', namespaces):\n",
    "        footnote_id = footnote.get(f'{{{namespaces[\"w\"]}}}id')\n",
    "        wrapped_text = f\"<footnote ID='{footnote_id}'>\"\n",
    "\n",
    "        # Append each run's text with special font tags and close the footnote tag\n",
    "        for run in footnote.findall('.//w:r', namespaces):\n",
    "            text_elem = run.find('w:t', namespaces)\n",
    "            font_elem = run.find('.//w:rPr//w:rFonts', namespaces)\n",
    "            \n",
    "            if text_elem is not None:\n",
    "                text = text_elem.text or \"\"\n",
    "                font = font_elem.get(f'{{{namespaces[\"w\"]}}}ascii') if font_elem is not None else \"Unknown\"\n",
    "                \n",
    "                # Wrap text in <specialFont ...> if in special font, otherwise add it normally\n",
    "                if font == special_font_name:\n",
    "                    wrapped_text += f\"<specialFont {text} >\"\n",
    "                else:\n",
    "                    wrapped_text += text\n",
    "                \n",
    "        wrapped_text += \"</footnote>\"\n",
    "\n",
    "        # Replace the original footnote text with wrapped text in the first <w:t> element\n",
    "        for elem in footnote.findall('.//w:t', namespaces):\n",
    "            elem.text = wrapped_text  # Replace with modified wrapped text\n",
    "            break  # Only replace the first <w:t> element to avoid duplicating\n",
    "\n",
    "    # Save the modified footnotes.xml\n",
    "    tree.write(footnotes_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "# Step 3: Modify main text with python-docx to add <specialFont ...> tags\n",
    "document = Document(docx_path)\n",
    "\n",
    "for para in document.paragraphs:\n",
    "    for run in para.runs:\n",
    "        font_name = run.font.name if run.font else \"Unknown\"\n",
    "        \n",
    "        # Wrap text in <specialFont ...> tags if it is in the special font\n",
    "        if font_name == special_font_name:\n",
    "            modified_text = ''.join(f\"<specialFont {char} >\" for char in run.text)\n",
    "            run.text = modified_text\n",
    "            run.font.name = replacement_font_name\n",
    "\n",
    "# Save the modified main text as a new temporary .docx file\n",
    "temp_main_docx_path = os.path.join(temp_dir, \"modified_main.docx\")\n",
    "document.save(temp_main_docx_path)\n",
    "\n",
    "# Step 4: Repackage the modified contents into a new .docx file\n",
    "# Replace the main document (document.xml) from the modified_main.docx in the temp directory\n",
    "with zipfile.ZipFile(temp_main_docx_path, 'r') as temp_main_zip:\n",
    "    temp_main_zip.extract('word/document.xml', temp_dir)\n",
    "\n",
    "# Create the final modified .docx by repackaging\n",
    "with zipfile.ZipFile(output_docx_path, 'w') as zip_out:\n",
    "    for foldername, subfolders, filenames in os.walk(temp_dir):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            arcname = os.path.relpath(file_path, temp_dir)\n",
    "            zip_out.write(file_path, arcname)\n",
    "\n",
    "# Cleanup temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(f\"Modified document saved as {output_docx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e7309e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[['\\t', 'Hosea 1'],\n",
       "    [],\n",
       "    ['1 יותם אחז יחזקיה] ][ &'],\n",
       "    ['מלכי] ]h numII', '----footnote1----'],\n",
       "    ['2 דִּבֶּר] ] λόγου', '----footnote2----', ' = [T'],\n",
       "    ['ב(הושע)] ]h πρός',\n",
       "     '----footnote3----',\n",
       "     ' + [ܕܗܘܐ ܥܠ',\n",
       "     '----footnote4----'],\n",
       "    ['ו(יאמר)] ]h[ >'],\n",
       "    ['לֵךְ] ]h >II'],\n",
       "    ['3 ויקח] [ + ܠܗ', '----footnote5----'],\n",
       "    ['לו] ]h*- >II III IV', '----footnote6----'],\n",
       "    ['4 יהוא] ]h Ιουδα', '----footnote7----'],\n",
       "    ['(ו)הִשְׁבַּתִּי] ]- ἀποστρέψω', '----footnote8----'],\n",
       "    ['בית2] ]h*hT- + prep', '----footnote9----'],\n",
       "    ['5 והיה (ביום ההוא)] [ >', '----footnote10----'],\n",
       "    ['(ו)היה] * >'],\n",
       "    ['ההוא] ]h + dicit dominus', '----footnote11----'],\n",
       "    ['6 עוד1] *hT- >II III IV'],\n",
       "    ['לו] ]h + κύριοςII',\n",
       "     '----footnote12----',\n",
       "     ' = [| [ pron',\n",
       "     '----footnote13----'],\n",
       "    ['נשׂא אשׂא] ] ἀντιτασσόμενος ἀντιτάξομαι',\n",
       "     '----footnote14----',\n",
       "     ' | * oblivione obliviscar',\n",
       "     '----footnote15----',\n",
       "     ' + ~'],\n",
       "    ['7 בית] ] υἱούς', '----footnote16----'],\n",
       "    ['יהודה] ]h >'],\n",
       "    ['ולא] ][ rep'],\n",
       "    ['(ו)במלחמה] ]- + (οὐδὲ) ἐν ἅρμασιν', '----footnote17----'],\n",
       "    ['בסוסים] ]*[ &III IV'],\n",
       "    ['8 ותהר] ]- + ἔτιII', '----footnote18----', ' = ['],\n",
       "    ['ותלד] ]h + αὐτῷIV', '----footnote19----', ' = *h'],\n",
       "    ['9 ויאמר] ]h + κύριος (αὐτῷ)II', '----footnote20----', ' = Th + ['],\n",
       "    ['\\t', 'כי – עמי ~ ואנכי – לכם] 9 ~'],\n",
       "    ['לכם] ]h + θεός', '----footnote21----', ' = *h + [h']]]]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_content.body_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a62e21f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 'Hos' - Font: Unknown\n",
      "Word: 'ea' - Font: Unknown\n",
      "Word: '1' - Font: Unknown\n",
      "Word: '1' - Font: Unknown\n",
      "Word: 'יותם' - Font: Unknown\n",
      "Word: 'אחז' - Font: Unknown\n",
      "Word: 'יחזקיה' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '&' - Font: HUBPSigla\n",
      "Word: 'מלכי' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: 'h' - Font: HUBPSigla\n",
      "Word: 'num' - Font: Unknown\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: '2' - Font: Unknown\n",
      "Word: 'דִּבֶּר' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: 'λόγου' - Font: GFS Porson\n",
      "Word: '=' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: 'T' - Font: HUBPSigla\n",
      "Word: 'ב' - Font: Unknown\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'הושע' - Font: Unknown\n",
      "Word: ')' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: 'πρός' - Font: GFS Porson\n",
      "Word: '+' - Font: HUBPSigla\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: 'ܕܗܘܐ' - Font: Unknown\n",
      "Word: 'ܥܠ' - Font: Unknown\n",
      "Word: 'ו' - Font: Unknown\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'יאמר' - Font: Unknown\n",
      "Word: ')' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'לֵךְ' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: '3' - Font: Unknown\n",
      "Word: 'ויקח' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'ܠܗ' - Font: Unknown\n",
      "Word: 'לו' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '*' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: 'III' - Font: Unknown\n",
      "Word: 'IV' - Font: Unknown\n",
      "Word: '4' - Font: Unknown\n",
      "Word: 'יהוא' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: 'Ιουδα' - Font: GFS Porson\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'ו' - Font: Unknown\n",
      "Word: ')' - Font: Unknown\n",
      "Word: 'הִ' - Font: Unknown\n",
      "Word: 'שְׁ' - Font: Unknown\n",
      "Word: 'בַּתִּי' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: 'ἀποστρέψω' - Font: GFS Porson\n",
      "Word: 'בית' - Font: Unknown\n",
      "Word: '2' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h*' - Font: HUBPSigla\n",
      "Word: 'h' - Font: HUBPSigla\n",
      "Word: 'T' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'prep' - Font: Unknown\n",
      "Word: '5' - Font: Unknown\n",
      "Word: 'והיה' - Font: Unknown\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'ביום' - Font: Unknown\n",
      "Word: 'ההוא' - Font: Unknown\n",
      "Word: ')' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'ו)היה' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '*' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'ההוא' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'dicit' - Font: Unknown\n",
      "Word: 'dominu' - Font: Unknown\n",
      "Word: 's' - Font: Unknown\n",
      "Word: '6' - Font: Unknown\n",
      "Word: 'עוד' - Font: Unknown\n",
      "Word: '1' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '*' - Font: HUBPSigla\n",
      "Word: 'hT' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: 'III' - Font: Unknown\n",
      "Word: 'IV' - Font: Unknown\n",
      "Word: 'לו' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'κύριος' - Font: GFS Porson\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: '=' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '|' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: 'pron' - Font: Unknown\n",
      "Word: 'נש' - Font: Unknown\n",
      "Word: 'ׂ' - Font: Unknown\n",
      "Word: 'א' - Font: Unknown\n",
      "Word: 'אש' - Font: Unknown\n",
      "Word: 'ׂ' - Font: Unknown\n",
      "Word: 'א' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: 'ἀντιτασσόμενος' - Font: GFS Porson\n",
      "Word: 'ἀντιτάξομαι' - Font: GFS Porson\n",
      "Word: '|' - Font: Unknown\n",
      "Word: '*' - Font: HUBPSigla\n",
      "Word: 'oblivione' - Font: Unknown\n",
      "Word: 'oblivisca' - Font: Unknown\n",
      "Word: 'r' - Font: Unknown\n",
      "Word: '+' - Font: HUBPSigla\n",
      "Word: '~' - Font: HUBPSigla\n",
      "Word: '7' - Font: Unknown\n",
      "Word: 'בית' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: 'υἱο' - Font: GFS Porson\n",
      "Word: 'ύ' - Font: GFS Porson\n",
      "Word: 'ς' - Font: GFS Porson\n",
      "Word: 'י' - Font: Unknown\n",
      "Word: 'הודה' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '>' - Font: Unknown\n",
      "Word: 'ולא' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '][' - Font: HUBPSigla\n",
      "Word: 'rep' - Font: Unknown\n",
      "Word: '(' - Font: Unknown\n",
      "Word: 'ו' - Font: Unknown\n",
      "Word: ')' - Font: Unknown\n",
      "Word: 'במלחמה' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: '(' - Font: GFS Porson\n",
      "Word: 'οὐδὲ' - Font: GFS Porson\n",
      "Word: ')' - Font: GFS Porson\n",
      "Word: 'ἐν' - Font: GFS Porson\n",
      "Word: 'ἅρμασιν' - Font: GFS Porson\n",
      "Word: 'בסוסים' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: '*' - Font: HUBPSigla\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: '&' - Font: HUBPSigla\n",
      "Word: 'III' - Font: Unknown\n",
      "Word: 'IV' - Font: Unknown\n",
      "Word: '8' - Font: Unknown\n",
      "Word: 'ותהר' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']' - Font: HUBPSigla\n",
      "Word: '-' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'ἔτι' - Font: GFS Porson\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: '=' - Font: Unknown\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: 'ותלד' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'α' - Font: GFS Porson\n",
      "Word: 'ὐτῷ' - Font: GFS Porson\n",
      "Word: 'IV' - Font: Unknown\n",
      "Word: '=' - Font: Unknown\n",
      "Word: '*h' - Font: HUBPSigla\n",
      "Word: '9' - Font: Unknown\n",
      "Word: 'ויאמר' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'κύριος' - Font: GFS Porson\n",
      "Word: '(' - Font: GFS Porson\n",
      "Word: 'α' - Font: GFS Porson\n",
      "Word: 'ὐτῷ' - Font: GFS Porson\n",
      "Word: ')' - Font: GFS Porson\n",
      "Word: 'II' - Font: Unknown\n",
      "Word: '=' - Font: Unknown\n",
      "Word: 'T' - Font: HUBPSigla\n",
      "Word: 'h' - Font: HUBPSigla\n",
      "Word: '+' - Font: HUBPSigla\n",
      "Word: '[' - Font: HUBPSigla\n",
      "Word: 'כי' - Font: Unknown\n",
      "Word: '–' - Font: Unknown\n",
      "Word: 'עמי' - Font: Unknown\n",
      "Word: '~' - Font: Unknown\n",
      "Word: 'ואנכי' - Font: Unknown\n",
      "Word: '–' - Font: Unknown\n",
      "Word: 'ל' - Font: Unknown\n",
      "Word: 'כ' - Font: Unknown\n",
      "Word: 'ם' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: '9' - Font: HUBPSigla\n",
      "Word: '~' - Font: Unknown\n",
      "Word: 'לכם' - Font: Unknown\n",
      "Word: ']' - Font: Unknown\n",
      "Word: ']h' - Font: HUBPSigla\n",
      "Word: '+' - Font: Unknown\n",
      "Word: 'θεός' - Font: GFS Porson\n",
      "Word: '=' - Font: Unknown\n",
      "Word: '*h' - Font: HUBPSigla\n",
      "Word: '+' - Font: HUBPSigla\n",
      "Word: '[h' - Font: HUBPSigla\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Load the document\n",
    "docx_path = 'Hosea.1.App I.Full.docx'  # Replace with the actual file path\n",
    "document = Document(docx_path)\n",
    "\n",
    "# Function to get font name from a run\n",
    "def get_font_name(run):\n",
    "    try:\n",
    "        if run.font and run.font.name:\n",
    "            return run.font.name\n",
    "        else:\n",
    "            # If the font is not set explicitly, it might be inherited from the style\n",
    "            if run.style and run.style.font and run.style.font.name:\n",
    "                return run.style.font.name\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Iterate over paragraphs and runs to print the font of each word\n",
    "for para in document.paragraphs:\n",
    "    for run in para.runs:\n",
    "        font_name = get_font_name(run)\n",
    "        words = run.text.split()  # Split run text into individual words\n",
    "        for word in words:\n",
    "            print(f\"Word: '{word}' - Font: {font_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64d4f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file with differentiated font encoding saved at: Hosea1_AppI_Full_withFonts.xml\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import xml.etree.ElementTree as ET\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "# Load the custom HUBPSigla font to access glyph mappings\n",
    "hubps_font_path = 'HUBPS_.ttf'\n",
    "hubps_font = TTFont(hubps_font_path)\n",
    "\n",
    "# Load mappings from the HUBPSigla font\n",
    "hubps_glyph_map = {code: glyph_name for cmap in hubps_font['cmap'].tables for code, glyph_name in cmap.cmap.items()}\n",
    "\n",
    "# Load the .docx file\n",
    "docx_path = 'Hosea.1.App I.Full.docx'\n",
    "document = Document(docx_path)\n",
    "\n",
    "# Function to get font name from a run\n",
    "def get_font_name(run):\n",
    "    try:\n",
    "        if run.font and run.font.name:\n",
    "            return run.font.name\n",
    "        elif run.style and run.style.font and run.style.font.name:\n",
    "            return run.style.font.name\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Check if a character is a special HUBPSigla glyph\n",
    "def is_hubps_special(char):\n",
    "    codepoint = ord(char)\n",
    "    return codepoint in hubps_glyph_map\n",
    "\n",
    "# Create the root element for the XML\n",
    "root = ET.Element(\"document\")\n",
    "\n",
    "# Process each paragraph in the Word document\n",
    "for para in document.paragraphs:\n",
    "    para_elem = ET.SubElement(root, \"paragraph\")\n",
    "    \n",
    "    # Process each run in the paragraph\n",
    "    for run in para.runs:\n",
    "        font_name = get_font_name(run)\n",
    "        run_elem = ET.SubElement(para_elem, \"run\")\n",
    "        run_elem.set(\"font\", font_name)\n",
    "\n",
    "        run_text = \"\"\n",
    "        for char in run.text:\n",
    "            if font_name == \"HUBPSigla\" and is_hubps_special(char):\n",
    "                # If it's a HUBPSigla special character, output it as a unique element\n",
    "                special_char_elem = ET.SubElement(run_elem, \"special_char\")\n",
    "                special_char_elem.set(\"unicode\", f\"U+{ord(char):04X}\")\n",
    "                special_char_elem.set(\"font\", \"HUBPSigla\")\n",
    "                special_char_elem.text = char\n",
    "            else:\n",
    "                # Otherwise, treat it as normal text\n",
    "                run_text += char\n",
    "        \n",
    "        # Add normal text to the run element\n",
    "        if run_text:\n",
    "            run_elem.text = run_text\n",
    "\n",
    "# Save the XML to a file\n",
    "xml_path = 'Hosea1_AppI_Full_withFonts.xml'\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(f\"XML file with differentiated font encoding saved at: {xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d98ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File converted successfully and saved as 'App1-output1.txt'\n"
     ]
    }
   ],
   "source": [
    "def convert_docx_to_txt(docx_file_path, txt_file_path):\n",
    "    # Load the .docx file\n",
    "    doc = Document(docx_file_path)\n",
    "\n",
    "    # Extract text from each paragraph in the document\n",
    "    text_content = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Write the extracted text to a .txt file\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text_content)\n",
    "\n",
    "    print(f\"File converted successfully and saved as '{txt_file_path}'\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "convert_docx_to_txt('Hosea.1.App I.Full.docx', 'App1-output1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3320fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'Hosea.1.App I.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHosea.1.App I.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m doc_text \u001b[38;5;241m=\u001b[39m \u001b[43mread_docx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc_text)\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mread_docx\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_docx\u001b[39m(file_path):\n\u001b[1;32m----> 4\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m document\u001b[38;5;241m.\u001b[39mparagraphs:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx\\api.py:23\u001b[0m, in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mloaded.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m docx \u001b[38;5;241m=\u001b[39m _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[1;32m---> 23\u001b[0m document_part \u001b[38;5;241m=\u001b[39m \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[0;32m     25\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx\\opc\\package.py:116\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     pkg_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPackageReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    118\u001b[0m     Unmarshaller\u001b[38;5;241m.\u001b[39munmarshal(pkg_reader, package, PartFactory)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx\\opc\\pkgreader.py:22\u001b[0m, in \u001b[0;36mPackageReader.from_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(pkg_file):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     phys_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(phys_reader\u001b[38;5;241m.\u001b[39mcontent_types_xml)\n\u001b[0;32m     24\u001b[0m     pkg_srels \u001b[38;5;241m=\u001b[39m PackageReader\u001b[38;5;241m.\u001b[39m_srels_for(phys_reader, PACKAGE_URI)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx\\opc\\phys_pkg.py:21\u001b[0m, in \u001b[0;36mPhysPkgReader.__new__\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m         reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     reader_cls \u001b[38;5;241m=\u001b[39m _ZipPkgReader\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at 'Hosea.1.App I.txt'"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def read_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    text = []\n",
    "    for paragraph in document.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "# Example usage\n",
    "file_path = open('Hosea.1.App I.txt'\n",
    "doc_text = read_docx(file_path)\n",
    "print(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ac348e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ufeffHosea 1\\n\\n1 יותם אחז יחזקיה] ][ &\\nמלכי] ]h numII1\\n2 דִּבֶּר] ] λόγου2 = [T\\nב(הושע)] ]h πρός3 + [ܕܗܘܐ ܥܠ4\\nו(יאמר)] ]h[ >\\nלֵךְ] ]h >II\\n3 ויקח] [ + ܠܗ5\\nלו] ]h*- >II III IV6\\n4 יהוא] ]h Ιουδα7\\n(ו)הִשְׁבַּתִּי] ]- ἀποστρέψω8\\nבית2] ]h*hT- + prep9\\n5 והיה (ביום ההוא)] [ >10\\n(ו)היה] * >\\nההוא] ]h + dicit dominus11\\n6 עוד1] *hT- >II III IV\\nלו] ]h + κύριοςII12 = [| [ pron13\\nנשׂא אשׂא] ] ἀντιτασσόμενος ἀντιτάξομαι14 | * oblivione obliviscar15 + ~\\n7 בית] ] υἱούς16\\nיהודה] ]h >\\nולא] ][ rep\\n(ו)במלחמה] ]- + (οὐδὲ) ἐν ἅρμασιν17\\nבסוסים] ]*[ &III IV\\n8 ותהר] ]- + ἔτιII18 = [\\nותלד] ]h + αὐτῷIV19 = *h\\n9 ויאמר] ]h + κύριος (αὐτῷ)II20 = Th + [\\nכי – עמי ~ ואנכי – לכם] 9 ~\\nלכם] ]h + θεός21 = *h + [h\\n1 cf app Mic 11\\n2 (a) voc דְּבַר (יהוה), formula, cf v1 41 et al; cf app 131; cf תחלת דִּבְרֵי Qoh 1013; note seq; (b) noun דִּבֵּר, cf app Jer 513 and Rabb Heb; cf gerund in * loquendi “of speaking”\\n3 “to”; main evid, cf v1\\n4 “which was to”, ex v1\\n5 “for himself”, cf v2\\n6 cf vv6,8; contrast Hier 10154\\n7 main evid; inner-Grk (בית יהודה common collocation), cf Hier 12208−211\\n8 voc הֲשִׁבֹתִי, similarly app 213 Ezek 724; for parall השיב//פקד cf 49 123; main evid ]h καταπαύσω (=x)\\n9 common formula השבית מן, cf e.g. Lev 266 Jer 734\\n10 formulaic change, cf app Joel 418 Mic 59 et al\\n11 “says the Lord”, formula (נאם יהוה), cf 218,23 et al\\n12 “the Lord”, cf v4, app v9 (ויאמר)\\n13 1sg, cf 31\\n14 “opposing I shall oppose” (= ] 1Kgs 1134); p etym \\\\ נשׁא“oppress, set against” (cf Ps 8923), cf Obad 7 הִשִּׁיאוּךָ J ] ἀντέστησάν σοι “they opposed you” (for interchange of ἀντιτάσσεσθαι “to oppose” / ἀνθίστασθαι “to stand against” cf ]i 4Macc 1623); cf also Jer 4916 הִשִּׁיא = ] [2917] ἐνεχείρησε “(it) attacked” (contrast etym \\\\נשׂא ~9 and pObad 3 ]) \\n15 “by forgetfulness I shall forget”, etym \\\\נשׁה, contrast app Jer 2339\\n16 “sons (of)”, for בני/בית cf app Amos 15 31 Zeph 18  \\n17 p (ו)ברכב; common collocation, cf e.g. Ezek 267 בסוס וברכב ובפרשים\\n18 “again”, cf v6\\n19 “(to) him”, cf v3\\n20 “the Lord (to him)”, cf app v6 (לו); for [ pron cf n14\\n21 “God”, cf e.g. Exod 67 Lev 2612 Zech 88; note diverse word-order in ]h\\n---------------\\n\\n------------------------------------------------------------\\n\\n---------------\\n\\n------------------------------------------------------------\\n\\n\\u200f06/11/2024 הושע א, אפ' א + ה (הערות באנגלית)\\n\\n2\\n\\n\\n1\\n\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('Hosea.1.App I.txt', mode='r', encoding ='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79577e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 יותם אחז יחזקיה] ][ &\n"
     ]
    }
   ],
   "source": [
    "lines = open('Hosea.1.App I.txt', mode='r', encoding ='utf8').read().split(sep=\"\\n\")\n",
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc98698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## render apparatus 4 ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7452534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lines(lines):\n",
    "    # Initialize variables\n",
    "    chapter = None\n",
    "    data = []\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            # Skip empty lines\n",
    "            continue\n",
    "        elif len(line.split()) == 2:\n",
    "            # If the line has only two words, save it as the chapter\n",
    "            chapter = line\n",
    "        else:\n",
    "            # If the line is text, couple it with the chapter\n",
    "            if chapter is not None:\n",
    "                data.append([chapter, line])\n",
    "                chapter = None  # Reset the chapter after coupling\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Chapter', 'Line'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_entries(df):\n",
    "    # Define the regular expression pattern to split on\n",
    "    pattern = r'\\xa0+\\'\\xa0+|\\xa0+(?=\\s?\\d+)'\n",
    "    \n",
    "    # Split each line into entries based on the pattern\n",
    "    df['Entries'] = df['Line'].apply(lambda x: re.split(pattern, x))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b00593b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Line</th>\n",
       "      <th>Entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>הושע א</td>\n",
       "      <td>1 (בספר החילופים וברשימת הסדרים שבכ\"י ל מצוין ...</td>\n",
       "      <td>[1 (בספר החילופים וברשימת הסדרים שבכ\"י ל מצוין...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>הושע ב</td>\n",
       "      <td>1 (פ)] ל ל18 פ ר מ: (ס)   '   וְהיה(1)] ר': וַ...</td>\n",
       "      <td>[1 (פ)] ל ל18 פ ר מ: (ס) ,  וְהיה(1)] ר': וַ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>הושע ג</td>\n",
       "      <td>1 (פ)] ל29 פ: (ס)   '   אֱהב־] נ: אֶ  '   אהבת...</td>\n",
       "      <td>[1 (פ)] ל29 פ: (ס) ,  אֱהב־] נ: אֶ,  אהבת] ר: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>הושע ד</td>\n",
       "      <td>1 (פ)] ל29 37 פ מ: (ס) ; ר: (&gt;) ; ר־מ\"ק: \"פרשׄ...</td>\n",
       "      <td>[1 (פ)] ל29 37 פ מ: (ס) ; ר: (&gt;) ; ר־מ\"ק: \"פרש...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>הושע ה</td>\n",
       "      <td>1 (פ)] ל ל18 פ ק ש מ: (ס) ; נ: (&gt;)  '   והקש֣י...</td>\n",
       "      <td>[1 (פ)] ל ל18 פ ק ש מ: (ס) ; נ: (&gt;),  והקש֣יבו...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>זכריה יג</td>\n",
       "      <td>1 (&gt;)] ל מ: (ס)   '   דויד] פ ר: דוד     2 נאם...</td>\n",
       "      <td>[1 (&gt;)] ל מ: (ס) ,  דויד] פ ר: דוד ,  2 נאם ׀ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>זכריה יד</td>\n",
       "      <td>1 (פ)] ל ל10 18 פ מ: (ס)     2 הגויִ֥ם ׀ ] ק: ...</td>\n",
       "      <td>[1 (פ)] ל ל10 18 פ מ: (ס) ,  2 הגויִ֥ם ׀ ] ק: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>מלאכי א</td>\n",
       "      <td>1 &gt;] ל: יט (התוספת \"יט\" מיד שנייה)     2 הלוֹא־...</td>\n",
       "      <td>[1 &gt;] ל: יט (התוספת \"יט\" מיד שנייה) ,  2 הלוֹא־...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>מלאכי ב</td>\n",
       "      <td>1 אליכם] ל20 ק־מ\"ק: \"דׄ מטעׄ\" ; ל20־מ\"ג: \"אליכ...</td>\n",
       "      <td>[1 אליכם] ל20 ק־מ\"ק: \"דׄ מטעׄ\" ; ל20־מ\"ג: \"אלי...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>מלאכי ג</td>\n",
       "      <td>1 שלח֙] ל28 30 ק: של֙ח֙  '   ופתאֹם] ר: ופתאוֹם...</td>\n",
       "      <td>[1 שלח֙] ל28 30 ק: של֙ח֙,  ופתאֹם] ר: ופתאוֹם, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Chapter                                               Line  \\\n",
       "0     הושע א  1 (בספר החילופים וברשימת הסדרים שבכ\"י ל מצוין ...   \n",
       "1     הושע ב  1 (פ)] ל ל18 פ ר מ: (ס)   '   וְהיה(1)] ר': וַ...   \n",
       "2     הושע ג  1 (פ)] ל29 פ: (ס)   '   אֱהב־] נ: אֶ  '   אהבת...   \n",
       "3     הושע ד  1 (פ)] ל29 37 פ מ: (ס) ; ר: (>) ; ר־מ\"ק: \"פרשׄ...   \n",
       "4     הושע ה  1 (פ)] ל ל18 פ ק ש מ: (ס) ; נ: (>)  '   והקש֣י...   \n",
       "..       ...                                                ...   \n",
       "62  זכריה יג  1 (>)] ל מ: (ס)   '   דויד] פ ר: דוד     2 נאם...   \n",
       "63  זכריה יד  1 (פ)] ל ל10 18 פ מ: (ס)     2 הגויִ֥ם ׀ ] ק: ...   \n",
       "64   מלאכי א  1 >] ל: יט (התוספת \"יט\" מיד שנייה)     2 הלוֹא־...   \n",
       "65   מלאכי ב  1 אליכם] ל20 ק־מ\"ק: \"דׄ מטעׄ\" ; ל20־מ\"ג: \"אליכ...   \n",
       "66   מלאכי ג  1 שלח֙] ל28 30 ק: של֙ח֙  '   ופתאֹם] ר: ופתאוֹם...   \n",
       "\n",
       "                                              Entries  \n",
       "0   [1 (בספר החילופים וברשימת הסדרים שבכ\"י ל מצוין...  \n",
       "1   [1 (פ)] ל ל18 פ ר מ: (ס) ,  וְהיה(1)] ר': וַ, ...  \n",
       "2   [1 (פ)] ל29 פ: (ס) ,  אֱהב־] נ: אֶ,  אהבת] ר: ...  \n",
       "3   [1 (פ)] ל29 37 פ מ: (ס) ; ר: (>) ; ר־מ\"ק: \"פרש...  \n",
       "4   [1 (פ)] ל ל18 פ ק ש מ: (ס) ; נ: (>),  והקש֣יבו...  \n",
       "..                                                ...  \n",
       "62  [1 (>)] ל מ: (ס) ,  דויד] פ ר: דוד ,  2 נאם ׀ ...  \n",
       "63  [1 (פ)] ל ל10 18 פ מ: (ס) ,  2 הגויִ֥ם ׀ ] ק: ...  \n",
       "64  [1 >] ל: יט (התוספת \"יט\" מיד שנייה) ,  2 הלוֹא־...  \n",
       "65  [1 אליכם] ל20 ק־מ\"ק: \"דׄ מטעׄ\" ; ל20־מ\"ג: \"אלי...  \n",
       "66  [1 שלח֙] ל28 30 ק: של֙ח֙,  ופתאֹם] ר: ופתאוֹם, ...  \n",
       "\n",
       "[67 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_lines(lines)\n",
    "df = split_entries(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be98a2a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 (פ)] ל29 37 פ מ: (ס) ; ר: (>) ; ר־מ\"ק: \"פרשׄ\"',\n",
       " \" יושבי] פ ק1' ר: ישבי\",\n",
       " ' ואֵֽין־דַּ֥עת] ל20\\': ואֵ֥ין דַּעת־ ; ב\"א: ואֵֽין־דַּ֥עת, ב\"נ: ואֵ֥ין דַּֽעת־ ',\n",
       " '2 אלֹה] נ־מ\"ק: \"לו קׄ\" ; ר־מ\"ק: \"וׄ קׄ\"',\n",
       " \" ורצח] פ': רצח II; ר: רצוח\",\n",
       " ' וגנב] ר: וגנוב ',\n",
       " '3 כ֣ן ׀ ] ל18־מ\"ק: \"ל֣גׄ ׀\" ; נ ר־מ\"ק: \"לגׄ\"',\n",
       " ' יושב] ל29\\' פ: ישב ; ק1\\' ר: ישבי ; ר־מ\"ק: \"יתׄ יׄ\"III II I',\n",
       " ' בחית] ר: בחיית ',\n",
       " '4 (>)] מ: (פ) ',\n",
       " \" איש(2)] ל': אמש\",\n",
       " ' כִּמְרִיבי] ש!: כמִרְ ',\n",
       " '6 מאסת] ר: מאסתה',\n",
       " ' וְאמאסאך] מ: וָֽ ; ל18 פ ר\\': ואמאסך ; א ל ל29 ק1 מ־מ\"ק: \"יתיר אׄ\" ; ק־מ\"ק: \"ן לׄ ויתׄ אׄ\" ; ל30־מ\"ק: \"ןׄ ואמאסך קרי\" ; ר־מ\"ק: \"לׄ ויתיר אׄ\" ; ל20־מ\"ק: \"לׄ וכתׄ כן\" ; ל37־מ\"ק: \"יתי א\" ; פ־מ\"ק: \"ואמאסאך כך כתׄ ולא ק א תלתא\" ',\n",
       " '8 נפשוֹ] ר: נפשֹם ; ר־מ\"ק: \"נפשו קׄ <...>פשם כתׄ וחד מן <...>לין דכתבין <...>וׄ תיבוׄ ק וׄ\"  ; ל־מ\"ק: \"דׄ מטעׄ\" ; ל20 נ־מ\"ק: \"גׄ מטע\" ; ל29־מ\"ק: \"גׄ דמטעׄ\" ; ל29־מ\"ג: \"נפשו גׄ דמטעׄ...\" ; ל30־מ\"ג: \"נפשוֹ גׄ מטעיין וסימנהׄ...\" ; ש־מ\"ק: \"גׄ מטׄ\" ; מ־מ\"ק: \"ג\\' מטעי\\' בהון\" ; ל20־מ\"ג: \"נפשו ג מטעׄ וסימנהון ואל עונם ישאו נפשו להושיע משפטי נפשו ולא נתתי לחטא חכי לשאול באלה נפשו\" ; נ־מ\"ג: \"נפשו גׄ מטעין וסימנהון ואל עונם להושיע משפטי ולא נתתי לחטא\" ; מ־מ\"ג: \"נפשו ג\\' מטעי\\' בהון וסי\\' ואל עונם ישא נפשו. להושיע משפטי נפשו. ולא נתתי\"III I ',\n",
       " \"9 עליו] פ': >\",\n",
       " \" דרכיו ומעלליו] ק1'' = א \",\n",
       " \"10 ולא(2)] ל18': לא\",\n",
       " ' לשמר] ל18: לשמור ',\n",
       " '11 ותירוש] ר: ותירש ',\n",
       " '12 (>)] מ: (ס) ',\n",
       " \" ישאל] ל20': שאל\",\n",
       " ' ומקלו] א\\': ומקולו ; מעׄ: ומקלו, מדׄ: ומקולו ; ל20־מ\"ק: \"למדנׄ ומקולו כתׄ ופלגׄ\" ; פ־מ\"ק: \"ומקולו כת ופולׄ\" ',\n",
       " \"13 בנותיכם] ל29' ר: בנתיכם\",\n",
       " ' וכלותיכם] ל18 ר: וכלתיכם ',\n",
       " '14 אפקוד] ל18 ר: אפקד',\n",
       " \" בנותיכם] ר ש: בנתיכם ; נ': בניכם\",\n",
       " ' כלותיכם] ר: כלתיכם',\n",
       " ' כי(2)] ל18: >',\n",
       " ' יְפָרדו] פ: יִפָּ',\n",
       " ' הקְּדשות] ל20 ש: קְ ',\n",
       " '15 ואל־(1)] ל20\\' נ\\' פ\\': אל ; נ־מ\"ק: \"לבבלאי ואל\"III I ',\n",
       " '16 כְּפרה] ל20! ש!: כְ ',\n",
       " \"17–18 סר – צרר] ק'' = א \",\n",
       " \"18 אהבו הבו] ר': >\",\n",
       " ' מגניה] ר: מגיניה ',\n",
       " \"19 אותה] ל29' נ פ ר: אתה\",\n",
       " \" מזִּבְחותם] מ: מזבחתם ; ל18': זְבְּIII I\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Entries\"][3]#[5]#.strip()\n",
    "#22 וארשתיך – יהוה] ר!: כל הפסוק אינו מנוקד  '   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3c734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def lemma_verse_processor(text, chapter):\n",
    "    # Regex to match the verse numbers at the beginning\n",
    "    verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "    # Extract verses\n",
    "    verses_match = re.match(verse_regex, text)\n",
    "    if verses_match:\n",
    "        verse_range = verses_match.group(1).split('–')\n",
    "        if len(verse_range) == 2 and verse_range[0] != verse_range[1]:\n",
    "            verses = {'from': int(verse_range[0]), 'to': int(verse_range[1])}\n",
    "        else:\n",
    "            verses = int(verse_range[0])\n",
    "    else:\n",
    "        verses = None\n",
    "    \n",
    "    # Isolate lemmas part by removing the verses\n",
    "    lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "    \n",
    "    return chapter, verses, process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~\\.]*)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>[^\\(\\)]*)                           # Captures reading (excluding parentheses)\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                            # Captures comments\n",
    "    \"\"\"\n",
    "\n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v.strip() for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    # Special handling for comments containing specific markers\n",
    "    if \"Comment\" in parsed_entry and any(marker in parsed_entry['Comment'] for marker in [\"(ס)\", \"(פ)\", \"(>)\"]):\n",
    "        parsed_entry['Reading'] = parsed_entry.pop('Comment')\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "def process_lemma_with_range_and_diacritics(lemma):\n",
    "    # Adjust regex to include diacritical marks and punctuation within Hebrew words\n",
    "    \n",
    "    \n",
    "    # Check for range indicated by \"–\" and process accordingly\n",
    "    if \"–\" in lemma:\n",
    "        from_lemma, to_lemma = lemma.split(\"–\")\n",
    "        return {\n",
    "            'from': process_individual_lemma(from_lemma.strip()),\n",
    "            'to': process_individual_lemma(to_lemma.strip())\n",
    "        }\n",
    "    if len(lemma.strip().split(' '))>1:#add lemma for a two word lemma separated by a space\n",
    "        return{'lemma1':process_individual_lemma(lemma.strip().split(' ')[0]),\n",
    "              'lemma2':process_individual_lemma(lemma.strip().split(' ')[1])}\n",
    "    else:\n",
    "        return process_individual_lemma(lemma.strip())\n",
    "\n",
    "\n",
    "# Function to process individual lemmas or ranges, after the split,\n",
    "lemma_regex = r'(\\(?[^\\d\\(\\)]+\\)?)([\\(\\d\\)]*)?(.+?׀)?'#(\\d+(?:–\\d+)?)\\s\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "    processed_lemmas = []\n",
    "#     for match in matches[:1]:\n",
    "#         word, number = match\n",
    "#         lemma_dict = {'lemma': word}\n",
    "#         if number: lemma_dict['number'] = (number)\n",
    "#         processed_lemmas.append(lemma_dict)\n",
    "\n",
    "    for match in matches[:1]:\n",
    "        word, number, paseq = match\n",
    "        lemma_dict = {'lemma': word}\n",
    "        if number: lemma_dict['number'] = (number)\n",
    "        if paseq: lemma_dict['paseq'] = (paseq)\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "    return processed_lemmas\n",
    "\n",
    "def extract_cross_references(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "#split the string of the witnesses into each ms and if there is a pm\\sm\\tm as if there is a מ\"ק\\מ\"ג also\n",
    "def split_into_witnesses(text):\n",
    "    witnesses = text.split()\n",
    "    result = []\n",
    "    mgk = None\n",
    "    for witness in witnesses:\n",
    "        # Splitting the witness based on the special character ־\n",
    "        parts = witness.split('־')\n",
    "        base_witness = parts[0]\n",
    "        if len(parts)>1:\n",
    "            mgk = parts[1]\n",
    "        # Handling the ' character\n",
    "        if base_witness.endswith(\"'\") or base_witness.endswith(\"''\") or base_witness.endswith(\"'''\") or base_witness.endswith(\"!\"):\n",
    "            base_witness = base_witness.rstrip(\"'\").rstrip(\"!\")\n",
    "            annotations = witness[len(base_witness):]\n",
    "            result.append([base_witness, annotations])\n",
    "        else:\n",
    "            result.append([base_witness])\n",
    "    if mgk:\n",
    "        for ms in result:\n",
    "            ms.append(mgk)\n",
    "    return result\n",
    "\n",
    "def process_input(input_str):\n",
    "    if \"=\" in input_str and '(' in input_str:\n",
    "        # Remove the '=' and return as \"affirming variants\"\n",
    "        input_str = input_str.replace('(','').replace(')','')\n",
    "        key, value = input_str.strip().split(\"=\")\n",
    "        return {\"affirming variants\": {key.strip(): value.strip()}}\n",
    "    else:\n",
    "        if \"=\" in input_str:\n",
    "            # Return as \"affirming and unidentified\"\n",
    "            key, value = input_str.split(\"=\")\n",
    "            return {\"affirming and unidentified\": {key.strip(): value.strip()}}\n",
    "        else:\n",
    "            # Return as comment\"\n",
    "            return {\"comment\": input_str.strip()}\n",
    "\n",
    "def process_entry_variants(entry):\n",
    "    variants = entry.split(';')\n",
    "    split_variants = []\n",
    "    for variant in variants:\n",
    "        if ',' in variant:\n",
    "            var, related_var = variant.split(',')#\n",
    "            split_variants.append({'Type':'Variant','Info':var})\n",
    "            split_variants.append({'Type':'Related Variant','Info':related_var})\n",
    "        else:\n",
    "            split_variants.append({'Type':'Variant','Info':variant})          \n",
    "            \n",
    "    processed_variants = []\n",
    "    \n",
    "    for variant_dict in split_variants:\n",
    "        variant = variant_dict['Info']\n",
    "#         print(type(variant['Info']))\n",
    "        variant_type = variant_dict['Type']\n",
    "        \n",
    "        # Extract cross-references from the variant\n",
    "        clean_variant, cross_reference = extract_cross_references(variant)\n",
    "\n",
    "        if ':' in clean_variant:\n",
    "            witnesses, reading = clean_variant.split(':', 1)\n",
    "            #print(witnesses)\n",
    "            if '=' in witnesses:\n",
    "                witnesses = witnesses.replace('=', ' ') #if = in structured witnesses separated by :\n",
    "                #print(witnesses)\n",
    "            processed_variants.append({\n",
    "                'Type': variant_type,\n",
    "                'Witnesses': split_into_witnesses(witnesses),\n",
    "                'Reading Info': parse_reading_entry(reading.strip()), #need to handle + and < as well as \n",
    "                'CrossReference': cross_reference\n",
    "            })\n",
    "        else: # this means that either there is a comment here in parenthesis, or a \"= א\" format in parenthesis.\n",
    "            decoded_input = process_input(clean_variant)\n",
    "            #if 'comment' in \n",
    "            if 'comment' in list(decoded_input.keys()):\n",
    "                processed_variants.append({\n",
    "                    'Comment': decoded_input['comment'],\n",
    "                    'CrossReference': cross_reference\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                if \"affirming variants\" in list(decoded_input.keys()):\n",
    "                    processed_variants.append({\n",
    "                        'Type': 'Affirming Variant',\n",
    "                        'Witnesses': split_into_witnesses(list(decoded_input['affirming variants'].keys())[0]),\n",
    "                        'CrossReference': cross_reference\n",
    "                    })\n",
    "                else: #its a \"affirming and unidentified\" type, which means there are two variants here\n",
    "                    witnesses = split_into_witnesses(list(decoded_input['affirming and unidentified'].keys())[0])\n",
    "                    processed_variants.append({\n",
    "                        'Type': 'Affirming Variant',\n",
    "                        'Witnesses': witnesses,\n",
    "                        'CrossReference': cross_reference\n",
    "                    })\n",
    "                    processed_variants.append({\n",
    "                        'Type': 'Unidentified Variant',\n",
    "                        'Witnesses': (witnesses[0][0],\"'\"),\n",
    "                        'CrossReference': cross_reference\n",
    "                    })\n",
    "                    \n",
    "\n",
    "    return processed_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070908d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_apparatus_entries(entries, chapter):\n",
    "    processed_entries = []\n",
    "    current_verse = 1\n",
    "    for entry in entries:\n",
    "        if ']' in entry:\n",
    "            verse_lemma, entry_content = entry.split(']', 1)\n",
    "            #if digits, so there are verse numbers, if not, take the previous verse number\n",
    "            match = re.match(r'^(\\d+)(.*)', verse_lemma)\n",
    "            if match:\n",
    "                chapter, verses, lemmas = lemma_verse_processor(verse_lemma, chapter)\n",
    "                processed_entries.append({'Verse': verses, 'Lemma Info': lemmas, 'Entry': process_entry_variants(entry_content.strip())})\n",
    "                if type(verses)=='list':\n",
    "                    current_verse = verses[-1]\n",
    "                else:\n",
    "                    current_verse = verses\n",
    "            else: # no verse number, so take previous verse and process lemma separately\n",
    "                processed_entries.append({'Verse': current_verse, 'Lemma Info': process_lemma_with_range_and_diacritics(verse_lemma), 'Entry': process_entry_variants(entry_content.strip())})\n",
    "    \n",
    "        else:# no lemma, but still maybe verse number\n",
    "            match = re.match(r'^(\\d+)(.*)', entry)\n",
    "            if match:\n",
    "                verse, clean_entry = match.group(1), match.group(2)\n",
    "                processed_entries.append({'Verse': verse, 'Entry': process_entry_variants(clean_entry.strip())})\n",
    "                current_verse = verse\n",
    "            else: #no verse, use previous\n",
    "                processed_entries.append({'Verse': current_verse, 'Entry': process_entry_variants(entry.strip())})       \n",
    "\n",
    "    return processed_entries\n",
    "\n",
    "\n",
    "processed_entries = process_apparatus_entries(df[\"Entries\"][1], 'five')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6625a022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22 וארשתיך – יהוה] ר!: כל הפסוק אינו מנוקד'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Entries\"][1][-6]#.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadbc465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Verse': 5,\n",
       " 'Lemma Info': [{'lemma': 'ושתה֙'}],\n",
       " 'Entry': [{'Type': 'Variant',\n",
       "   'Witnesses': [['ל18'],\n",
       "    ['20', \"'\"],\n",
       "    ['29', \"'\"],\n",
       "    ['נ', \"'\"],\n",
       "    ['פ'],\n",
       "    ['ר'],\n",
       "    ['ק1', \"'\"]],\n",
       "   'Reading Info': {'Reading': 'ושתיה'},\n",
       "   'CrossReference': []},\n",
       "  {'Type': 'Variant',\n",
       "   'Witnesses': [['ל'],\n",
       "    ['ל18'],\n",
       "    ['20', \"'\"],\n",
       "    ['29'],\n",
       "    ['30'],\n",
       "    ['37'],\n",
       "    ['נ'],\n",
       "    ['ק'],\n",
       "    ['מ']],\n",
       "   'Reading Info': {'Reading': 'ושת֙', 'Comment': '(י)'},\n",
       "   'CrossReference': []},\n",
       "  {'Type': 'Affirming Variant',\n",
       "   'Witnesses': [['ל20', \"''\"], ['ק1', \"''\"], ['ש']],\n",
       "   'CrossReference': []}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_entries[12]#[\"Entry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bec1541",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Verse': 1,\n",
       "  'Lemma Info': [{'lemma': '(פ)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל'], ['ל18'], ['פ'], ['ר'], ['מ']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 1,\n",
       "  'Lemma Info': [{'lemma': 'וְהיה', 'number': '(1)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'וַ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 1,\n",
       "  'Lemma Info': {'lemma1': [{'lemma': 'מספ֤ר'}],\n",
       "   'lemma2': [{'lemma': 'בני־ישראל֙'}]},\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'מספ֞ר בנ֤י ישראל֙'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 2,\n",
       "  'Lemma Info': [{'lemma': 'י֥ום'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'י֖'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 3,\n",
       "  'Lemma Info': [{'lemma': '(>)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 3,\n",
       "  'Lemma Info': [{'lemma': 'ולאחותיכם'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'], ['ר']],\n",
       "    'Reading Info': {'Reading': 'ולאחתיכם'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 4,\n",
       "  'Lemma Info': [{'lemma': 'הִיא'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['פ']],\n",
       "    'Reading Info': {'Reading': 'הִוא'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 4,\n",
       "  'Lemma Info': [{'lemma': 'זנונ֙יה֙'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ש']],\n",
       "    'Reading Info': {'Reading': 'זנוניה֙'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 4,\n",
       "  'Lemma Info': [{'lemma': 'מפניהָ'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל', '!']],\n",
       "    'Reading Info': {'Reading': 'ה', 'Comment': '(לא מנוקדת)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 5,\n",
       "  'Lemma Info': [{'lemma': 'ערמה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['נ'], ['פ', \"'\"], ['ר']],\n",
       "    'Reading Info': {'Reading': 'ערומה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 5,\n",
       "  'Lemma Info': [{'lemma': 'וה֨צגת֔יה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'], ['ש']],\n",
       "    'Reading Info': {'Reading': 'והצגת֔יה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 5,\n",
       "  'Lemma Info': [{'lemma': 'הולדהּ'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'הֿ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 5,\n",
       "  'Lemma Info': [{'lemma': 'ושתה֙'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'],\n",
       "     ['20', \"'\"],\n",
       "     ['29', \"'\"],\n",
       "     ['נ', \"'\"],\n",
       "     ['פ'],\n",
       "     ['ר'],\n",
       "     ['ק1', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'ושתיה'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ל'],\n",
       "     ['ל18'],\n",
       "     ['20', \"'\"],\n",
       "     ['29'],\n",
       "     ['30'],\n",
       "     ['37'],\n",
       "     ['נ'],\n",
       "     ['ק'],\n",
       "     ['מ']],\n",
       "    'Reading Info': {'Reading': 'ושת֙', 'Comment': '(י)'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Affirming Variant',\n",
       "    'Witnesses': [['ל20', \"''\"], ['ק1', \"''\"], ['ש']],\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 7,\n",
       "  'Lemma Info': [{'lemma': '(>)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 7,\n",
       "  'Lemma Info': [{'lemma': 'הבישה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'], ['29'], ['פ'], ['ק', \"'\"], ['ר']],\n",
       "    'Reading Info': {'Reading': 'הובישה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 7,\n",
       "  'Lemma Info': [{'lemma': 'מאהבי'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18']],\n",
       "    'Reading Info': {'Reading': 'מאהבו'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ל18', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"מְאַהבַי֙ קׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 8,\n",
       "  'Lemma Info': [{'lemma': '(>)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 8,\n",
       "  'Lemma Info': [{'lemma': 'שָׂך'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', 'מ\"ג']],\n",
       "    'Reading Info': {'Reading': '\"נׄאׄ מילין דחזי להון למכתב סמך וכתׄ שין...\"'},\n",
       "    'CrossReference': ['III', 'II']}]},\n",
       " {'Verse': 8,\n",
       "  'Lemma Info': [{'lemma': 'גדרהּ'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': 'ה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 8,\n",
       "  'Lemma Info': [{'lemma': 'ונתיבותיה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל29', \"'\"], ['ק1', \"'\"], ['ר']],\n",
       "    'Reading Info': {'Reading': 'ונתיבתיה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 9,\n",
       "  'Lemma Info': [{'lemma': 'אתם'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'], ['ק1', \"'\"], ['פ']],\n",
       "    'Reading Info': {'Reading': 'אותם'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 9,\n",
       "  'Lemma Info': [{'lemma': 'ואש֙ובה֙'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ש']],\n",
       "    'Reading Info': {'Reading': 'ואשובה֙'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 10,\n",
       "  'Lemma Info': [{'lemma': 'והתירוש'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['א', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'ותירוש'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'והתירש'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 10,\n",
       "  'Lemma Info': [{'lemma': 'וכ֨סף'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': 'וכ֙סף֙'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 10, 'Entry': [{'Comment': '', 'CrossReference': []}]},\n",
       " {'Verse': 11,\n",
       "  'Lemma Info': [{'lemma': 'אשוב'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['פ', \"'\"]],\n",
       "    'Reading Info': {'Sigla': '>', 'Reading': \"ותירושי] ק1' ר: ותירשי\"},\n",
       "    'CrossReference': ['III']}]},\n",
       " {'Verse': 11,\n",
       "  'Lemma Info': [{'lemma': 'במועדו'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'במעדו'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 11,\n",
       "  'Lemma Info': [{'lemma': 'והשמת֗י'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': 'ת֤'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 12,\n",
       "  'Lemma Info': [{'lemma': 'נבלתה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר'], ['ש', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'נבלותה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 13,\n",
       "  'Lemma Info': [{'lemma': 'משושהּ'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ש', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"גׄ פסוקׄ למערׄ כל מליהון מפק הׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 14,\n",
       "  'Lemma Info': [{'lemma': 'ה֙מה֙'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ש']],\n",
       "    'Reading Info': {'Reading': 'המה֙'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 14,\n",
       "  'Lemma Info': [{'lemma': 'הבְּעלים'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל37', '!']],\n",
       "    'Reading Info': {'Reading': 'בֳּ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 15,\n",
       "  'Lemma Info': [{'lemma': 'ותַּעד'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': 'תָּ'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['נ', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"פלג וַתָ֤עַד\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 15,\n",
       "  'Lemma Info': [{'lemma': 'וחליתהּ'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'וחלייתהֿ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 16,\n",
       "  'Lemma Info': [{'lemma': '(ס)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל'], ['ל18'], ['ש']],\n",
       "    'Reading Info': {'Reading': '(פ)'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ק1'], ['מ']],\n",
       "    'Reading Info': {'Reading': '(>)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 16,\n",
       "  'Lemma Info': [{'lemma': 'והלכתיה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18'], ['ק1', \"'\"], ['ר'], ['ש', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'והולכתיה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 16,\n",
       "  'Lemma Info': [{'lemma': 'על־'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל', '!']],\n",
       "    'Reading Info': {'Reading': 'על'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 17,\n",
       "  'Lemma Info': {'lemma1': [{'lemma': 'וכי֖ום'}],\n",
       "   'lemma2': [{'lemma': 'עלות֥ה'}]},\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'וכי֥ום עלות֖ה מא֥רץ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 17,\n",
       "  'Lemma Info': [{'lemma': 'עלותה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל'], ['ל29', \"'\"], ['ר']],\n",
       "    'Reading Info': {'Reading': 'עלתה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 18,\n",
       "  'Lemma Info': [{'lemma': '(>)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['נ']],\n",
       "    'Reading Info': {'Reading': '(פ)'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ל'], ['פ'], ['ק'], ['ק1'], ['ר'], ['ש'], ['מ']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Affirming Variant',\n",
       "    'Witnesses': [['ל18'], ['20'], ['29'], ['30']],\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 18,\n",
       "  'Lemma Info': [{'lemma': 'תקראי', 'number': '(1)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'תיקראי'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['ר', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"יתׄ יׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 18,\n",
       "  'Lemma Info': [{'lemma': 'ולא־'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"מוגׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 18,\n",
       "  'Lemma Info': [{'lemma': 'לי'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"מוגׄ\"'},\n",
       "    'CrossReference': ['II']},\n",
       "   {'Comment': 'אך היא שייכת לכאן)', 'CrossReference': []}]},\n",
       " {'Verse': 18,\n",
       "  'Lemma Info': [{'lemma': 'בעְלי'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['מ']],\n",
       "    'Reading Info': {'Reading': 'עֲ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 19,\n",
       "  'Lemma Info': [{'lemma': 'והסרתי'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20']],\n",
       "    'Reading Info': {'Reading': 'והסירתי'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 19,\n",
       "  'Lemma Info': [{'lemma': 'ולא־'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל20', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"מוגׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 20,\n",
       "  'Lemma Info': [{'lemma': 'ומלחמה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'מלחמה'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 20,\n",
       "  'Lemma Info': [{'lemma': 'אשבור'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'אשבר'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 20,\n",
       "  'Lemma Info': [{'lemma': 'לעולם'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'לעלם'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 21,\n",
       "  'Lemma Info': [{'lemma': 'ובחסד'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['פ', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'בחסד'},\n",
       "    'CrossReference': []},\n",
       "   {'Type': 'Variant',\n",
       "    'Witnesses': [['פ', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"לׄ ובסיפׄ מוגׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 22,\n",
       "  'Lemma Info': {'from': [{'lemma': 'וארשתיך'}], 'to': [{'lemma': 'יהוה'}]},\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר', '!']],\n",
       "    'Reading Info': {'Reading': 'כל הפסוק אינו מנוקד'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 22,\n",
       "  'Lemma Info': [{'lemma': 'באֱמונה'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18']],\n",
       "    'Reading Info': {'Reading': 'אְ'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 22,\n",
       "  'Lemma Info': [{'lemma': 'את־'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['פ', \"'\"]],\n",
       "    'Reading Info': {'Reading': 'כי אני'},\n",
       "    'CrossReference': ['III', 'II', 'I']}]},\n",
       " {'Verse': 23,\n",
       "  'Lemma Info': [{'lemma': '(פ)'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל'], ['ל29'], ['נ'], ['פ'], ['ק1'], ['ר'], ['מ']],\n",
       "    'Reading Info': {'Reading': '(ס)'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 23,\n",
       "  'Lemma Info': {'lemma1': [{'lemma': 'והי֣ה'}], 'lemma2': [{'lemma': '׀'}]},\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ל18', 'מ\"ק'], ['נ', 'מ\"ק'], ['ר', 'מ\"ק']],\n",
       "    'Reading Info': {'Reading': '\"לגׄ\"'},\n",
       "    'CrossReference': []}]},\n",
       " {'Verse': 24,\n",
       "  'Lemma Info': [{'lemma': 'התירוש'}],\n",
       "  'Entry': [{'Type': 'Variant',\n",
       "    'Witnesses': [['ר']],\n",
       "    'Reading Info': {'Reading': 'התירש'},\n",
       "    'CrossReference': []}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_entries#[0]#[\"Entry\"]#[16]['Reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "0488ad19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(>)'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_entries[0][\"Entry\"][1]['Reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "6dc8ce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"נפשוֹ גׄ מטעיין וסימנהׄ...\"'"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_entries[14][\"Entry\"][-6]['Reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "a1dc1d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"<...>שו קׄ\" + \"נפשו קׄ <...>פשם כתׄ וחד מן <...>לין דכתבין <...>וׄ תיבוׄ ק וׄ\"'"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_entries[14][\"Entry\"][1]['Reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc94c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now need to process the reading, including sigla [+< <...>] and comments\n",
    "#also catch special marks in witnesses, like !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# old functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cadb22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_entry(text, previous_verse=None):\n",
    "    lemma, part_entry = split_full_entry(text)\n",
    "    lemma_dict = lemma_verse_processor(lemma)\n",
    "\n",
    "    # Use the previous verse if the current verse list is empty\n",
    "    if not lemma_dict['verses'] and previous_verse is not None:\n",
    "        lemma_dict['verses'] = previous_verse\n",
    "\n",
    "    # Split part_entry by '|'\n",
    "    if '|' in part_entry:\n",
    "        entry_parts = part_entry.split('|')\n",
    "    else:\n",
    "        entry_parts = [part_entry]\n",
    "\n",
    "    # Initialize a list to hold all processed parts\n",
    "    processed_parts = []\n",
    "\n",
    "    # Process each part separately\n",
    "    for part in entry_parts:\n",
    "        # Split part by ',' not inside parentheses\n",
    "        sub_parts = split_on_comma_not_in_parentheses(part)\n",
    "\n",
    "        # Process each sub-part using process_comma_entry\n",
    "        processed_sub_parts = [process_comma_entry(sub_part) for sub_part in sub_parts]\n",
    "\n",
    "        # Concatenate processed sub-parts for each part\n",
    "        processed_parts.append(processed_sub_parts)\n",
    "\n",
    "    # Combine processed parts. Assuming you want them as a nested list\n",
    "    decoded_entries = processed_parts\n",
    "\n",
    "    # Return the lemma_dict and decoded_entries, along with the verses used for this entry\n",
    "    return lemma_dict, decoded_entries, lemma_dict['verses']\n",
    "\n",
    "def split_on_comma_not_in_parentheses(part):\n",
    "    \"\"\"\n",
    "    Splits the string on ',' not inside parentheses.\n",
    "    \"\"\"\n",
    "    sub_parts = []\n",
    "    current_part = []\n",
    "    paren_depth = 0  # Track depth of parentheses\n",
    "\n",
    "    for char in part:\n",
    "        if char == '(':\n",
    "            paren_depth += 1\n",
    "        elif char == ')':\n",
    "            paren_depth -= 1\n",
    "        elif char == ',' and paren_depth == 0:\n",
    "            # At a top-level comma, split here\n",
    "            sub_parts.append(''.join(current_part))\n",
    "            current_part = []\n",
    "            continue\n",
    "\n",
    "        current_part.append(char)\n",
    "\n",
    "    # Add the last part if there's any\n",
    "    if current_part:\n",
    "        sub_parts.append(''.join(current_part))\n",
    "\n",
    "    return sub_parts\n",
    "\n",
    "def split_full_entry(text):\n",
    "    sliced_entry = text.split(sep=']')\n",
    "    lemma, entry = sliced_entry\n",
    "#         print(f\"lemma: {lemma}\")\n",
    "#         print(f\"entry: {entry}\")\n",
    "    return lemma, entry    \n",
    "\n",
    "def lemma_verse_processor(text):\n",
    "    # Simplified approach: first split into digits and lemmas\n",
    "    # Regex to match the verse numbers at the beginning\n",
    "    verse_regex = r'^(\\d+(?:–\\d+)?)\\s'\n",
    "    \n",
    "    # Extract verses\n",
    "    verses_match = re.match(verse_regex, text)\n",
    "    verses = list(map(int, verses_match.group(1).split('–'))) if verses_match else []\n",
    "    \n",
    "    # Isolate lemmas part by removing the verses\n",
    "    lemmas_part = text[len(verses_match.group(0)):].strip() if verses_match else text\n",
    "    return {\n",
    "        'verses': verses,\n",
    "        'lemmas': process_lemma_with_range_and_diacritics(lemmas_part)\n",
    "    }\n",
    "\n",
    "# Function to process individual lemmas or ranges, after the split,\n",
    "lemma_regex = r'(k|q)?\\s*([^\\d\\s]+)(\\d?\\,?\\d?)'#(\\d+(?:–\\d+)?)\\s\n",
    "\n",
    "def process_lemma_with_range_and_diacritics(lemma):\n",
    "    # Adjust regex to include diacritical marks and punctuation within Hebrew words\n",
    "    \n",
    "    \n",
    "    # Check for range indicated by \"–\" and process accordingly\n",
    "    if \"–\" in lemma:\n",
    "        from_lemma, to_lemma = lemma.split(\"–\")\n",
    "        return {\n",
    "            'from': process_individual_lemma(from_lemma.strip()),\n",
    "            'to': process_individual_lemma(to_lemma.strip())\n",
    "        }\n",
    "\n",
    "    # Split lemma if there are separate lemmas with \"/\"\n",
    "    split_lemmas = re.split(r'\\s*/\\s*', lemma) if '/' in lemma else [lemma]\n",
    "    \n",
    "    processed_lemmas = []\n",
    "    for split_lemma in split_lemmas:\n",
    "        processed = process_individual_lemma(split_lemma)\n",
    "        processed_lemmas.extend(processed)\n",
    "    \n",
    "    return processed_lemmas\n",
    "\n",
    "def process_individual_lemma(individual_lemma):\n",
    "    matches = re.findall(lemma_regex, individual_lemma)\n",
    "    processed_lemmas = []\n",
    "    for match in matches:\n",
    "        prefix, word, number = match\n",
    "        lemma_dict = {'lemma': word}\n",
    "        if prefix: lemma_dict[prefix] = True\n",
    "        if number: lemma_dict['number'] = (number)\n",
    "        processed_lemmas.append(lemma_dict)\n",
    "    return processed_lemmas\n",
    "\n",
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def extract_cross_references(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "def parse_witnesses(text):\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d.]+)?\\)?', re.DOTALL | re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    # Filter out empty tuples\n",
    "    return [part for part in parts if any(part)]\n",
    "\n",
    "def parse_comma_witnesses(text):\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d*)?\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL | re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    # Filter out empty tuples\n",
    "    return [part for part in parts if any(part)]\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~\\.]*)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]*\\s?)[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "#splitting entry into witnesses and reading (if only one group assign to witnesses)\n",
    "def witness_reading_splitter(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>~.]*\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>~])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "\n",
    "def process_entry(entry):\n",
    "    clean_entry, cross_references = extract_cross_references(entry)\n",
    "    split_entry = witness_reading_splitter(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry[0])}\n",
    "        if len(split_entry) == 2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else:  # there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1] + split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': parse_witnesses(split_entry)}\n",
    "        reading = ''\n",
    "    # Include \"Cross References\" only if the list is not empty\n",
    "    result = [witnesses, {\"Reading\": reading}]\n",
    "    if cross_references:\n",
    "        result.append({\"Cross References\": cross_references})\n",
    "    return result\n",
    "\n",
    "def process_comma_entry(entry):\n",
    "    clean_entry, cross_references = extract_cross_references(entry)\n",
    "    split_entry = witness_reading_splitter(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': parse_comma_witnesses(split_entry[0])}\n",
    "        if len(split_entry) == 2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else:  # there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1] + split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': parse_comma_witnesses(split_entry)}\n",
    "        reading = ''\n",
    "    # Include \"Cross References\" only if the list is not empty\n",
    "    result = [witnesses, {\"Reading\": reading}]\n",
    "    if cross_references:\n",
    "        result.append({\"Cross References\": cross_references})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over full entries. get verse number from previous if needed. also get reading from lemma. and split on | ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "715948c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process lemma:\n",
    "#split into digits and lemmas. then process each separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "b18607da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'verses': [10], 'lemmas': [{'lemma': 'ואסרם'}]},\n",
       " [[({'Witnesses': [('', '96', 'pm')]},\n",
       "    {'Reading': {'Reading': 'יאשרם '}},\n",
       "    {'Cross References': []})],\n",
       "  [({'Witnesses': [('', '150', '')]},\n",
       "    {'Reading': {'Reading': 'על'}},\n",
       "    {'Cross References': []})]])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[process_full_entry(example) for example in full_entries][-1]#[0]['verses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d89e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "b8c31720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old function, didnt take the splitting into commas consideration\n",
    "# def process_full_entry(text):\n",
    "#     lemma, part_entry = split_full_entry(text)\n",
    "#     lemma_dict = lemma_verse_processor(lemma)\n",
    "# #     if len(lemma_dict['verses'])==0: #get verse from previous entry\n",
    "# #         lemma_dict['verses'] = \n",
    "        \n",
    "#     #entry_units = split_entry_units # splits on | and ,\n",
    "#     #for entry in entry_units:\n",
    "#     decoded_entry = process_entry(part_entry)\n",
    "#     return lemma_dict, decoded_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "4b9188ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing functions for sub-units of app_entry, for which there is matching lemma and verse data processed above\n",
    "\n",
    "def remove_and_list_roman_numerals(text): #extract cross-references\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "def custom_split_string(text): #process witnesses\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~]?)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "#splitting entry into witnesses and reading (if only one group assign to witnesses)\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>]?\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "\n",
    "def process_entry(entry):\n",
    "    clean_entry, cross_references = remove_and_list_roman_numerals(entry)\n",
    "    split_entry = split_string(clean_entry)\n",
    "    if type(split_entry) is tuple:\n",
    "        witnesses = {'Witnesses': custom_split_string(split_entry[0])}\n",
    "        if len(split_entry)==2:\n",
    "            reading = parse_reading_entry(split_entry[1])\n",
    "        else: #there are 3 groups:\n",
    "            reading = parse_reading_entry(split_entry[1]+split_entry[2])\n",
    "    else:\n",
    "        witnesses = {'Witnesses': custom_split_string(split_entry)}\n",
    "        reading = ''\n",
    "    return witnesses, {\"Reading\":reading}, {\"Cross References\":cross_references}\n",
    "\n",
    "\n",
    "# for entry in sample_texts:\n",
    "#     print(f\"entry: {entry}\")\n",
    "#     clean_entry, cross_references = remove_and_list_roman_numerals(entry)\n",
    "#     split_entry = split_string(clean_entry)\n",
    "#     if type(split_entry) is tuple:\n",
    "#         witnesses = {'witnesses': custom_split_string(split_entry[0])}\n",
    "#         reading = parse_reading_entry(split_entry[1])\n",
    "#         print(f\"witnesses: {witnesses}\")\n",
    "#         print(f\"reading: {reading}\")\n",
    "#     else:\n",
    "#         witnesses = {'witnesses': custom_split_string(split_entry)}\n",
    "#         print(f\"witnesses: {witnesses}\")\n",
    "#     print(f\"references: {cross_references}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "cd41862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"96 (non voc)\",\n",
    "    \"30 (pm) 93 (pm) 150 (pm) + סךII IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))\",\n",
    "    \"93 (non voc) 96 150 (non voc) + את\",\n",
    "    \"30 (pm) >\",\n",
    "    \"30 + לי (non voc)I II\",\n",
    "    \"30 (pm) >I II IV (similarly b. Pesaḥim 87bmss)\",\n",
    "    \"93 (pm) ביהושעIV (similarly PesiqtaR 33 (153b))\",\n",
    "    \"96 >I II IV\",\n",
    "    \"130 k\",\n",
    "    \"G-B Msr 34 k ממני / q ממנוIV\",\n",
    "    \"93 כד..\",\n",
    "    \"150 ..דברים\",\n",
    "    \"G-B Eb 94 ותָעָד (understood as \\עוד (rather than \\עדי))\",\n",
    "    \"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "a0ae7422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Witnesses': [('', '93', 'pm')]},\n",
       " {'Reading': {'Reading': 'ביהושע ',\n",
       "   'Comment': '(similarly PesiqtaR 33 (153b))'}},\n",
       " {'Cross References': ['IV']})"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_entry(sample_texts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "2a94af94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Reading': 'ביהושע ', 'Comment': '(similarly PesiqtaR 33 (153b))'}"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_entry, cross_references = remove_and_list_roman_numerals(sample_texts[6])\n",
    "split_entry = split_string(clean_entry)\n",
    "if type(split_entry) is tuple:\n",
    "    witnesses = {'Witnesses': custom_split_string(split_entry[0])}\n",
    "    if len(split_entry)==2:\n",
    "        reading = parse_reading_entry(split_entry[1])\n",
    "    else: #there are 3 groups:\n",
    "        reading = parse_reading_entry(split_entry[1]+split_entry[2])\n",
    "\n",
    "reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "08fa1e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '30', 'pm'), ('', '93', ''), ('', '150', 'pm')]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_split_string(text): #process witnesses\n",
    "    pattern = re.compile(r'\\s?([^\\d]*?)?(\\d+)\\s?\\(?([^\\)\\d]+)?\\)?', re.DOTALL|re.UNICODE)    \n",
    "    #pattern = r'([^,\\d]*?)?(\\d+)\\s?(\\(([^\\)]+)?\\)?)?([\\skq]?)+'\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "# \"96 (non voc)\",\n",
    "# \"30 (pm) 93 (pm) 150 (pm)\n",
    "test_witness = \"30 (pm) 93 150 (pm)\"\n",
    "custom_split_string(test_witness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "fbefcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['96 (non voc)', ('30 (pm) 93 (pm) 150 (pm) ', '+ סך', 'II IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'), ('93 (non voc) 96 150 (non voc) ', '+ את', ''), ('30 (pm) ', '>', ''), ('30 ', '+ לי', ' (non voc)I II'), ('30 (pm) ', '>', 'I II IV (similarly b. Pesaḥim 87bmss)'), ('93 (pm)', ' ביהושע', 'IV (similarly PesiqtaR 33 (153b))'), ('96 ', '>', 'I II IV'), ('130', ' k', ''), ('G-B Msr 34', ' k', ' ממני / q ממנוIV'), ('93', ' כד', '..'), ('150 ..', 'דברים', ''), ('G-B Eb 94', ' ותָעָד', ' (understood as \\\\עוד (rather than \\\\עדי))'), ('30 89 (sm) 93 (pm) 150 (non voc) ', '+ כי', 'I II IV')]\n"
     ]
    }
   ],
   "source": [
    "#try parsing single entry app, splitting into witnesses and reading (if only one group assign to witnesses)\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'(.*?)?([\\+<>]?\\s?[kq\\u0590-\\u05FF]+)(.*)?', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        pattern = re.compile(r'(.*?)([\\+<>])(.*)?', re.DOTALL)\n",
    "        match = pattern.match(text)\n",
    "        if match:\n",
    "            return match.groups()  # Returns a tuple with the three parts\n",
    "        else:\n",
    "            return text\n",
    "        return text  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "processed_sample = [split_string(text) for text in sample_texts]\n",
    "\n",
    "print(processed_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "473068c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Reading': 'סך ',\n",
       "  'Comment': '(See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'},\n",
       " {'Sigla': '+', 'Reading': 'את'},\n",
       " {'Sigla': '>'},\n",
       " {'Sigla': '+', 'Reading': 'לי ', 'Comment': '(non voc)'},\n",
       " {'Sigla': '>', 'Comment': '(similarly b. Pesaḥim 87bmss)'},\n",
       " {'Reading': 'ביהושע ', 'Comment': '(similarly PesiqtaR 33 (153b))'},\n",
       " {},\n",
       " {'Reading': 'k'},\n",
       " {'Reading': 'k ממני / q ממנו'},\n",
       " {'Reading': 'כד..'},\n",
       " {'Reading': '..דברים'},\n",
       " {'Reading': 'נַחֵם ',\n",
       "  'Comment': '(taken as infinitive, see Yeivin, Babylonian Vocalization, 1:542)'},\n",
       " {'Reading': 'חכֵם ', 'Comment': '(!)'}]"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for parsing the reading+comment (assumes witnesses and cross references have been removed)\n",
    "def parse_reading_entry(entry):\n",
    "    # Refined regex pattern\n",
    "    pattern = r\"\"\"\n",
    "        \\s?(?P<Sigla>[+<>~]?)                         # Captures special sigla\n",
    "        \\s*\n",
    "        (?P<Reading>(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*    # Hebrew reading, including 'k', 'q'\n",
    "                   (?:/\\s(?:[kq]?\\s?)?[\\u0590-\\u05FF\\uFB1D-\\uFB4F\\s.]*)?)  # Allows for 'k'/'q' followed by Hebrew, separated by '/'\n",
    "        \\s*\n",
    "        (?P<Comment>\\(.*\\))?                     # Captures comments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compiling regex with VERBOSE flag for better readability and explanation\n",
    "    compiled_pattern = re.compile(pattern, re.VERBOSE)\n",
    "    match = compiled_pattern.match(entry)\n",
    "\n",
    "    if not match:\n",
    "        return None  # Return None if no match is found\n",
    "\n",
    "    # Extracting groups into a dictionary\n",
    "    parsed_entry = {k: v for k, v in match.groupdict().items() if v}\n",
    "\n",
    "    return parsed_entry\n",
    "\n",
    "# Process the sample reading texts with the refined function\n",
    "parse_reading_entry = [parse_reading_entry(text) for text in sample_reading_texts]\n",
    "\n",
    "parse_reading_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "94a5de6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [{'witnesses': '30 (pm) 93 (pm) 150 (pm)', 'reading': '+ סך', 'comments': 'II IV (See b. R.HaŠanamss 23b, (LamR) Buber 1:16 (40b))'}], [{'witnesses': '93 (non voc) 96 150 (non voc)', 'reading': '+ את', 'comments': ''}], [], [{'witnesses': '30', 'reading': '+ לי', 'comments': '(non voc)I II'}], [], [{'witnesses': '93 (pm)', 'reading': 'ביהושע', 'comments': 'IV (similarly PesiqtaR 33 (153b))'}], [], [], [{'witnesses': 'G-B Msr 34 k', 'reading': 'ממני', 'comments': '/ q ממנוIV'}], [{'witnesses': '93', 'reading': 'כד', 'comments': '..'}], [{'witnesses': '150 ..', 'reading': 'דברים', 'comments': ''}], [{'witnesses': 'G-B Eb 94', 'reading': 'ותָעָד', 'comments': '(understood as \\\\עוד (rather than \\\\עדי))'}], [{'witnesses': '30 89 (sm) 93 (pm) 150 (non voc)', 'reading': '+ כי', 'comments': 'I II IV'}]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_string_processor(input_string, regex_pattern):\n",
    "    # Helper function to apply regex and extract groups\n",
    "    def apply_regex_and_extract(text):\n",
    "        matches = re.finditer(regex_pattern, text)\n",
    "        results = []\n",
    "        for match in matches:\n",
    "            results.append({\n",
    "                'witnesses': match.group(1).strip(),\n",
    "                'reading': match.group(2).strip(),\n",
    "                'comments': match.group(3).strip() if match.group(3) else ''\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Process splits with \"|\", then \",\"\n",
    "    def process_splits(text, delimiter):\n",
    "        parts = text.split(delimiter)\n",
    "        processed_parts = []\n",
    "        for part in parts:\n",
    "            # Apply regex to each part\n",
    "            processed = apply_regex_and_extract(part)\n",
    "            if processed:\n",
    "                processed_parts.extend(processed)\n",
    "        return processed_parts\n",
    "\n",
    "    # Start processing\n",
    "    processed_result = process_splits(input_string, '|')  # Start with the highest level of split\n",
    "\n",
    "    return processed_result\n",
    "\n",
    "# Custom regex pattern as provided\n",
    "custom_regex = r'^(.*?)([\\+<~>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$'\n",
    "\n",
    "# Test with the provided sample input\n",
    "sample_input = \"G-B msr. 30 (pm) G-A 89 (sm?) 150 (non voc) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) + שערורהIV II (bla bla (f)) | 150 >\"\n",
    "processed_sample = [custom_string_processor(text, custom_regex) for text in sample_texts]\n",
    "\n",
    "\n",
    "print(processed_sample)\n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        return None  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "# text = \"G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV (bla bla (f))\"#\"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"#\"93 (pm) < ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "# split_parts = split_string(text)\n",
    "# if split_parts:\n",
    "#     print(\"witnesses:\", split_parts[0])\n",
    "#     print(\"reading:\", split_parts[1])\n",
    "#     print(\"After dividers:\", split_parts[2])\n",
    "# else:\n",
    "#     print(\"No dividers found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a5b4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_apparatus_entry(entry):\n",
    "    \"\"\"Parse an apparatus entry into lemma(s) and content.\"\"\"\n",
    "    parts = entry.split(']')\n",
    "    lemmas_contents = []\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            lemma, content = part.split('[', 1) if '[' in part else (part, '')\n",
    "            lemmas_contents.append((lemma.strip(), content.strip()))\n",
    "    return lemmas_contents\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    \"\"\"Create a TEI document from apparatus lines.\"\"\"\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    NSMAP = {\"tei\": TEI_NAMESPACE}\n",
    "    \n",
    "    tei_root = ET.Element(TEI+\"TEI\", nsmap=NSMAP)\n",
    "    tei_header = ET.SubElement(tei_root, TEI+\"teiHeader\")\n",
    "    text = ET.SubElement(tei_root, TEI+\"text\")\n",
    "    body = ET.SubElement(text, TEI+\"body\")\n",
    "    current_chapter = None\n",
    "    last_verse = None\n",
    "    \n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('Chapter'):\n",
    "            chapter_number = line.split(' ')[1]\n",
    "            current_chapter = ET.SubElement(body, TEI+\"div\", type=\"chapter\", n=chapter_number)\n",
    "            last_verse = None\n",
    "            continue\n",
    "        # Use regex to check if the line starts with a verse number and capture it\n",
    "        match = re.match(r\"^(\\d+)\\s*(.*)\", line)\n",
    "        if match:\n",
    "            verse_number, entry = match.groups()\n",
    "            last_verse = verse_number\n",
    "        else:\n",
    "            entry = line\n",
    "            verse_number = last_verse\n",
    "        \n",
    "        if current_chapter is not None and verse_number:\n",
    "            lemmas_contents = parse_apparatus_entry(entry)\n",
    "            for lemma, content in lemmas_contents:\n",
    "                app = ET.SubElement(current_chapter, TEI+\"app\")\n",
    "                lem = ET.SubElement(app, TEI+\"lem\", n=verse_number)\n",
    "                lem.text = lemma\n",
    "                if content:\n",
    "                    rdg = ET.SubElement(app, TEI+\"rdg\")\n",
    "                    rdg.text = content\n",
    "\n",
    "    return ET.ElementTree(tei_root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    \"\"\"Save the TEI XML tree to a file.\"\"\"\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\", short_empty_elements=True)\n",
    "\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caaa3e86",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line does not conform to expected format: ﻿App III: Hosea\n",
      "Line does not conform to expected format: יחזקיה] 30 93 (pm) 96 יחזקיהו\n",
      "Line does not conform to expected format: ירבעם בן] 30 + נבט (non voc)\n",
      "Line does not conform to expected format: לו] 96 >I  II IV\n",
      "Line does not conform to expected format: ממלכוּת] 96 ממלכוֹת\n",
      "Line does not conform to expected format: יזרעאל] 150 ישראל (parall; but 150-Tg: יזרעאל)\n",
      "Line does not conform to expected format: כי] 93 (pm) + את\n",
      "Line does not conform to expected format: אוסיף] 150 (pm) >\n",
      "Line does not conform to expected format: את] 93 (pm) >\n",
      "Line does not conform to expected format: בסוסים] 96 ובסוסיםI IV\n",
      "Line does not conform to expected format: אשר2] 96 + לא\n",
      "Line does not conform to expected format: אחד] 30 (pm) 150 (pm) >\n",
      "Line does not conform to expected format: והצגתיה] 93 (pm) + כיום ערומה והצגתיה\n",
      "Line does not conform to expected format: ושתִּה] 150 (pm) ושמתיה\n",
      "Line does not conform to expected format: כי] 150 (non voc) + כי\n",
      "Line does not conform to expected format: שכְחה] 150 (pm) שכחת\n",
      "Line does not conform to expected format: מפתיה] 89 מְפַּתֶיהָ\n",
      "Line does not conform to expected format: המדבר] 96 המדברה (similarly SifreDeut 313 (356:6), RuthR 5:6)\n",
      "Line does not conform to expected format: תקראי1] 150 (pm) תקראו | 30 + לי (non voc)I II\n",
      "Line does not conform to expected format: לי] G-B Eb 54 (pm) >II\n",
      "Line does not conform to expected format: עוד] 30 (pm?) >\n",
      "Line does not conform to expected format: את] 30 89 (sm) 93 (pm) אניI II IV\n",
      "Line does not conform to expected format: לא] 96 ולאI\n",
      "Line does not conform to expected format: תזני] 93 (pm) תתי (Caused by ligature ז+נ)\n",
      "Line does not conform to expected format: אליך] 93 (pm) אלהיך\n",
      "Line does not conform to expected format: וגם] 30 (pm) וכל\n",
      "Line does not conform to expected format: אתה] 96 (pm) את\n",
      "Line does not conform to expected format: k ואמאסאך / q ואמאסך] 30 93 (sm) 96 150 (pm) q IV | 93 (pm) אמסך\n",
      "Line does not conform to expected format: ותשכח] 150 (pm) תשכחי\n",
      "Line does not conform to expected format: נפשו] 89 (pm) 96 (pm) 150 (pm) נפשםI IV\n",
      "Line does not conform to expected format: עליו] 96 + ודמו (non voc)\n",
      "Line does not conform to expected format: דרכיו] 30 (pm) כדרכיו\n",
      "Line does not conform to expected format: הזנו] 150 (pm) והזנו\n",
      "Line does not conform to expected format: כי] 96 >\n",
      "Line does not conform to expected format: צלה] 93 (pm) יגלה\n",
      "Line does not conform to expected format: וכלותיכם] 150 (pm) + כי\n",
      "Line does not conform to expected format: הזֹנות] 93 (pm) זנות\n",
      "Line does not conform to expected format: יְפָרדו] 93 96 יִפָּרדו\n",
      "Line does not conform to expected format: ואל1] 150 (pm) אלI IV\n",
      "Line does not conform to expected format: ואַל3] 30 ואֵל\n",
      "Line does not conform to expected format: אהבו] 93 (pm) אתם\n",
      "Line does not conform to expected format: קלון] 150 (pm) קלו\n",
      "Line does not conform to expected format: מגניה] 96 (pm) מקלון\n",
      "Line does not conform to expected format: מִזִּבְחותם] 30 93 150 מִזְבְּחתםI IV | 96 מִזְבְחותם\n",
      "Line does not conform to expected format: אל] 93 (pm) + אלI IV\n",
      "Line does not conform to expected format: בקרבם] 96 (pm) בקרבכם\n",
      "Line does not conform to expected format: עמם] 93 (pm) עמכם\n",
      "Line does not conform to expected format: את] 93 + דבר (non voc)II IV\n",
      "Line does not conform to expected format: עתה] 93 (pm) ועתה\n",
      "Line does not conform to expected format: את] 30 (pm) >\n",
      "Line does not conform to expected format: בית] 150 בין (similarly b. R.HaŠanams 32b)\n",
      "Line does not conform to expected format: אחריך] 93 (pm) >\n",
      "Line does not conform to expected format: גבול] 93 + עולם (non voc) (cf גבול עולם Prov 2228 2310)\n",
      "Line does not conform to expected format: וְכָרקב] 93 96 וּכְרקב\n",
      "Line does not conform to expected format: וישלח] 93 מ..\n",
      "Line does not conform to expected format: מיֹמים] 30 (pm?) >\n",
      "Line does not conform to expected format: כמלקוש] 150 (pm) ומלקושI II IV\n",
      "Line does not conform to expected format: מה2] 30 93 150 (pm) ומהI\n",
      "Line does not conform to expected format: לְךָ2] 96 לָךְ | 30 (pm) + אפרים\n",
      "Line does not conform to expected format: וחסדכם] 93 150 חסדכםI\n",
      "Line does not conform to expected format: הֹלך] 96 והולךI\n",
      "Line does not conform to expected format: חבר] 93 (pm) וחבר\n",
      "Line does not conform to expected format: שׁם] 96 שׂם\n",
      "Line does not conform to expected format: וגנב] 30 וכגנב\n",
      "Line does not conform to expected format: פשט] 93 (pm) ופשטI (similarly S.Eli.R 22 (125))\n",
      "Line does not conform to expected format: בחוץ] 96 (pm) בחרץ\n",
      "Line does not conform to expected format: מעיר] 96 (sm) עיר\n",
      "Line does not conform to expected format: מלוש] 150 (pm) בלוש\n",
      "Line does not conform to expected format: שרים] 30 (pm) >\n",
      "Line does not conform to expected format: ידו את] 30 ~\n",
      "Line does not conform to expected format: אֹפֵהֶם] 93 (pm) אפריםI\n",
      "Line does not conform to expected format: לב] 89 (pm) לבי\n",
      "Line does not conform to expected format: איסירם] 30 89 איסרם | 96 (pm) אייסרם, (sm) אייסירים | 150 (pm) אסירים\n",
      "Line does not conform to expected format: פשעוּ] 96 פשעִי\n",
      "Line does not conform to expected format: ואנכי] 150 (pm) ואניIV\n",
      "Line does not conform to expected format: דברו] 93 (pm) >\n",
      "Line does not conform to expected format: בלבם] 93 (pm) בלבבםIV\n",
      "Line does not conform to expected format: יתגוררו] 93 (pm) >\n",
      "Line does not conform to expected format: על היו] 150 (pm) ~\n",
      "Line does not conform to expected format: זוֹ] 96 זוּ\n",
      "Line does not conform to expected format: עברו] 96 (pm) עבר\n",
      "Line does not conform to expected format: תורָתי] 89 (pm) תורֹתי\n",
      "Line does not conform to expected format: בו] 96 בֹה\n",
      "Line does not conform to expected format: שרים] 96 150 ושריםI II IV\n",
      "Line does not conform to expected format: אפרים] 30 (pm) ישראל\n",
      "Line does not conform to expected format: לו] 93 (pm) לי\n",
      "Line does not conform to expected format: k רבו / q רבי] 30 89 93 96 q, 150 (pm) רובו\n",
      "Line does not conform to expected format: יהוה] 150 (pm) ויהוהI IV\n",
      "Line does not conform to expected format: עתה] 150 (pm) ועתה\n",
      "Line does not conform to expected format: חטֹאותם] 89 93 96 150 (pm) חטאתם, 30 חטָאתם\n",
      "Line does not conform to expected format: המה] 150 (pm) והמהIV\n",
      "Line does not conform to expected format: הרבה] 30 (pm) + מזבחות\n",
      "Line does not conform to expected format: ואכלה] 150 (pm) + כל\n",
      "Line does not conform to expected format: ארמנתיה] 93 (pm) ארמנותיו\n",
      "Line does not conform to expected format: בית] 93 (pm) >\n",
      "Line does not conform to expected format: לכספם] 150 לנפשם\n",
      "Line does not conform to expected format: אויל] 96 (pm) >\n",
      "Line does not conform to expected format: רֹב] 96 רַב\n",
      "Line does not conform to expected format: עונְךָ] 96 עונֵךְ\n",
      "Line does not conform to expected format: ורבה] 150 (pm) רבהIV\n",
      "Line does not conform to expected format: נביא] 96 (pm) הנביא\n",
      "Line does not conform to expected format: יקוֹש] 93 96 יקוּש\n",
      "Line does not conform to expected format: חטאותם] 89 93 150 (pm) חטֹאתם, 30 חטָאתם\n",
      "Line does not conform to expected format: וַינזרו] 30 וְינזרו\n",
      "Line does not conform to expected format: ויהיו] 96 (pm) ויהי\n",
      "Line does not conform to expected format: רחם] 96 (pm) מרחם\n",
      "Line does not conform to expected format: מביתי] 93 (pm) מביתIV\n",
      "Line does not conform to expected format: ילדון] 93 (pm) ילזון\n",
      "Line does not conform to expected format: לו] 150 >\n",
      "Line does not conform to expected format: למזבחות] 93 (pm) למזבח\n",
      "Line does not conform to expected format: יָרֵאנו] 30 יַרְאֵנו\n",
      "Line does not conform to expected format: כרֹת] 96 (pm) וכרותI\n",
      "Line does not conform to expected format: ופרח] 150 (pm) ופתח\n",
      "Line does not conform to expected format: וכְמריו] G-B Msr 34 כֹ\n",
      "Line does not conform to expected format: יגילו] 150 (sm) יגלו\n",
      "Line does not conform to expected format: על] 93 (pm) ועל\n",
      "Line does not conform to expected format: ממנו] 96 (pm) ממני\n",
      "Line does not conform to expected format: מלכהּ] 93 (pm) מלכם\n",
      "Line does not conform to expected format: מים] 93 (pm) המים\n",
      "Line does not conform to expected format: כַּסונו] 89 כִּסונו (?)\n",
      "Line does not conform to expected format: חטאתָ] 93 (sm) 96 (sm) חטא (ת non voc)\n",
      "Line does not conform to expected format: עלוה] 89 (sm) עולהI\n",
      "Line does not conform to expected format: k עינתם / q עונֹתם] 30 k, 89 G-B Eb 16 q, 93 96 150 (pm) עונותם\n",
      "Line does not conform to expected format: ועת] 150 (pm) עת\n",
      "Line does not conform to expected format: לדרוש] 96 >\n",
      "Line does not conform to expected format: את] 30 (pm) >, 96 (non voc)\n",
      "Line does not conform to expected format: יהוה] 93 (pm) יהודה\n",
      "Line does not conform to expected format: ויֹרה] 93 (pm) G-B Eb 16 יורה\n",
      "Line does not conform to expected format: שלמן] 96 שלומך\n",
      "Line does not conform to expected format: ארבֵאל] 93 ארבְּאֵל\n",
      "Line does not conform to expected format: ולפסִלים] 96 לפסילים\n",
      "Line does not conform to expected format: רפאתים] 96 (pm) רפאתיו\n",
      "Line does not conform to expected format: לחֵיהם] 30 93 לחָיֵיהם\n",
      "Line does not conform to expected format: ממֹעצותיהם] 96 (pm) ממעֲוצותיהם\n",
      "Line does not conform to expected format: למשובתי] 30 (pm) למשבתוI\n",
      "Line does not conform to expected format: ולא2] 93 (sm) 150 (pm) לאII IV\n",
      "Line does not conform to expected format: ושֹׁד] 93 (pm) >\n",
      "Line does not conform to expected format: וברית] 96 (pm) ובריית\n",
      "Line does not conform to expected format: ישיב] 30 (pm) 150 (pm) אשיבI\n",
      "Line does not conform to expected format: בגלגל] 150 (pm) ובגלגל\n",
      "Line does not conform to expected format: זבחו] 93 (pm) >\n",
      "Line does not conform to expected format: גם מזבחותם] 96 >\n",
      "Line does not conform to expected format: ויושיעֲך] 96 (pm) ויושיעוך\n",
      "Line does not conform to expected format: לי] 96 לנו\n",
      "Line does not conform to expected format: הוא] 93 (pm) 150 (pm) והואIV\n",
      "Line does not conform to expected format: לא1] 93 ולא\n",
      "Line does not conform to expected format: דבריך] 30 93 (pm) 96 דברךI II IV\n",
      "Line does not conform to expected format: נֹחם] G-B Msr 34 נַחֵם (taken as infinitive, see Yeivin, Babylonian Vocalization, 1:542)\n",
      "Line does not conform to expected format: ויבוש] 93 (pm) תיבש\n",
      "Line does not conform to expected format: הוא2] 93 (pm) 96 150 (pm) והואI II\n",
      "Line does not conform to expected format: אמרו] 30 (pm) 93 (pm) 96 150 (pm) ואמרוII IV\n",
      "Line does not conform to expected format: כל] 89 (pm) בלI\n",
      "Line does not conform to expected format: וצדקים] 96 צדיקים\n",
      "Line does not conform to expected format: ‏App III: Hosea\n",
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    \n",
    "    tei_root = ET.Element(TEI + \"TEI\", xmlns=TEI_NAMESPACE)\n",
    "    tei_header = ET.SubElement(tei_root, TEI + \"teiHeader\")\n",
    "    text = ET.SubElement(tei_root, TEI + \"text\")\n",
    "    body = ET.SubElement(text, TEI + \"body\")\n",
    "    current_chapter = None\n",
    "    last_verse_number = None\n",
    "    \n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('Chapter'):\n",
    "            chapter_number = line.split(' ')[1].strip()\n",
    "            current_chapter = ET.SubElement(body, TEI + \"div\", type=\"chapter\", n=chapter_number)\n",
    "        else:\n",
    "            # Attempt to extract verse number and lemma content\n",
    "            parts = re.match(r\"^(\\d+)\\s*(.*)\", line)\n",
    "            if parts:\n",
    "                verse_number, remainder = parts.groups()\n",
    "                last_verse_number = verse_number  # Update last verse number with current\n",
    "                \n",
    "                # Further split to separate lemma from variants, if present\n",
    "                lemma_section, variants_section = remainder.split(']', 1) if ']' in remainder else (remainder, \"\")\n",
    "                lemma_section = lemma_section.strip()\n",
    "                variants_section = variants_section.strip()\n",
    "\n",
    "                if current_chapter is not None and verse_number:\n",
    "                    # Create an apparatus entry for the lemma\n",
    "                    app = ET.SubElement(current_chapter, TEI + \"app\")\n",
    "                    lem = ET.SubElement(app, TEI + \"lem\", n=verse_number)\n",
    "                    lem.text = lemma_section\n",
    "                    \n",
    "                    # Add variant readings if present\n",
    "                    if variants_section:\n",
    "                        rdg = ET.SubElement(app, TEI + \"rdg\")\n",
    "                        rdg.text = variants_section\n",
    "            else:\n",
    "                print(f\"Line does not conform to expected format: {line}\")\n",
    "\n",
    "    return ET.ElementTree(tei_root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\", short_empty_elements=True)\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808c9cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "def create_apparatus_entry(verse_number, content, TEI):\n",
    "    \"\"\"Create TEI element for an apparatus entry.\"\"\"\n",
    "    app = ET.Element(TEI + \"app\")\n",
    "    \n",
    "    # Extract lemma text and the rest (witnesses, variant reading, and comments)\n",
    "    lemma_text, _, rest = content.partition(']')\n",
    "    lem = ET.SubElement(app, TEI + \"lem\")\n",
    "    lem.text = lemma_text.strip()\n",
    "    \n",
    "    # Extract comments\n",
    "    comments = re.findall(r'\\((.*?)\\)', rest)\n",
    "    for comment in comments:\n",
    "        note = ET.SubElement(app, TEI + \"note\")\n",
    "        note.text = comment\n",
    "    \n",
    "    # Remove comments from rest for further processing\n",
    "    rest = re.sub(r'\\(.*?\\)', '', rest).strip()\n",
    "    \n",
    "    # Extract and process witnesses and cross-references\n",
    "    if rest:\n",
    "        rdg = ET.SubElement(app, TEI + \"rdg\")\n",
    "        witnesses, _, variant_reading = rest.partition(' ')\n",
    "        if witnesses:\n",
    "            rdg.set('wit', witnesses.strip())\n",
    "        if variant_reading:\n",
    "            rdg.text = variant_reading.strip()\n",
    "        \n",
    "        # Extract cross-references, assuming they are indicated by Roman numerals at the start\n",
    "        cross_refs = re.findall(r'\\bI{1,3}V?|\\bIV', rest)\n",
    "        for ref in cross_refs:\n",
    "            ref_element = ET.SubElement(rdg, TEI + \"ref\")\n",
    "            ref_element.set('target', '#' + ref)  # Assuming target IDs are prefixed with '#'\n",
    "            ref_element.text = \"See apparatus entry \" + ref\n",
    "    \n",
    "    return app\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    TEI = \"{%s}\" % TEI_NAMESPACE\n",
    "    root = ET.Element(TEI + \"TEI\", xmlns=TEI_NAMESPACE)\n",
    "    header = ET.SubElement(root, TEI + \"teiHeader\")\n",
    "    text = ET.SubElement(root, TEI + \"text\")\n",
    "    body = ET.SubElement(text, TEI + \"body\")\n",
    "    div = ET.SubElement(body, TEI + \"div\")\n",
    "    \n",
    "    last_verse_number = None\n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Determine if the line starts with a verse number\n",
    "        match = re.match(r'^(\\d+)', line)\n",
    "        if match:\n",
    "            last_verse_number = match.group(1)\n",
    "            content = line[len(last_verse_number):].strip()\n",
    "        else:\n",
    "            content = line\n",
    "        \n",
    "        if last_verse_number:\n",
    "            entry = create_apparatus_entry(last_verse_number, content, TEI)\n",
    "            div.append(entry)\n",
    "    \n",
    "    return ET.ElementTree(root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\")\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a6244acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "witnesses: G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) \n",
      "reading: > שערורה\n",
      "After dividers: IV (bla bla (f))\n"
     ]
    }
   ],
   "source": [
    "#split entry into witnesses, reading, and comments \n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()  # Returns a tuple with the three parts\n",
    "    else:\n",
    "        return None  # No divider matching the pattern was found\n",
    "\n",
    "# Example usage\n",
    "text = \"G-B msr. 30 (pm) G-A 89 (pm?) 150 (sm) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV (bla bla (f))\"#\"30 89 (sm) 93 (pm) 150 (non voc) + כיI II IV\"#\"93 (pm) < ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "split_parts = split_string(text)\n",
    "if split_parts:\n",
    "    print(\"witnesses:\", split_parts[0])\n",
    "    print(\"reading:\", split_parts[1])\n",
    "    print(\"After dividers:\", split_parts[2])\n",
    "else:\n",
    "    print(\"No dividers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "70cc4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('G-B msr. ', '30', 'pm'), ('G-A ', '89', 'pm?'), ('', '150', 'non voc'), ('', '30', 'sm'), ('', '89', 'sm'), ('', '93', 'sm'), ('MS-G ', '150', 'pm')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "25936b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified text: (See b. R.HaŠanamss 23b, LamR Buber 1:16 (40b))\n",
      "Numerals found: ['II', 'IV']\n"
     ]
    }
   ],
   "source": [
    "#parse comments\n",
    "import re\n",
    "\n",
    "def remove_and_list_roman_numerals(text):\n",
    "    # Regex to match some Roman numerals: sequences of \"I\"s followed by an optional \"V\"\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    # Find all occurrences of the pattern\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    # Remove empty matches from the list\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    # Replace found Roman numerals with an empty string\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "# Example usage\n",
    "text = \"II IV (See b. R.HaŠanamss 23b, LamR Buber 1:16 (40b))\"\n",
    "result_text, numerals_found = remove_and_list_roman_numerals(text)\n",
    "print(\"Modified text:\", result_text.strip())\n",
    "print(\"Numerals found:\", numerals_found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d33f9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(entry):\n",
    "    split_parts = split_string(entry)\n",
    "    if split_parts is None:\n",
    "        return None\n",
    "    \n",
    "    witnesses, reading, comments = split_parts\n",
    "\n",
    "    structured_entry = {\n",
    "        'witnesses': [],\n",
    "        'reading': reading,\n",
    "        'comments': '',\n",
    "        'cross_references': []\n",
    "    }\n",
    "\n",
    "    for part in custom_split_string(witnesses):\n",
    "        # Assuming part[1] contains the witness number and part[0], part[2], part[3] contain additional info\n",
    "        witness_info = {\n",
    "            'n': part[1],\n",
    "            'text': f\"{part[0]}{part[1]} {part[2].strip()}{part[3]}\"\n",
    "        }\n",
    "        structured_entry['witnesses'].append(witness_info)\n",
    "\n",
    "    comments_text, numerals_found = remove_and_list_roman_numerals(comments)\n",
    "    structured_entry['comments'] = comments_text\n",
    "    structured_entry['cross_references'] = numerals_found\n",
    "\n",
    "    return structured_entry\n",
    "\n",
    "def create_apparatus_entry(verse_number, content, TEI):\n",
    "    \"\"\"Create TEI element for an apparatus entry.\"\"\"\n",
    "    TEI_ns = {'tei': TEI}  # Define the namespace dictionary if needed\n",
    "    app = ET.Element(f\"{{{TEI}}}app\")  # Using namespace in the tag\n",
    "    \n",
    "    # Extract lemma text and the rest (witnesses, variant reading, and comments)\n",
    "    lemma_text, _, rest = content.partition(']')\n",
    "    lem = ET.SubElement(app, f\"{{{TEI}}}lem\")\n",
    "    lem.text = lemma_text.strip('[] ')\n",
    "\n",
    "    structured_entry = process_entry(rest)\n",
    "    if not structured_entry:\n",
    "        return None\n",
    "\n",
    "    for witness in structured_entry['witnesses']:\n",
    "        wit_element = ET.SubElement(app, f\"{{{TEI}}}wit\", {'n': witness['n']})\n",
    "        wit_element.text = witness['text']\n",
    "    \n",
    "    rdg_element = ET.SubElement(app, f\"{{{TEI}}}rdg\")\n",
    "    rdg_element.text = structured_entry['reading']\n",
    "\n",
    "    if structured_entry['comments']:\n",
    "        comment_element = ET.SubElement(app, f\"{{{TEI}}}note\")\n",
    "        comment_element.text = structured_entry['comments']\n",
    "\n",
    "    for ref in structured_entry['cross_references']:\n",
    "        ref_element = ET.SubElement(app, f\"{{{TEI}}}ref\")\n",
    "        ref_element.text = ref\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8e18d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEI document has been saved to apparatus_tei.xml.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "def create_tei_document(apparatus_lines):\n",
    "    TEI_NAMESPACE = \"http://www.tei-c.org/ns/1.0\"\n",
    "    ET.register_namespace('', TEI_NAMESPACE)  # Register the default namespace\n",
    "\n",
    "    # Create the root element without redundantly specifying the xmlns attribute\n",
    "    root = ET.Element(\"{%s}TEI\" % TEI_NAMESPACE)\n",
    "    header = ET.SubElement(root, \"{%s}teiHeader\" % TEI_NAMESPACE)\n",
    "    text = ET.SubElement(root, \"{%s}text\" % TEI_NAMESPACE)\n",
    "    body = ET.SubElement(text, \"{%s}body\" % TEI_NAMESPACE)\n",
    "    div = ET.SubElement(body, \"{%s}div\" % TEI_NAMESPACE)\n",
    "    \n",
    "    last_verse_number = None\n",
    "    for line in apparatus_lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Determine if the line starts with a verse number\n",
    "        match = re.match(r'^(\\d+)', line)\n",
    "        if match:\n",
    "            last_verse_number = match.group(1)\n",
    "            content = line[len(last_verse_number):].strip()\n",
    "        else:\n",
    "            content = line\n",
    "        \n",
    "        if last_verse_number:\n",
    "            entry = create_apparatus_entry(last_verse_number, content, TEI_NAMESPACE)\n",
    "            if entry is not None:  # Ensure entry creation was successful\n",
    "                div.append(entry)\n",
    "    \n",
    "    return ET.ElementTree(root)\n",
    "\n",
    "def save_tei_file(tree, filename):\n",
    "    tree.write(filename, encoding=\"UTF-8\", xml_declaration=True, method=\"xml\")\n",
    "\n",
    "\n",
    "# Replace 'your_input_file.txt' with the path to your actual input file\n",
    "input_file = '01 Hosea App III - מתוקן.txt'\n",
    "output_file = 'apparatus_tei.xml'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tei_tree = create_tei_document(lines)\n",
    "save_tei_file(tei_tree, output_file)\n",
    "\n",
    "print(f\"TEI document has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a4a220d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'n': '30', 'text': 'G-B msr. 30 (pm) '}, {'n': '89', 'text': 'G-A 89 (pm?) '}, {'n': '150', 'text': '150 (non voc) k'}, {'n': '30', 'text': ' 30 (sm) '}, {'n': '89', 'text': '89 (sm) '}, {'n': '93', 'text': '93 (sm) '}, {'n': '96', 'text': '96 '}, {'n': '150', 'text': '150 (pm) q'}, {'n': '93', 'text': ' 93 (pm) '}], 'reading': '> שערורה', 'comments': '  (bla bla (f))', 'cross_references': ['IV', 'II']}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_string(text):\n",
    "    pattern = re.compile(r'^(.*?)([\\+<~>]?\\s?[\\u0590-\\u05FF]+.*?)(.*)$', re.DOTALL)\n",
    "    match = pattern.match(text)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def custom_split_string(text):\n",
    "    pattern = re.compile(r'([^,\\d]*?)?(\\d+)\\s?(\\([^\\)]+\\)?)?([\\skq]*)?', re.DOTALL|re.UNICODE)\n",
    "    parts = re.findall(pattern, text)\n",
    "    return parts\n",
    "\n",
    "def remove_and_list_roman_numerals(text):\n",
    "    pattern = r'([I]*[V]?)'\n",
    "    found_numerals = re.findall(pattern, text)\n",
    "    found_numerals = [numeral for numeral in found_numerals if numeral]\n",
    "    result_text = re.sub(pattern, '', text)\n",
    "    return result_text, found_numerals\n",
    "\n",
    "# def process_entry(entry):\n",
    "#     split_parts = split_string(entry)\n",
    "#     if split_parts is None:\n",
    "#         return \"Unable to process entry: No valid dividers found.\"\n",
    "    \n",
    "#     witnesses, reading, comments = split_parts\n",
    "\n",
    "#     witness_entries = []\n",
    "#     for part in custom_split_string(witnesses):\n",
    "#         witness_entry = f'<witness n=\"{part[1]}\">{part[0]}{part[1]} {part[2].strip()}{part[3]}</witness>'\n",
    "#         witness_entries.append(witness_entry)\n",
    "#     witnesses_tagged = \"\\n\".join(witness_entries)\n",
    "\n",
    "#     reading_tagged = f'<reading>{reading}</reading>'\n",
    "\n",
    "#     comments_text, numerals_found = remove_and_list_roman_numerals(comments)\n",
    "#     comments_tagged = f'<comment>{comments_text}</comment>'\n",
    "#     cross_references = \"\\n\".join([f'<ref>{numeral}</ref>' for numeral in numerals_found])\n",
    "\n",
    "#     # Combine all parts, placing cross_references outside the comment\n",
    "#     tei_entry = f\"{witnesses_tagged}\\n{reading_tagged}\\n{comments_tagged}\\n{cross_references}\"\n",
    "#     return tei_entry\n",
    "\n",
    "# Example usage\n",
    "entry =\"G-B msr. 30 (pm) G-A 89 (pm?) 150 (non voc) k, 30 (sm) 89 (sm) 93 (sm) 96 150 (pm) q, 93 (pm) > שערורהIV II (bla bla (f))\"\n",
    "processed_entry = process_entry(entry)\n",
    "print(processed_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12325297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'93 (pm) + ביהושעIV (similarly PesiqtaR 33 (153b))'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest = \"93 (pm) + ביהושעIV (similarly PesiqtaR 33 (153b))\"\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caa1d542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tei_hebrew_output_enhanced.xml'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def strip_non_hebrew(word):\n",
    "    normalized_word = unicodedata.normalize('NFD', word)\n",
    "    stripped_word = ''.join(re.findall(r'[\\u05D0-\\u05EA]', normalized_word))\n",
    "    return unicodedata.normalize('NFC', stripped_word)\n",
    "\n",
    "def process_word(token, verse_id, word_id, parent_element):\n",
    "    parts = token.split('־')\n",
    "    pe_count = 1  # Counter for 'פ' tags\n",
    "\n",
    "    for part in parts:\n",
    "        w = ET.SubElement(parent_element, 'w', id=f'verse{verse_id}_word{word_id}')\n",
    "\n",
    "        alphabetic = strip_non_hebrew(part)\n",
    "        non_alphabetic = ''.join(re.findall(r'[^\\u05D0-\\u05EA]', part))\n",
    "\n",
    "        original = ET.SubElement(w, 'original')\n",
    "        original.text = part\n",
    "        stripped = ET.SubElement(w, 'stripped')\n",
    "        stripped.text = alphabetic\n",
    "        punctuation = ET.SubElement(w, 'punctuation')\n",
    "        punctuation.text = non_alphabetic\n",
    "\n",
    "        if \"פ\" in part:\n",
    "            pe_tag = ET.SubElement(w, 'pe', id=f'verse{verse_id}_pe{pe_count}')\n",
    "            pe_tag.text = \"פ\"\n",
    "            pe_count += 1\n",
    "        \n",
    "        word_id += 1\n",
    "    return word_id\n",
    "\n",
    "def encode_tei_hebrew_word_details_enhanced(file_path, output_file):\n",
    "    text = read_text_from_file(file_path)\n",
    "    TEI = ET.Element('TEI', xmlns='http://www.tei-c.org/ns/1.0')\n",
    "    text_element = ET.SubElement(TEI, 'text')\n",
    "    body = ET.SubElement(text_element, 'body')\n",
    "\n",
    "    chapter_id = 1\n",
    "    verse_id = 1\n",
    "\n",
    "    chapters = text.split('פרק')\n",
    "    for chapter in chapters[1:]:\n",
    "        div = ET.SubElement(body, 'div', type='chapter', id=f'chapter{chapter_id}')\n",
    "        chapter_id += 1\n",
    "\n",
    "        verses = re.split(r'(\\[\\פ\\]|:)', chapter)\n",
    "        for verse in verses:\n",
    "            if verse.strip() and verse not in ['[פ]', ':']:\n",
    "                p = ET.SubElement(div, 'p', type='verse', id=f'verse{verse_id}')\n",
    "                word_id = 1\n",
    "\n",
    "                tokens = verse.strip().split()\n",
    "                for token in tokens:\n",
    "                    word_id = process_word(token, verse_id, word_id, p)\n",
    "\n",
    "                verse_id += 1\n",
    "\n",
    "    tree = ET.ElementTree(TEI)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        tree.write(f, encoding=\"unicode\")\n",
    "\n",
    "# Specify the file paths\n",
    "file_path = 'file.txt'  # Replace with your input file path\n",
    "output_file = 'tei_hebrew_output_enhanced.xml'  # Replace with your output file path\n",
    "\n",
    "# Run the function\n",
    "encode_tei_hebrew_word_details_enhanced(file_path, output_file)\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[' ', '\"', '$', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', 'E', 'I', 'T', '_', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', '֑', '֔', '֕', '֖', '֗', '֙', '֛', '֜', '֞', '֣', '֤', '֥', '֨', '֩', 'ְ', 'ֱ', 'ֲ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֻ', 'ּ', 'ֽ', '׀', 'ׁ', 'ׂ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee3cf327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '$', '$1', '$2', '$4', '2', ':', '[', ']', '֑', '֔', '֕', '֖', '֗', '֙', '֜', '֣', '֤', '֥', '֥$', '֨', '֩', 'ְ', 'ְ$', 'ְ֙', 'ְּ', 'ְׁ', 'ְׂ', 'ֱ', 'ֲ', 'ִ', 'ִ$', 'ִ֔', 'ִ֖', 'ִ֜', 'ִ֨', 'ִּ', 'ִֽ', 'ִׁ', 'ֵ', 'ֵ$', 'ֵ֔', 'ֵ֖', 'ֵ֗', 'ֵ֛', 'ֵ֣', 'ֵ֤', 'ֵ֨', 'ֵּ', 'ֵֽ', 'ֵׁ', 'ֶ', 'ֶ֑', 'ֶ֙', 'ֶ֣', 'ֶ֤', 'ֶ֥', 'ֶּ', 'ֶֽ', 'ֶׁ', 'ַ', 'ַ֗', 'ַ֙', 'ַּ', 'ַׁ', 'ָ', 'ָ֑', 'ָ֔', 'ָ֖', 'ָ֗', 'ָ֛', 'ָ֜', 'ָ֞', 'ָ֣', 'ָ֥', 'ָ֨', 'ָּ', 'ָֽ', 'ֹ', 'ֹ֖', 'ֹ֣', 'ֹ֤', 'ֹ֨', 'ֹּ', 'ֹׂ', 'ֻ', 'ּ', 'ּ֣', 'ֽ', '־', '־$', '׀', 'ׁ', 'ׂ֖', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "def extract_consecutive_non_hebrew_groups(file_path):\n",
    "    text = read_text_from_file(file_path)\n",
    "    non_hebrew_groups = set()\n",
    "\n",
    "    # Using a regular expression to find sequences of non-Hebrew characters\n",
    "    pattern = re.compile(r'([^\\u05D0-\\u05EA]{,2})')\n",
    "    matches = pattern.findall(unicodedata.normalize('NFD', text))\n",
    "\n",
    "    for match in matches:\n",
    "        non_hebrew_groups.add(match.strip())\n",
    "\n",
    "    return non_hebrew_groups\n",
    "\n",
    "# Extract and print groups of consecutive non-Hebrew characters\n",
    "file_path = 'file.txt'\n",
    "\n",
    "consecutive_non_hebrew_groups = extract_consecutive_non_hebrew_groups(file_path)\n",
    "print(sorted(consecutive_non_hebrew_groups))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
